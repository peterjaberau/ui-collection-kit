[
  [
    "Map",
    1,
    2,
    9,
    10,
    252,
    253,
    310,
    311,
    461,
    462,
    812,
    813,
    963,
    964,
    1177,
    1178,
    1334,
    1335,
    1584,
    1585,
    1686,
    1687,
    1738,
    1739,
    2114,
    2115,
    2122,
    2123,
    2189,
    2190
  ],
  "meta::meta",
  ["Map", 3, 4, 5, 6, 7, 8],
  "astro-version",
  "5.7.0",
  "content-config-digest",
  "ea21fd9d91a1ef4e",
  "astro-config-digest",
  "{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://eventcatalog.dev\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":3000,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":false},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[[null,{\"themes\":[\"github-light\"],\"defaultProps\":{\"wrap\":true}}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":true,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}",
  "users",
  [
    "Map",
    11,
    12,
    21,
    22,
    30,
    31,
    42,
    43,
    54,
    55,
    63,
    64,
    75,
    76,
    87,
    88,
    96,
    97,
    105,
    106,
    117,
    118,
    129,
    130,
    138,
    139,
    150,
    151,
    159,
    160,
    168,
    169,
    177,
    178,
    189,
    190,
    201,
    202,
    213,
    214,
    222,
    223,
    234,
    235,
    243,
    244
  ],
  "asmith",
  {
    "id": 11,
    "data": 13,
    "body": 17,
    "filePath": 18,
    "digest": 19,
    "deferredRender": 20
  },
  { "id": 11, "name": 14, "avatarUrl": 15, "role": 16 },
  "Amy Smith",
  "https://randomuser.me/api/portraits/women/48.jpg",
  "Product owner",
  "Hello! I'm Amy Smith, the Product Owner of the innovative Full Stackers team. With a strong focus on delivering exceptional value, I specialize in connecting business objectives with technical solutions to create products that users love.\n\n### About Me\n\nWith a comprehensive background in product management and a solid understanding of software development, I bring a unique perspective to the table. My career has been driven by a passion for understanding user needs, defining clear product visions, and leading teams to successful product deliveries.\n\n### What I Do\n\nAs the Product Owner for Full Stackers, my role involves a wide range of responsibilities aimed at ensuring our products are both high-quality and user-centric. Key aspects of my role include:\n\n- **Product Vision & Strategy**: Defining and communicating the long-term vision and strategy for our products, ensuring alignment with the company's objectives and market demands.\n- **Roadmap Planning**: Developing and maintaining a product roadmap that highlights key features and milestones, prioritizing tasks based on their business value and user feedback.\n- **Stakeholder Management**: Engaging with stakeholders across the organization to gather requirements, provide updates, and ensure everyone is aligned on the product's direction.\n- **User-Centric Design**: Championing the end-users by conducting user research, analyzing feedback, and ensuring our products effectively solve their problems.\n- **Agile Leadership**: Leading the development process using Agile methodologies, facilitating sprint planning, and ensuring the team has clear priorities and objectives.\n\nMy mission is to deliver products that not only meet but exceed customer expectations. I thrive on the challenge of translating complex requirements into simple, intuitive solutions.\n\nIf you’re interested in product management, user experience, or discussing the latest trends in technology, feel free to reach out!",
  "../examples/default/users/aSmith.mdx",
  "4b4649610e7798d3",
  true,
  "alee",
  {
    "id": 21,
    "data": 23,
    "body": 27,
    "filePath": 28,
    "digest": 29,
    "deferredRender": 20
  },
  { "id": 21, "name": 24, "avatarUrl": 25, "role": 26 },
  "Alice Lee",
  "https://randomuser.me/api/portraits/women/91.jpg",
  "Technical Writer",
  "As a Technical Writer on the Documentation team, I create clear, comprehensive documentation for our products and APIs. My focus is on making complex technical concepts accessible to developers and end-users alike. I collaborate with engineering teams to ensure our documentation stays current and accurate.",
  "../examples/default/users/alee.mdx",
  "291f1a7dd066b8fb",
  "azhang",
  {
    "id": 30,
    "data": 32,
    "body": 39,
    "filePath": 40,
    "digest": 41,
    "deferredRender": 20
  },
  {
    "id": 30,
    "name": 33,
    "avatarUrl": 34,
    "role": 35,
    "email": 36,
    "slackDirectMessageUrl": 37,
    "msTeamsDirectMessageUrl": 38
  },
  "Alice Zhang",
  "https://randomuser.me/api/portraits/women/97.jpg",
  "Data Engineer",
  "azhang@company.com",
  "https://yourteam.slack.com/channels/azhang",
  "https://teams.microsoft.com/l/chat/0/0?users=azhang@company.com",
  "Building and maintaining data pipelines and infrastructure...",
  "../examples/default/users/azhang.mdx",
  "06d833c1da19ebf3",
  "dboyne",
  {
    "id": 42,
    "data": 44,
    "body": 51,
    "filePath": 52,
    "digest": 53,
    "deferredRender": 20
  },
  {
    "id": 42,
    "name": 45,
    "avatarUrl": 46,
    "role": 47,
    "email": 48,
    "slackDirectMessageUrl": 49,
    "msTeamsDirectMessageUrl": 50
  },
  "David Boyne",
  "https://pbs.twimg.com/profile_images/1262283153563140096/DYRDqKg6_400x400.png",
  "Lead developer",
  "test@test.com",
  "https://yourteam.slack.com/channels/boyney123",
  "https://teams.microsoft.com/l/chat/0/0?users=test@test.com",
  "Hello! I'm David Boyne, the Tech Lead of an amazing team called Full Stackers. With a passion for building robust and scalable systems, I specialize in designing and implementing event-driven architectures that power modern, responsive applications.\n\n### About Me\n\nWith over a decade of experience in the tech industry, I have honed my skills in full-stack development, cloud computing, and distributed systems. My journey has taken me through various roles, from software engineer to architect, and now as a tech lead, I am committed to driving innovation and excellence within my team.\n\n### What I Do\n\nAt Full Stackers, we focus on creating seamless and efficient event-driven architectures that enhance the performance and scalability of our applications. My role involves:\n\n- **Architecture Design**: Crafting scalable and resilient system architectures using event-driven paradigms.\n- **Team Leadership**: Guiding a talented team of developers, fostering a collaborative and innovative environment.\n- **Code Reviews & Mentorship**: Ensuring code quality and sharing knowledge to help the team grow.\n- **Stakeholder Collaboration**: Working closely with other teams and stakeholders to align our technical solutions with business goals.\n- **Continuous Improvement**: Advocating for best practices in software development, deployment, and monitoring.\n\nI am passionate about leveraging the power of events to build systems that are not only highly responsive but also easier to maintain and extend. In an ever-evolving tech landscape, I strive to stay ahead of the curve, continuously learning and adapting to new technologies and methodologies.\n\nFeel free to connect with me to discuss all things tech, event-driven architectures, or to exchange ideas on building better software systems!\n\n---\n*David Boyne*\n*Tech Lead, Full Stackers*",
  "../examples/default/users/dboyne.mdx",
  "3cdd8f419886c4c9",
  "jchen",
  {
    "id": 54,
    "data": 56,
    "body": 60,
    "filePath": 61,
    "digest": 62,
    "deferredRender": 20
  },
  { "id": 54, "name": 57, "avatarUrl": 58, "role": 59 },
  "Julia Chen",
  "https://randomuser.me/api/portraits/women/23.jpg",
  "DevOps Engineer",
  "As a DevOps Engineer on the Platform Engineering team, I specialize in building and maintaining our cloud infrastructure and deployment pipelines. My focus is on automating processes, improving system reliability, and ensuring smooth continuous integration and deployment (CI/CD). I work closely with development teams to streamline their workflows and implement robust monitoring and alerting solutions.",
  "../examples/default/users/jchen.mdx",
  "c42f7763c802e018",
  "dkim",
  {
    "id": 63,
    "data": 65,
    "body": 72,
    "filePath": 73,
    "digest": 74,
    "deferredRender": 20
  },
  {
    "id": 63,
    "name": 66,
    "avatarUrl": 67,
    "role": 68,
    "email": 69,
    "slackDirectMessageUrl": 70,
    "msTeamsDirectMessageUrl": 71
  },
  "David Kim",
  "https://randomuser.me/api/portraits/men/96.jpg",
  "Performance Engineer",
  "dkim@company.com",
  "https://yourteam.slack.com/channels/dkim",
  "https://teams.microsoft.com/l/chat/0/0?users=dkim@company.com",
  "Optimizing application performance and user experience...",
  "../examples/default/users/dkim.mdx",
  "ef72804be37dc447",
  "jbrown",
  {
    "id": 75,
    "data": 77,
    "body": 84,
    "filePath": 85,
    "digest": 86,
    "deferredRender": 20
  },
  {
    "id": 75,
    "name": 78,
    "avatarUrl": 79,
    "role": 80,
    "email": 81,
    "slackDirectMessageUrl": 82,
    "msTeamsDirectMessageUrl": 83
  },
  "Jessica Brown",
  "https://randomuser.me/api/portraits/women/95.jpg",
  "UI/UX Designer",
  "jbrown@company.com",
  "https://yourteam.slack.com/channels/jbrown",
  "https://teams.microsoft.com/l/chat/0/0?users=jbrown@company.com",
  "Creating intuitive and engaging user interfaces...",
  "../examples/default/users/jbrown.mdx",
  "3735a1e080253fce",
  "lkim",
  {
    "id": 87,
    "data": 89,
    "body": 93,
    "filePath": 94,
    "digest": 95,
    "deferredRender": 20
  },
  { "id": 87, "name": 90, "avatarUrl": 91, "role": 92 },
  "Lisa Kim",
  "https://randomuser.me/api/portraits/women/89.jpg",
  "Backend Engineer",
  "As a Backend Engineer on the API Platform team, I design and implement",
  "../examples/default/users/lkim.mdx",
  "27114d4f72c87120",
  "jmiller",
  {
    "id": 96,
    "data": 98,
    "body": 102,
    "filePath": 103,
    "digest": 104,
    "deferredRender": 20
  },
  { "id": 96, "name": 99, "avatarUrl": 100, "role": 101 },
  "James Miller",
  "https://randomuser.me/api/portraits/men/18.jpg",
  "Systems Architect",
  "I'm a Systems Architect on the Infrastructure team, responsible for designing and evolving our technical architecture. My expertise includes cloud architecture, system integration, and scalability planning. I work closely with various teams to ensure our technical solutions align with business needs while maintaining technical excellence.",
  "../examples/default/users/jmiller.mdx",
  "2f54e03d64b0621b",
  "kpatel",
  {
    "id": 105,
    "data": 107,
    "body": 114,
    "filePath": 115,
    "digest": 116,
    "deferredRender": 20
  },
  {
    "id": 105,
    "name": 108,
    "avatarUrl": 109,
    "role": 110,
    "email": 111,
    "slackDirectMessageUrl": 112,
    "msTeamsDirectMessageUrl": 113
  },
  "Kiran Patel",
  "https://randomuser.me/api/portraits/men/92.jpg",
  "Cloud Architect",
  "kpatel@company.com",
  "https://yourteam.slack.com/channels/kpatel",
  "https://teams.microsoft.com/l/chat/0/0?users=kpatel@company.com",
  "As a Cloud Architect, I design and implement scalable cloud infrastructure solutions...",
  "../examples/default/users/kpatel.mdx",
  "486c11bd8f9ac042",
  "lnguyen",
  {
    "id": 117,
    "data": 119,
    "body": 126,
    "filePath": 127,
    "digest": 128,
    "deferredRender": 20
  },
  {
    "id": 117,
    "name": 120,
    "avatarUrl": 121,
    "role": 122,
    "email": 123,
    "slackDirectMessageUrl": 124,
    "msTeamsDirectMessageUrl": 125
  },
  "Lily Nguyen",
  "https://randomuser.me/api/portraits/women/1.jpg",
  "Frontend Developer",
  "lnguyen@company.com",
  "https://yourteam.slack.com/channels/lnguyen",
  "https://teams.microsoft.com/l/chat/0/0?users=lnguyen@company.com",
  "Developing responsive web applications...",
  "../examples/default/users/lnguyen.mdx",
  "9646d464f4617b94",
  "msmith",
  {
    "id": 129,
    "data": 131,
    "body": 135,
    "filePath": 136,
    "digest": 137,
    "deferredRender": 20
  },
  { "id": 129, "name": 132, "avatarUrl": 133, "role": 134 },
  "Martin Smith",
  "https://randomuser.me/api/portraits/men/51.jpg",
  "Senior software engineer",
  "As a Senior Mobile Developer on The Mobile Devs team, I play a key role in designing, developing, and maintaining our company’s mobile applications. My focus is on creating a seamless and intuitive user experience for our customers on both iOS and Android platforms. I work closely with cross-functional teams, including backend developers, UX/UI designers, and product managers, to deliver high-quality mobile solutions that meet business objectives and exceed user expectations.",
  "../examples/default/users/mSmith.mdx",
  "022ea06fa0ebccf2",
  "mwang",
  {
    "id": 138,
    "data": 140,
    "body": 147,
    "filePath": 148,
    "digest": 149,
    "deferredRender": 20
  },
  {
    "id": 138,
    "name": 141,
    "avatarUrl": 142,
    "role": 143,
    "email": 144,
    "slackDirectMessageUrl": 145,
    "msTeamsDirectMessageUrl": 146
  },
  "Michael Wang",
  "https://randomuser.me/api/portraits/men/98.jpg",
  "ML Engineer",
  "mwang@company.com",
  "https://yourteam.slack.com/channels/mwang",
  "https://teams.microsoft.com/l/chat/0/0?users=mwang@company.com",
  "Developing and deploying machine learning models...",
  "../examples/default/users/mwang.mdx",
  "1bfd84a572614958",
  "nshah",
  {
    "id": 150,
    "data": 152,
    "body": 156,
    "filePath": 157,
    "digest": 158,
    "deferredRender": 20
  },
  { "id": 150, "name": 153, "avatarUrl": 154, "role": 155 },
  "Nina Shah",
  "https://randomuser.me/api/portraits/women/33.jpg",
  "Scrum Master",
  "As a Scrum Master for the Platform team, I facilitate agile processes and help remove obstacles to team productivity. My focus is on improving team dynamics, sprint planning, and process optimization. I work closely with product owners and development teams to ensure smooth project delivery.",
  "../examples/default/users/nshah.mdx",
  "55e02c0422b1eb9e",
  "mwilson",
  {
    "id": 159,
    "data": 161,
    "body": 165,
    "filePath": 166,
    "digest": 167,
    "deferredRender": 20
  },
  { "id": 159, "name": 162, "avatarUrl": 163, "role": 164 },
  "Mike Wilson",
  "https://randomuser.me/api/portraits/men/77.jpg",
  "QA Engineer",
  "I'm a QA Engineer on the Quality Assurance team, ensuring our products meet the highest standards of quality and reliability. My focus includes automated testing, performance testing, and developing comprehensive test strategies. I work closely with development teams to implement effective testing practices throughout the development lifecycle.",
  "../examples/default/users/mwilson.mdx",
  "8c5de39853acbd9b",
  "rjohnson",
  {
    "id": 168,
    "data": 170,
    "body": 174,
    "filePath": 175,
    "digest": 176,
    "deferredRender": 20
  },
  { "id": 168, "name": 171, "avatarUrl": 172, "role": 173 },
  "Robert Johnson",
  "https://randomuser.me/api/portraits/men/32.jpg",
  "UX Designer",
  "I'm a UX Designer on the Design Systems team, passionate about creating intuitive and accessible user experiences. With a background in cognitive psychology and human-computer interaction, I focus on user research, wireframing, and prototyping. I collaborate closely with product teams to ensure our solutions meet both user needs and business objectives.",
  "../examples/default/users/rjohnson.mdx",
  "fcf5e6bf3271a8d0",
  "rjones",
  {
    "id": 177,
    "data": 179,
    "body": 186,
    "filePath": 187,
    "digest": 188,
    "deferredRender": 20
  },
  {
    "id": 177,
    "name": 180,
    "avatarUrl": 181,
    "role": 182,
    "email": 183,
    "slackDirectMessageUrl": 184,
    "msTeamsDirectMessageUrl": 185
  },
  "Robert Jones",
  "https://randomuser.me/api/portraits/men/100.jpg",
  "Security Analyst",
  "rjones@company.com",
  "https://yourteam.slack.com/channels/rjones",
  "https://teams.microsoft.com/l/chat/0/0?users=rjones@company.com",
  "Analyzing and improving system security...",
  "../examples/default/users/rjones.mdx",
  "ebf05ef0f5a24352",
  "rsingh",
  {
    "id": 189,
    "data": 191,
    "body": 198,
    "filePath": 199,
    "digest": 200,
    "deferredRender": 20
  },
  {
    "id": 189,
    "name": 192,
    "avatarUrl": 193,
    "role": 194,
    "email": 195,
    "slackDirectMessageUrl": 196,
    "msTeamsDirectMessageUrl": 197
  },
  "Raj Singh",
  "https://randomuser.me/api/portraits/men/94.jpg",
  "Mobile Developer",
  "rsingh@company.com",
  "https://yourteam.slack.com/channels/rsingh",
  "https://teams.microsoft.com/l/chat/0/0?users=rsingh@company.com",
  "Focused on developing cross-platform mobile applications...",
  "../examples/default/users/rsingh.mdx",
  "38b7b0c32f6f124a",
  "mchen",
  {
    "id": 201,
    "data": 203,
    "body": 210,
    "filePath": 211,
    "digest": 212,
    "deferredRender": 20
  },
  {
    "id": 201,
    "name": 204,
    "avatarUrl": 205,
    "role": 206,
    "email": 207,
    "slackDirectMessageUrl": 208,
    "msTeamsDirectMessageUrl": 209
  },
  "Michelle Chen",
  "https://randomuser.me/api/portraits/women/93.jpg",
  "Site Reliability Engineer",
  "mchen@company.com",
  "https://yourteam.slack.com/channels/mchen",
  "https://teams.microsoft.com/l/chat/0/0?users=mchen@company.com",
  "Specializing in maintaining system reliability and performance...",
  "../examples/default/users/mchen.mdx",
  "86e6cd325266b90d",
  "rthomas",
  {
    "id": 213,
    "data": 215,
    "body": 219,
    "filePath": 220,
    "digest": 221,
    "deferredRender": 20
  },
  { "id": 213, "name": 216, "avatarUrl": 217, "role": 218 },
  "Ray Thomas",
  "https://randomuser.me/api/portraits/men/62.jpg",
  "Frontend Engineer",
  "I'm a Frontend Engineer on the Web Platform team, specializing in building responsive and accessible web applications. My expertise includes modern JavaScript frameworks, CSS architecture, and web performance optimization. I work closely with designers and backend teams to deliver exceptional user experiences.",
  "../examples/default/users/rthomas.mdx",
  "3fa7b31898d2d937",
  "slee",
  {
    "id": 222,
    "data": 224,
    "body": 231,
    "filePath": 232,
    "digest": 233,
    "deferredRender": 20
  },
  {
    "id": 222,
    "name": 225,
    "avatarUrl": 226,
    "role": 227,
    "email": 228,
    "slackDirectMessageUrl": 229,
    "msTeamsDirectMessageUrl": 230
  },
  "Sarah Lee",
  "https://randomuser.me/api/portraits/women/99.jpg",
  "Product Manager",
  "slee@company.com",
  "https://yourteam.slack.com/channels/slee",
  "https://teams.microsoft.com/l/chat/0/0?users=slee@company.com",
  "Managing product development and strategy...",
  "../examples/default/users/slee.mdx",
  "1ab927a1b26c57b6",
  "spatel",
  {
    "id": 234,
    "data": 236,
    "body": 240,
    "filePath": 241,
    "digest": 242,
    "deferredRender": 20
  },
  { "id": 234, "name": 237, "avatarUrl": 238, "role": 239 },
  "Sanya Patel",
  "https://randomuser.me/api/portraits/women/67.jpg",
  "Data Scientist",
  "As a Data Scientist on the Analytics team, I leverage machine learning and statistical analysis to derive meaningful insights from our data. My expertise includes predictive modeling, A/B testing, and developing data-driven solutions. I work closely with product and engineering teams to implement data-driven features and optimize user experiences.",
  "../examples/default/users/spatel.mdx",
  "3ae379142fd7a84a",
  "tgarcia",
  {
    "id": 243,
    "data": 245,
    "body": 249,
    "filePath": 250,
    "digest": 251,
    "deferredRender": 20
  },
  { "id": 243, "name": 246, "avatarUrl": 247, "role": 248 },
  "Tom Garcia",
  "https://randomuser.me/api/portraits/men/41.jpg",
  "Security Engineer",
  "I'm a Security Engineer on the InfoSec team, responsible for maintaining and enhancing our organization's security posture. My focus includes vulnerability assessment, security architecture review, and implementing security best practices. I work closely with development teams to ensure security is built into our products from the ground up.",
  "../examples/default/users/tgarcia.mdx",
  "cee196eb2cf844d3",
  "teams",
  ["Map", 254, 255, 270, 271, 283, 284, 297, 298],
  "full-stack",
  {
    "id": 254,
    "data": 256,
    "body": 267,
    "filePath": 268,
    "digest": 269,
    "deferredRender": 20
  },
  {
    "id": 254,
    "name": 257,
    "summary": 258,
    "email": 48,
    "slackDirectMessageUrl": 49,
    "msTeamsDirectMessageUrl": 259,
    "members": 260
  },
  "Full stackers",
  "Full stack developers based in London, UK",
  "https://teams.microsoft.com/l/channel/\u003CYour Channel Id>/\u003CYour team chat name>?groupId=\u003CYour group Id>&tenantId=\u003CYour tenant Id>",
  [261, 262, 263, 264, 265, 266],
  { "id": 42, "collection": 9 },
  { "id": 213, "collection": 9 },
  { "id": 87, "collection": 9 },
  { "id": 117, "collection": 9 },
  { "id": 105, "collection": 9 },
  { "id": 63, "collection": 9 },
  "## Overview\n\nThe Full Stack Team is responsible for developing and maintaining both the front-end and back-end components of our applications. This team ensures that the user interfaces are intuitive and responsive, and that the server-side logic and database interactions are efficient and secure. The Full Stack Team handles the entire lifecycle of web applications, from initial development to deployment and ongoing maintenance.\n\n## Responsibilities\n\n### Key Responsibilities\n- **Front-End Development**: Design and implement user interfaces using modern web technologies (e.g., HTML, CSS, JavaScript, React).\n- **Back-End Development**: Develop and maintain server-side logic, APIs, and database interactions (e.g., Node.js, Express, SQL/NoSQL databases).\n- **Integration**: Ensure seamless communication between the front-end and back-end components.\n- **Performance Optimization**: Optimize the performance and scalability of web applications.\n- **Testing and Debugging**: Write and maintain unit, integration, and end-to-end tests to ensure the quality and reliability of the applications.\n- **Deployment**: Manage the deployment of applications to production environments using CI/CD pipelines.",
  "../examples/default/teams/full-stack.mdx",
  "c911174da3e34911",
  "mobile-devs",
  {
    "id": 270,
    "data": 272,
    "body": 280,
    "filePath": 281,
    "digest": 282,
    "deferredRender": 20
  },
  { "id": 270, "name": 273, "members": 274 },
  "The mobile devs",
  [275, 276, 277, 278, 279],
  { "id": 129, "collection": 9 },
  { "id": 189, "collection": 9 },
  { "id": 75, "collection": 9 },
  { "id": 138, "collection": 9 },
  { "id": 54, "collection": 9 },
  "The Mobile Devs team is responsible for the development and maintenance of the mobile applications for our company. This includes the iOS and Android apps that customers use to interact with our services, make purchases, and manage their accounts. The team ensures that the mobile apps are user-friendly, secure, and performant.\n\n## Responsibilities\n\n### 1. Mobile Application Development\n- **Platform Support**: Developing and maintaining apps for iOS and Android platforms.\n- **Feature Implementation**: Implementing new features based on product requirements and user feedback.\n- **User Interface Design**: Ensuring a consistent and intuitive user interface across all mobile platforms.\n- **Performance Optimization**: Optimizing the performance of mobile apps to ensure fast and smooth user experiences.\n\n### 2. Integration with Backend Services\n- **API Integration**: Integrating mobile apps with backend services using RESTful APIs and other communication protocols.\n- **Real-time Updates**: Implementing real-time data updates and synchronization with backend services.",
  "../examples/default/teams/mobile-devs.mdx",
  "3a3774310fd551db",
  "subscriptions-management",
  {
    "id": 283,
    "data": 285,
    "body": 294,
    "filePath": 295,
    "digest": 296,
    "deferredRender": 20
  },
  { "id": 283, "name": 286, "members": 287 },
  "The subscriptions management team",
  [288, 289, 290, 291, 292, 293],
  { "id": 243, "collection": 9 },
  { "id": 21, "collection": 9 },
  { "id": 168, "collection": 9 },
  { "id": 30, "collection": 9 },
  { "id": 177, "collection": 9 },
  { "id": 96, "collection": 9 },
  "The subscriptions management team is responsible for overseeing and optimizing the entire subscription lifecycle within our organization. Their key responsibilities include:\n\n- Processing and validating incoming customer orders\n- Managing order fulfillment workflows and inventory allocation\n- Coordinating with warehouse and shipping teams\n- Handling order modifications and cancellations\n- Resolving order-related customer issues and disputes\n\nThe team works closely with sales, customer service, and logistics departments to ensure smooth order processing and customer satisfaction.",
  "../examples/default/teams/subscriptions-management.mdx",
  "96c8af41cfde66c4",
  "order-management",
  {
    "id": 297,
    "data": 299,
    "body": 307,
    "filePath": 308,
    "digest": 309,
    "deferredRender": 20
  },
  { "id": 297, "name": 300, "members": 301 },
  "The order management team",
  [302, 303, 304, 305, 306],
  { "id": 11, "collection": 9 },
  { "id": 201, "collection": 9 },
  { "id": 150, "collection": 9 },
  { "id": 222, "collection": 9 },
  { "id": 234, "collection": 9 },
  "The order management team is responsible for overseeing and optimizing the entire order lifecycle within our organization. Their key responsibilities include:\n\n- Processing and validating incoming customer orders\n- Managing order fulfillment workflows and inventory allocation\n- Coordinating with warehouse and shipping teams\n- Handling order modifications and cancellations\n- Resolving order-related customer issues and disputes\n- Monitoring order processing metrics and KPIs\n- Implementing and maintaining order management systems\n- Developing strategies to improve order accuracy and efficiency\n\nThe team works closely with sales, customer service, and logistics departments to ensure smooth order processing and customer satisfaction.",
  "../examples/default/teams/order-management.mdx",
  "a84d175b65535bcc",
  "chatPrompts",
  [
    "Map",
    312,
    313,
    328,
    329,
    341,
    342,
    356,
    357,
    365,
    366,
    373,
    374,
    385,
    386,
    406,
    407,
    419,
    420,
    433,
    434,
    448,
    449
  ],
  "chat-prompts/learning/tell-me-more-about-this-event",
  {
    "id": 312,
    "data": 314,
    "body": 325,
    "filePath": 326,
    "digest": 327,
    "deferredRender": 20
  },
  { "title": 315, "type": 316, "inputs": 317, "category": 322 },
  "Tell me more about the given event",
  "text",
  [318],
  { "id": 319, "label": 320, "type": 321 },
  "event-name",
  "Select the event you want to know more about",
  "resource-list-events",
  { "id": 323, "label": 323, "icon": 324 },
  "Learning",
  "GraduationCap",
  "Tell me more about the given event `{{event-name}}`.",
  "../examples/default/chat-prompts/learning/tell-me-more-about-this-event.mdx",
  "7febfb852b64ff32",
  "chat-prompts/learning/tell-me-more-about-this-service",
  {
    "id": 328,
    "data": 330,
    "body": 338,
    "filePath": 339,
    "digest": 340,
    "deferredRender": 20
  },
  { "title": 331, "type": 316, "inputs": 332, "category": 337 },
  "Tell me more about the given service",
  [333],
  { "id": 334, "label": 335, "type": 336 },
  "service-name",
  "Select the service you want to know more about",
  "resource-list-services",
  { "id": 323, "label": 323, "icon": 324 },
  "Tell me more about the given service `{{service-name}}`.",
  "../examples/default/chat-prompts/learning/tell-me-more-about-this-service.mdx",
  "866c68f52afd72f3",
  "chat-prompts/quick-feedback/validate-this-schema",
  {
    "id": 341,
    "data": 343,
    "body": 353,
    "filePath": 354,
    "digest": 355,
    "deferredRender": 20
  },
  { "title": 344, "type": 316, "inputs": 345, "category": 350 },
  "Validate schema against FlowMart EDA best practices",
  [346],
  { "id": 347, "label": 348, "type": 349 },
  "schema",
  "Schema",
  "code",
  { "id": 351, "label": 351, "icon": 352 },
  "Code Review",
  "MessageSquareText",
  "Please review the following event schema:\n\n```json\n{{schema}}\n```\n\nValidate this schema against the FlowMart EDA best practices listed below and provide feedback on any areas where the schema does not align or could be improved based on these guidelines.\n\n--- \n\n## FlowMart EDA Best Practices for Reference:\n\n### Event Naming and Structure\n\n*   **Past Tense Naming:** All event names MUST be in the past tense (e.g., `OrderCreated`, `UserLoggedIn`, `PaymentProcessed`). This reflects that an event represents something that has already occurred.\n*   **Correlation IDs:** Every event MUST include a `correlationId`. This allows us to track related events across different services and flows, which is crucial for debugging and observability.\n*   **Clear Event Payloads:** Event payloads should be well-defined, documented, and contain only the necessary information relevant to the event itself. Avoid overly large or generic payloads.\n\n### Contract Management and Ownership\n\n*   **Producer Ownership:** The service that produces an event owns its contract (schema). Consumers should rely on this defined contract.\n*   **Public vs. Private Events:** Events MUST be explicitly marked as either `public` or `private`.\n    *   `Public` events have strong backward compatibility guarantees.\n    *   `Private` events are typically for internal service use and may have less stringent compatibility requirements, but changes should still be coordinated.\n*   **No Breaking Changes (Public Events):** We strive to avoid breaking changes in `public` event contracts. If a change is necessary, it must be additive or managed through versioning.\n*   **Semantic Versioning (SemVer):** Event schemas SHOULD follow Semantic Versioning (e.g., `1.0.0`, `1.1.0`, `2.0.0`).\n    *   MAJOR version bumps indicate breaking changes (avoided for public events).\n    *   MINOR version bumps indicate backward-compatible additions.\n    *   PATCH version bumps indicate backward-compatible fixes.\n\n### Design Principles\n\n*   **Idempotent Consumers:** Consumers should be designed to be idempotent, meaning they can safely process the same event multiple times without unintended side effects. This handles scenarios like message replays or network issues.\n*   **Asynchronous Communication:** Favor asynchronous communication patterns. Events decouple services, allowing them to operate independently.\n*   **Domain-Driven Design (DDD):** Align events with bounded contexts and domain concepts from DDD. This helps create meaningful, cohesive events.\n*   **Schema Registry:** Utilize a schema registry (like the one provided by EventCatalog!) to manage, validate, and evolve event schemas centrally.\n\n### Documentation and Discovery\n\n*   **EventCatalog:** All events, schemas, producers, and consumers MUST be documented in our EventCatalog. This serves as the single source of truth for our EDA landscape.\n*   **Clear Descriptions:** Provide clear, concise descriptions for each event, explaining its purpose and context.\n\nBy following these practices, we aim to build a resilient and understandable event-driven ecosystem at FlowMart.",
  "../examples/default/chat-prompts/quick-feedback/validate-this-schema.mdx",
  "ab61d9de3a299cd9",
  "chat-prompts/learning/validate-this-schema",
  {
    "id": 356,
    "data": 358,
    "body": 353,
    "filePath": 363,
    "digest": 364,
    "deferredRender": 20
  },
  { "title": 359, "type": 316, "inputs": 360, "category": 362 },
  "Validate schema against best practices",
  [361],
  { "id": 347, "label": 348, "type": 349 },
  { "id": 323, "label": 323, "icon": 324 },
  "../examples/default/chat-prompts/learning/validate-this-schema.mdx",
  "2358c64a9d46a6c4",
  "chat-prompts/learning/what-are-the-best-practices-for-eda-in-the-company",
  {
    "id": 365,
    "data": 367,
    "body": 370,
    "filePath": 371,
    "digest": 372,
    "deferredRender": 20
  },
  { "title": 368, "type": 316, "category": 369 },
  "What are the best practices for EDA in the company?",
  { "id": 323, "label": 323, "icon": 324 },
  "At FlowMart, we adhere to several best practices to ensure our Event-Driven Architecture (EDA) is robust, scalable, and maintainable. Here are the key guidelines:\n\n### Event Naming and Structure\n\n*   **Past Tense Naming:** All event names MUST be in the past tense (e.g., `OrderCreated`, `UserLoggedIn`, `PaymentProcessed`). This reflects that an event represents something that has already occurred.\n*   **Correlation IDs:** Every event MUST include a `correlationId`. This allows us to track related events across different services and flows, which is crucial for debugging and observability.\n*   **Clear Event Payloads:** Event payloads should be well-defined, documented, and contain only the necessary information relevant to the event itself. Avoid overly large or generic payloads.\n\n### Contract Management and Ownership\n\n*   **Producer Ownership:** The service that produces an event owns its contract (schema). Consumers should rely on this defined contract.\n*   **Public vs. Private Events:** Events MUST be explicitly marked as either `public` or `private`.\n    *   `Public` events have strong backward compatibility guarantees.\n    *   `Private` events are typically for internal service use and may have less stringent compatibility requirements, but changes should still be coordinated.\n*   **No Breaking Changes (Public Events):** We strive to avoid breaking changes in `public` event contracts. If a change is necessary, it must be additive or managed through versioning.\n*   **Semantic Versioning (SemVer):** Event schemas SHOULD follow Semantic Versioning (e.g., `1.0.0`, `1.1.0`, `2.0.0`).\n    *   MAJOR version bumps indicate breaking changes (avoided for public events).\n    *   MINOR version bumps indicate backward-compatible additions.\n    *   PATCH version bumps indicate backward-compatible fixes.\n\n### Design Principles\n\n*   **Idempotent Consumers:** Consumers should be designed to be idempotent, meaning they can safely process the same event multiple times without unintended side effects. This handles scenarios like message replays or network issues.\n*   **Asynchronous Communication:** Favor asynchronous communication patterns. Events decouple services, allowing them to operate independently.\n*   **Domain-Driven Design (DDD):** Align events with bounded contexts and domain concepts from DDD. This helps create meaningful, cohesive events.\n*   **Schema Registry:** Utilize a schema registry (like the one provided by EventCatalog!) to manage, validate, and evolve event schemas centrally.\n\n### Documentation and Discovery\n\n*   **EventCatalog:** All events, schemas, producers, and consumers MUST be documented in our EventCatalog. This serves as the single source of truth for our EDA landscape.\n*   **Clear Descriptions:** Provide clear, concise descriptions for each event, explaining its purpose and context.\n\nBy following these practices, we aim to build a resilient and understandable event-driven ecosystem at FlowMart.",
  "../examples/default/chat-prompts/learning/what-are-the-best-practices-for-eda-in-the-company.mdx",
  "ad942af3e0eadab2",
  "chat-prompts/code/create-an-example-schema",
  {
    "id": 373,
    "data": 375,
    "body": 382,
    "filePath": 383,
    "digest": 384,
    "deferredRender": 20
  },
  { "title": 376, "type": 316, "inputs": 377, "category": 380 },
  "Generate a JSON Schema Following FlowMart Standards",
  [378],
  { "id": 319, "label": 379, "type": 316 },
  "What is the name of the event?",
  { "id": 381, "label": 381, "icon": 381 },
  "Code",
  "Generate a JSON schema for an event called `{{event-name}}`. This schema must follow FlowMart's payload standards.\n\n**FlowMart Payload Standards:**\n\n*   Every payload MUST include a top-level `metadata` field.\n*   The `metadata` field MUST contain:\n    *   `correlationId`: A string, preferably in UUID format.\n    *   `createdDate`: A string in ISO 8601 format (e.g., \"2023-11-02T14:29:55Z\").\n*   The main event details should be within a top-level `data` field.\n\n**Example `UserSignedUp` Event Payload:**\n\n```json\n{\n  \"metadata\": {\n     \"correlationId\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",\n     \"createdDate\": \"2023-11-02T14:29:55Z\"\n  },\n  \"data\": {\n    \"userId\": \"user-789\",\n    \"email\": \"test@example.com\",\n    \"signUpMethod\": \"Google\"\n  }\n}\n```\n\n**Invalid Example (Missing `metadata`):**\n```json\n{\n  \"data\": {\n    \"userId\": \"user-123\",\n    \"email\": \"invalid@example.com\"\n  }\n}\n```\n\n**Invalid Example (Missing `correlationId` in `metadata`):**\n```json\n{\n  \"metadata\": {\n     \"createdDate\": \"2023-11-01T10:00:00Z\"\n  },\n  \"data\": {\n     \"userId\": \"user-456\",\n     \"email\": \"incomplete@example.com\"\n  }\n}\n```\n\n**Task:**\n\nCreate the JSON schema definition for the `UserSignedUp` event described above, ensuring it enforces the FlowMart payload standards.",
  "../examples/default/chat-prompts/code/create-an-example-schema.mdx",
  "d6386cefcf6ca87c",
  "chat-prompts/code/create-consumer-code-for-event",
  {
    "id": 385,
    "data": 387,
    "body": 403,
    "filePath": 404,
    "digest": 405,
    "deferredRender": 20
  },
  { "title": 388, "type": 316, "inputs": 389, "category": 402 },
  "Create Kafka Consumer Code for Event",
  [390, 392],
  { "id": 319, "label": 391, "type": 321 },
  "Select the event you want to create a consumer for",
  { "id": 393, "label": 394, "type": 395, "options": 396 },
  "code-language",
  "What is the programming language of the code?",
  "select",
  [397, 398, 399, 400, 401],
  "Python",
  "Java",
  "TypeScript",
  "Go",
  "Ruby",
  { "id": 381, "label": 381, "icon": 381 },
  "Generate Kafka consumer code in `{{code-language}}` that consumes the event `{{event-name}}`.\n\n**Kafka Cluster Details:**\n\n*   **Bootstrap Servers:** `kafka-prod-1.flowmart.internal:9092,kafka-prod-2.flowmart.internal:9092,kafka-prod-3.flowmart.internal:9092`\n*   **Topic Name:** The topic name follows the pattern `fm.events.\u003Cevent-name>` (e.g., `fm.events.UserSignedUp` for the `UserSignedUp` event).\n*   **Consumer Group ID:** Use a descriptive consumer group ID, like `consumer-{{event-name}}-service`.\n*   **Security Protocol:** SASL_SSL\n*   **SASL Mechanism:** PLAIN\n\n**Best Practices to Follow:**\n\n1.  **Deserialization:** Assume the event payload is JSON and follows FlowMart's payload standards (with `metadata` and `data` fields). Deserialize the message payload into an appropriate data structure or class.\n2.  **Error Handling:** Implement robust error handling for connection issues, deserialization failures, and message processing errors. Use a dead-letter queue (DLQ) pattern for messages that cannot be processed after retries. The DLQ topic name is `dlq.fm.events.\u003Cevent-name>`.\n3.  **Configuration:** Externalize Kafka connection details and consumer group ID (don't hardcode them directly in the main logic).\n4.  **Logging:** Add basic logging for successful message consumption and any errors encountered.\n5.  **Commit Strategy:** Use manual commits (`enable.auto.commit=false`) to ensure messages are only marked as processed after successful handling.\n6.  **Idempotency:** Briefly mention how the consumer logic should handle potential duplicate messages (e.g., by checking a database record based on `metadata.correlationId` before processing).\n\n**Task:**\n\nProvide the complete, runnable Kafka consumer code snippet in `{{code-language}}` for the `{{event-name}}` event, incorporating the cluster details and best practices mentioned above. Include necessary imports and basic setup.\n\nIf you use any external libraries, please include the import statements and how to install them, step by step, make sure dependencies are listed first.",
  "../examples/default/chat-prompts/code/create-consumer-code-for-event.mdx",
  "94c8b311a58674ea",
  "chat-prompts/learning/what-events-are-relevant-to-this-feature",
  {
    "id": 406,
    "data": 408,
    "body": 416,
    "filePath": 417,
    "digest": 418,
    "deferredRender": 20
  },
  { "title": 409, "type": 316, "inputs": 410, "category": 415 },
  "What events are relevant to this feature?",
  [411],
  { "id": 412, "label": 413, "type": 414 },
  "feature-description",
  "Feature Description",
  "text-area",
  { "id": 323, "label": 323, "icon": 324 },
  "What events are relevant to this feature `{{feature-description}}`?",
  "../examples/default/chat-prompts/learning/what-events-are-relevant-to-this-feature.mdx",
  "2cddc5576f57b1b9",
  "chat-prompts/code/create-lambda-function-to-consume-event",
  {
    "id": 419,
    "data": 421,
    "body": 430,
    "filePath": 431,
    "digest": 432,
    "deferredRender": 20
  },
  { "title": 422, "type": 316, "inputs": 423, "category": 429 },
  "Create AWS Lambda Function to Consume EventBridge Event",
  [424, 426],
  { "id": 319, "label": 425, "type": 321 },
  "Select the EventBridge event the Lambda will consume",
  { "id": 393, "label": 427, "type": 395, "options": 428 },
  "What is the programming language for the Lambda function?",
  [397, 398, 399, 400, 401],
  { "id": 381, "label": 381, "icon": 381 },
  "Generate an AWS Lambda function handler in `{{code-language}}` that consumes the EventBridge event `{{event-name}}`.\n\n**Event Details:**\n\n*   **Event Source:** AWS EventBridge\n*   **Event Name:** `{{event-name}}`\n*   **Payload Location:** The actual event payload (`{{event-name}}`) is located within the `detail` field of the incoming EventBridge event object.\n*   **Payload Format:** Assume the event payload within `detail` is JSON and follows FlowMart's payload standards (containing `metadata` and `data` fields).\n\n**Best Practices to Follow:**\n\n1.  **Parsing & Validation:** Safely parse the incoming EventBridge event object. Extract and validate the `detail` field. Deserialize the JSON payload within `detail` into an appropriate data structure or class.\n2.  **AWS Lambda Powertools:** Utilize AWS Lambda Powertools for `{{code-language}}` to enhance observability and implement best practices:\n    *   **Logging:** Implement structured logging. Include key information like the EventBridge event ID and `metadata.correlationId` in logs.\n    *   **Tracing:** Enable AWS X-Ray tracing integration for request tracing across services.\n    *   **Metrics:** Emit custom business or operational metrics (e.g., successful processing count, processing latency).\n    *   **(Optional) Event Handler:** Consider using Powertools' Event Handler utility (if available for `{{code-language}}`) for simplified event parsing, validation, and routing based on event content.\n    *   **(Optional) Idempotency:** Implement idempotency using Powertools' Idempotency utility to safely handle potential duplicate event deliveries from EventBridge. Use the EventBridge event `id` or `detail.metadata.correlationId` as the idempotency key.\n3.  **Error Handling:** Implement robust error handling within the function code. For transient errors, allow the function to error out to leverage Lambda's built-in retry mechanism. Configure a Dead-Letter Queue (DLQ) on the Lambda function itself (via AWS console, CLI, or IaC) to capture events that fail permanently after retries. *Do not implement DLQ logic within the function code.*\n4.  **Configuration:** Use environment variables for any external configuration (e.g., API endpoints, table names). Do not hardcode configuration values.\n5.  **Permissions:** Note the essential IAM permissions required for the Lambda execution role (e.g., `logs:CreateLogGroup`, `logs:CreateLogStream`, `logs:PutLogEvents`, `xray:PutTraceSegments`, permissions for any AWS services the function interacts with, and potentially KMS permissions if using encryption).\n6.  **Dependencies:** Clearly list all external dependencies (like AWS Lambda Powertools) and provide instructions on how to install and package them with the Lambda function (e.g., using requirements.txt for Python, package.json for TypeScript, pom.xml/build.gradle for Java).\n\n**Task:**\n\nProvide the complete, runnable AWS Lambda function handler code snippet in `{{code-language}}` for the `{{event-name}}` event. The code should:\n*   Include necessary imports and initialization for AWS SDK and Powertools.\n*   Demonstrate parsing the EventBridge event and deserializing the `detail` payload.\n*   Incorporate Powertools for logging, tracing, and metrics as described.\n*   Include basic error handling.\n*   Be ready to be deployed as an AWS Lambda function.\n\nAlso, provide the necessary dependency management file content (e.g., `requirements.txt`, `package.json`) and brief packaging instructions.",
  "../examples/default/chat-prompts/code/create-lambda-function-to-consume-event.mdx",
  "2d88d26baa63003b",
  "chat-prompts/code/create-consumer-code-for-eventbridge-event",
  {
    "id": 433,
    "data": 435,
    "body": 445,
    "filePath": 446,
    "digest": 447,
    "deferredRender": 20
  },
  { "title": 436, "type": 316, "inputs": 437, "category": 443 },
  "Create EventBridge Rule for an Event",
  [438, 440],
  { "id": 319, "label": 439, "type": 321 },
  "Select the event you want to create a rule for",
  { "id": 441, "label": 442, "type": 316 },
  "rule-name",
  "Enter a name for the EventBridge rule",
  { "id": 444, "label": 444, "icon": 444 },
  "CloudFormation",
  "Generate an AWS CloudFormation template snippet (YAML) to create an EventBridge rule. This rule should trigger based on the selected event `{{event-name}}` from the default EventBus and invoke the specified Lambda function `{{target-lambda-arn}}`.\n\n**Rule Details:**\n\n*   **Rule Name:** `{{rule-name}}` (e.g., `trigger-lambda-on-{{event-name}}`)\n*   **Event Bus:** Use the `default` event bus.\n*   **Event Pattern:** The rule should match events where the `detail-type` is exactly `{{event-name}}`.\n*   **Target:** The primary target is the Lambda function with ARN `{{target-lambda-arn}}`.\n*   **Permissions:** Ensure the necessary permissions are granted for EventBridge to invoke the target Lambda function.\n\n**Best Practices to Follow:**\n\n1.  **Clarity:** Use clear and descriptive names for resources.\n2.  **Specificity:** Define the event pattern precisely to avoid unintended triggers.\n3.  **Permissions:** Include the `AWS::Lambda::Permission` resource to grant invocation rights.\n4.  **Least Privilege:** Grant only the necessary permissions.\n\n**Task:**\n\nProvide the complete, runnable CloudFormation YAML snippet for the `{{rule-name}}` EventBridge rule, incorporating the details and best practices mentioned above. Include the `AWSTemplateFormatVersion` and `Resources` sections.",
  "../examples/default/chat-prompts/code/create-consumer-code-for-eventbridge-event.mdx",
  "dbd9fb7cd1b1f6c1",
  "chat-prompts/code/create-producer-code-for-event",
  {
    "id": 448,
    "data": 450,
    "body": 458,
    "filePath": 459,
    "digest": 460,
    "deferredRender": 20
  },
  { "title": 451, "type": 316, "inputs": 452, "category": 457 },
  "Create Kafka Producer Code for Event",
  [453, 455],
  { "id": 319, "label": 454, "type": 321 },
  "Select the event you want to create a producer for",
  { "id": 393, "label": 394, "type": 395, "options": 456 },
  [397, 398, 399, 400, 401],
  { "id": 381, "label": 381, "icon": 381 },
  "Generate Kafka producer code in `{{code-language}}` that produces the event `{{event-name}}`.\n\n**Kafka Cluster Details:**\n\n*   **Bootstrap Servers:** `kafka-prod-1.flowmart.internal:9092,kafka-prod-2.flowmart.internal:9092,kafka-prod-3.flowmart.internal:9092`\n*   **Topic Name:** The topic name follows the pattern `fm.events.\u003Cevent-name>` (e.g., `fm.events.UserSignedUp` for the `UserSignedUp` event).\n*   **Security Protocol:** SASL_SSL\n*   **SASL Mechanism:** PLAIN\n*   **Required Acknowledgements (acks):** `all` (Ensure highest durability).\n\n**Best Practices to Follow:**\n\n1.  **Serialization:** Assume the event payload needs to be serialized to JSON and follow FlowMart's payload standards (with `metadata` and `data` fields). The `metadata` should include a unique `eventId` (UUID) and `timestamp` (ISO 8601).\n2.  **Error Handling:** Implement robust error handling for connection issues, serialization failures, and send failures. Consider retry mechanisms with backoff for transient errors.\n3.  **Configuration:** Externalize Kafka connection details (don't hardcode them directly in the main logic).\n4.  **Logging:** Add basic logging for successful message production and any errors encountered.\n5.  **Asynchronous Sending:** Use asynchronous send calls for better performance, but ensure proper handling of send callbacks/futures for error checking and logging.\n6.  **Partitioning:** Briefly mention how to set a message key (e.g., using `metadata.correlationId` or a relevant business ID) if specific partitioning is desired for ordering guarantees within a partition.\n\n**Task:**\n\nProvide the complete, runnable Kafka producer code snippet in `{{code-language}}` for the `{{event-name}}` event, incorporating the cluster details and best practices mentioned above. Include necessary imports, basic setup, and an example of how to construct and send an event message.\n\nIf you use any external libraries, please include the import statements and how to install them, step by step, make sure dependencies are listed first.",
  "../examples/default/chat-prompts/code/create-producer-code-for-event.mdx",
  "cb630df6440b2e3f",
  "customPages",
  [
    "Map",
    463,
    464,
    480,
    481,
    495,
    496,
    508,
    509,
    523,
    524,
    533,
    534,
    544,
    545,
    554,
    555,
    577,
    578,
    588,
    589,
    598,
    599,
    608,
    609,
    618,
    619,
    628,
    629,
    638,
    639,
    648,
    649,
    658,
    659,
    668,
    669,
    678,
    679,
    688,
    689,
    698,
    699,
    708,
    709,
    717,
    718,
    727,
    728,
    737,
    738,
    746,
    747,
    755,
    756,
    765,
    766,
    774,
    775,
    784,
    785,
    794,
    795,
    803,
    804
  ],
  "docs/guides/event-storming/01-index",
  {
    "id": 463,
    "data": 465,
    "body": 477,
    "filePath": 478,
    "digest": 479,
    "deferredRender": 20
  },
  {
    "title": 466,
    "summary": 467,
    "sidebar": 468,
    "owners": 471,
    "badges": 473
  },
  "Getting Started with Event Storming",
  "Learn how to use Event Storming to discover and model your business domain effectively",
  { "label": 469, "order": 470 },
  "Introduction",
  1,
  [472],
  { "id": 42 },
  [474],
  { "content": 475, "backgroundColor": 476, "textColor": 476 },
  "Guide",
  "teal",
  "# Introduction to Event Storming\n\nEvent Storming is a collaborative modeling technique that helps teams explore complex business domains. It was introduced by Alberto Brandolini and has become a valuable tool in Domain-Driven Design (DDD) and Event-Driven Architecture (EDA).\n\n## Why Event Storming?\n\nAt FlowMart, we use Event Storming because it:\n\n- Brings together domain experts and technical teams\n- Helps identify domain events, commands, and aggregates\n- Aligns perfectly with our event-driven architecture\n- Facilitates better understanding of business processes\n- Helps in defining service boundaries and responsibilities\n\n## Example Event Storming Session\n\nHere is an example of an event storming session for the Orders domain.\n\n\u003CMiro boardId=\"uXjVIHCImos=/\" moveToWidget=\"3074457347671667709\" edit={false} />\n\n## Event Storming Example in Lucid Chart\n\n\u003CLucid diagramId=\"e29f42a0-67e2-4f80-b0d7-6922bb7dd9c5\" />\n\n## Git workflow \n\n\u003CDrawIO url=\"https://viewer.diagrams.net/?border=0&tags=%7B%7D&lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=Untitled%20Diagram.drawio&dark=auto#R%3Cmxfile%3E%3Cdiagram%20id%3D%22yPxyJZ8AM_hMuL3Unpa9%22%20name%3D%22complex%20gitflow%22%3E7V1dm6I2FP41c6kP4ZvL0dHd7bbbbWf7tNubPhmJSheIhTjj7K9vwoeiiRgREGfZi1kJECDvez5yzoHcaeNg8y6Cq%2BUv2EX%2Bnaq4mzvt4U5VgaIq9D%2FW8pq2mEBLGxaR52YH7Roeve8oPzNrXXsuivcOJBj7xFvtN85wGKIZ2WuDUYRf9g%2BbY3%2F%2Fqiu4QFzD4wz6fOufnkuWWStQlN2O98hbLLNL20a2I4D5wVlDvIQufik0aZM7bRxhTNJfwWaMfDZ4%2Bbik502P7N3eWIRCInPC%2Bs8%2Fwk%2BTB%2FP9xFfhR2Puwg9PAyvt5Rn66%2ByB32My9za0bYyDwCPZvZPXfEAI2tDLjZYk8GkDoD9jEuFvaIx9HNGWEIf0yNHc8%2F2DJuh7i5BuzugNI9o%2BekYR8ehQ32c7As912WVGL0uPoMcVnLFrvlBi0TZMj577yQAu6XGInjDihyAbFdYz2hSasiF5h3CASPRKD8n26qo1VI30rIyilKTp9ssOcDsDcVmAWsvaYEaxxbbzHQr0RwbEGaDYHCgcCsilLM02cUSWeIFD6E92raMIr0MXscso%2B3D9iwh5zQQNrglmY7vt4WeMV9lxKx964eApuYcRCt17Jk07NGnL1GPPlfSfciCXEJ1jxR0dreRfciYdpL%2ByE5ONr2xjaOSbD5vizofX4tZnFHl0lBl%2FksajFIjxOpqhklHONQyB0QJl5zrL0cfPD9%2BxR15WH0bu9%2BnDb9YAZHRgYy5DKdVROUoBcJwrWUefsUcfYUdM%2FXQv6a1nJx6Qbntf1XnoVKDdbB09J6wDZRxEG4%2F8VfhdIADdOsT4JGH4E8rZKisCPKvnOCTZicConX7iAw1J%2BkXIh8R73jdctSun%2FL7LtFOu2H%2BGT8j%2FjGOPeJgp%2BCdMCA4Emp%2BwIedtyp4%2BWcIV6z7YLJinMWS2YLaEERnicMCs8D8RmiOKAxv1feVFT3A9ClCJiaL6aWRMtCkq1Vw12BvV5MQaOLy90Vq1NxKQXiT5QuESCGFRQ5xtcLpiVnIv9aRcK07dgi22JJamD3Vn9%2B%2B6RgUoxvXJVWp%2BdjxhG6%2BFjbMN0zmUO5%2FxzBhNYeD5rN8xJaZHO1OVT%2Bgl27mnwxynXIfVyWy9WyYr19I3b7KOmyaRMZvUY7JMtXsma6s83yCmGkUV3DeLqW2C7mGq1uGGXDzvFXkW5Zr5KB4SmlJSUeag1jK1BHmM6lgXqSlozgXQOKADKicc2F0KNRUnftuJYHar4JQtrkNgtQNhVThhNQTCqqtNCaterxd3QXDgBub6hqw%2BsLrlOBm3Y2RX62jl1%2BE6PRgTO7mLGqRW05yhdaCvbXBlM2t2RnLfYljPkhX12mf%2Fl7GCzwT1on5WIkk3uyfqdi%2FqDYq6IyvqoFuifkNT506KuqF3z6qrfAwfDMFQGVBOIRijBiZXPpqTNzG1snk0%2BVCIqgvgVJuC05IIWZ4bC2klC1NdnZqy6tSSDZq0o06tKvmMGpDJLOp2o9XUw56T0ALmHUuCOzVHP%2FocVks5LGnC2WqnCGf2PtuF0zPATc8088o%2Bm1VLod%2B16y6qC6MlWwJldSssat1QCVQnhdEAeueEMc9%2B3rYwtmxOLdlop9Mt%2F83qo50XSrDTPQkGSs3hzkumZT%2BEN%2B1I5zUV2Rr6luZvfWLzwpowp4PuNG%2FBXfSMfApLH%2Fw8Gvy0eBx1DkcH8Dg2FvvMQ%2Bk%2FUOwzjzacDkt0q7I2v%2B9ej1b1oxTlUP50nS%2FtancmZF3fjeqDm1W0iC6pRTo2G8vvu9ciVb0xADqnRWzeG%2Fvpw%2B%2F3A80YDNAGBmwUVWWOIFlHfXa61EE7RNbgkG03OW3z0ZMvS4Zm5m4HKKk2x2wc4lWU1p4%2FRTCcLW8UaKWKvj4DY4Nzwm3eCW8ZZD458ZWyuyizPaZnYaqbVxdcPouYCu62SKiHtPSlKU5MlWsXCjl8LiqFNH2jZoun6bPRf6Lyay7Idjh6hLlSMO5jMLwfBQqlvu2ALBMR2c1OZj6MY29W5v6e9FzOHuSTc4kTbyrlbRe%2B7E4lktO7hp67UCdecOM%2FwmIIejvoq%2BmPsEiEWHrs0xMECtrSqyGvWaf7ahp5%2FjXJHnkxWipf3aPmCf1zkVf51AbXV%2BPfyOAV%2Fhe4uNPuaVtS1t2A%2BW7uXdkaDLPhKNz76pbAMoM2fS%2Bg8BJagEn58WBS%2BY%2BWiWCy2kVJotijr%2Bw59g2Xm8vtb%2B%2B8DydXlGJBcv%2Fq7zcBUOU1i7645xINYEprgI69y6xI1HL2GqBEA1j8TKoDGoBHteBtqT%2Bet2UD3tsy7Ws7xTJfEmhp2pqXEexKB77eFSoHhGUEUgpz%2Fhj%2BbTyHX7%2FE%2Ftj4BKOPQfTPQDap3s6U2ALcF75MzagaCBP0piuNhUOEw9vHQGWhtxWuHECzKgIv6kttLAIqBF7kzqfpjHgFwz0GmP%2Bt2ef0R0zxDzIdzsxDpsa3%2B%2FMsyBQlyxXMI8RSJtTfow%2FluvRviAmKE3SSRCd8wms2KCRJrawiPEMx243n9E%2BAooUXLthjhEkXRJhTG4qSMGkLHZb0UW4qN6PK5Gbq%2B3KyJYrI8hlVYGpbTVWULdM8LkayBq9M7Qvo6XrPQnbyPNwndCPcPMzao3h4lH7b5uQJelKW1W%2BqQ3P%2F03y6zZNSB7aQlFpTpJR4XbiFEmoRMHWuCFHdV8u8ZImprSZp2i%2FNYAg%2Bsmsf%2BOhH7Cx1eeBr4bAVOyCudKmjOZGTp9Af6X3UavpFL0hXM%2F2MLArj3yBOCJlMGsFqI1THE8hKJig7FgmwqwjFKBlSBaYq1l3PknDB1sof16U3oTVBu1rT0Q9qGoEhmLaqouiCfbbC%2FCNG0a9P%2F7LFoVTFZ9Ge9NTJtgjV9eAigkFCkCgD%2BQVH35IRo%2Bh5dH4Ao9wI7%2BjyziOD9JgdgZSEWuwWfS%2F8ll5pSQhbreqe3aE6DZ89NJxhernpCseMVVM4iNczZrbna3%2BwoL2mJpoa7kGQrqg1TfvMhUW4To1aGPNMuHjiidhxqLEPqltzkpg1RS04VSL4li1blqmmEhu6WaBAKSVyIWYEWEZovqdY6kHxUNXU1Guue%2BC%2B4mmdg1oNHKyBY9tswevBdjF4abTOr0eErqcerG5Aw4u%2FEBy1QfHnfA%2Fxsl8SacQ%2B4JRiavGL6B06r9IBJ0FfupwjXMHrFCN%2FfEr%2FViJOfQyqmvIy%2BPUXBHkxYIq9F7tEsmSn%2B2LXrxuvTB9%2BW%2FcWFvEqXfLxCp%2F2FWtFw%2BTq3xpcuKt0TG4hqX5kxR2qDSb35sg0Oe3FltexDd0Q5NWn9ngyHtejP3Ref7S57I4QV5PXHXtFjIBD%2BY2n1XXVPgxPNFfDSDd3S0CnorpbSFub%2FA8%3D%3C%2Fdiagram%3E%3C%2Fmxfile%3E\" />\n\n## Key Concepts\n\n### Domain Events\n- Represent something significant that has happened in the business domain\n- Written in past tense (e.g., \"Order Placed\", \"Payment Received\")\n- Captured on orange sticky notes during the session\n\n### Commands\n- Triggers that cause domain events\n- Written in imperative form (e.g., \"Place Order\", \"Process Payment\")\n- Captured on blue sticky notes\n\n### Aggregates\n- Business entities that ensure consistency\n- Group related events and commands\n- Captured on yellow sticky notes\n\n### Policies\n- Business rules that react to events\n- Automated processes or human workflows\n- Captured on purple sticky notes\n\n## Event Storming Levels\n\n1. **Big Picture Event Storming**\n   - High-level view of the entire business domain\n   - Focuses on major events and workflows\n   - Helps identify bounded contexts\n\n2. **Process Level Event Storming**\n   - Detailed view of specific processes\n   - Includes commands, policies, and external systems\n   - Helps design individual services\n\n3. **Software Design Level Event Storming**\n   - Technical perspective of the domain\n   - Includes aggregates, entities, and value objects\n   - Leads to implementation decisions\n\n## Benefits for FlowMart\n\nEvent Storming has helped us:\n- Design our microservices architecture\n- Identify service boundaries\n- Define event contracts between services\n- Improve collaboration between teams\n- Reduce misunderstandings and rework\n\n## Prerequisites for Event Storming\n\nTo conduct an effective Event Storming session, you'll need:\n\n- A large modeling space (physical or virtual)\n- Sticky notes in different colors\n- Domain experts and technical team members\n- A skilled facilitator\n- 2-4 hours of uninterrupted time\n\n## Next Steps\n\nContinue reading to learn:\n- [How to Facilitate an Event Storming Session](/docs/guides/event-storming/02-facilitation)\n- [From Event Storming to Implementation](/docs/guides/event-storming/03-implementation)",
  "../examples/default/docs/guides/event-storming/01-index.mdx",
  "e328de565374d25c",
  "docs/guides/event-storming/02-facilitation",
  {
    "id": 480,
    "data": 482,
    "body": 492,
    "filePath": 493,
    "digest": 494,
    "deferredRender": 20
  },
  {
    "title": 483,
    "summary": 484,
    "sidebar": 485,
    "owners": 488,
    "badges": 490
  },
  "Facilitating an Event Storming Session",
  "A comprehensive guide on how to run effective Event Storming workshops at FlowMart",
  { "label": 486, "order": 487 },
  "Facilitation Guide",
  2,
  [489],
  { "id": 42 },
  [491],
  { "content": 475, "backgroundColor": 476, "textColor": 476 },
  "# Facilitating an Event Storming Session\n\nThis guide will help you run effective Event Storming sessions at FlowMart, ensuring you get the most value from this collaborative modeling technique.\n\n## Pre-Session Preparation\n\n### 1. Define the Scope\n- Identify the business domain or process to explore\n- Set clear objectives for the session\n- Determine the appropriate level (Big Picture, Process, or Design)\n\n### 2. Invite the Right People\n- Domain experts who understand the business processes\n- Technical team members who will implement the solution\n- Product owners and stakeholders\n- Limit to 8-12 participants for optimal interaction\n\n### 3. Prepare the Space\n- Large continuous wall space or virtual whiteboard\n- Sticky notes in different colors:\n  - Orange: Domain Events\n  - Blue: Commands\n  - Yellow: Aggregates\n  - Purple: Policies\n  - Pink: External Systems\n  - Red: Problems/Questions\n\n## Running the Session\n\n### 1. Introduction (15 minutes)\n- Explain Event Storming concepts and notation\n- Set ground rules:\n  - No laptops/phones unless necessary\n  - Everyone participates\n  - No wrong answers\n  - Focus on the business process\n\n### 2. Event Discovery (45-60 minutes)\n- Start with \"What happens in this domain?\"\n- Let participants write domain events on orange stickies\n- Place events on the timeline (left to right)\n- Don't worry about order initially\n\n### 3. Timeline Organization (30 minutes)\n- Review all events as a group\n- Organize events chronologically\n- Identify missing events\n- Group related events together\n\n### 4. Adding Detail (60-90 minutes)\n- Add commands (blue) that trigger events\n- Identify external systems (pink)\n- Mark problem areas (red)\n- Add policies and reactions (purple)\n\n### 5. Identifying Boundaries (45 minutes)\n- Group related concepts\n- Look for natural service boundaries\n- Discuss integration points\n- Identify aggregates (yellow)\n\n## Common Challenges and Solutions\n\n### Challenge: Dominant Participants\n- Actively engage quieter participants\n- Use round-robin techniques\n- Split into smaller groups temporarily\n\n### Challenge: Too Much Detail\n- Keep focus on relevant abstraction level\n- Park detailed discussions for later\n- Use \"parking lot\" for important but off-topic items\n\n### Challenge: Losing Focus\n- Take regular breaks (10 minutes every hour)\n- Use timeboxing for each activity\n- Keep referring back to session goals\n\n## Remote Facilitation Tips\n\nWhen running remote Event Storming sessions:\n\n- Use tools like Miro or Mural\n- Pre-create templates and sticky note colors\n- Use breakout rooms for small group discussions\n- Schedule more frequent but shorter sessions\n- Use video to maintain engagement\n\n## Post-Session Activities\n\n1. **Documentation**\n   - Photograph or export the board\n   - Capture key insights and decisions\n   - Document identified bounded contexts\n\n2. **Follow-up**\n   - Schedule deep-dive sessions for specific areas\n   - Create action items and assign owners\n   - Plan next steps for implementation\n\n3. **Review and Refine**\n   - Review findings with stakeholders\n   - Validate assumptions\n   - Plan additional sessions if needed\n\n## Measuring Success\n\nA successful Event Storming session should:\n- Create shared understanding\n- Identify key domain events and processes\n- Highlight potential problems and solutions\n- Generate actionable next steps\n- Engage all participants effectively\n\n## Next Steps\n\nContinue to [From Event Storming to Implementation](/docs/guides/event-storming/03-implementation) to learn how to turn your Event Storming insights into working software.",
  "../examples/default/docs/guides/event-storming/02-facilitation.mdx",
  "0c36be94ef209ba3",
  "docs/guides/creating-new-microservices/01-index",
  {
    "id": 495,
    "data": 497,
    "body": 505,
    "filePath": 506,
    "digest": 507,
    "deferredRender": 20
  },
  {
    "title": 498,
    "summary": 499,
    "sidebar": 500,
    "owners": 501,
    "badges": 503
  },
  "Creating new microservices",
  "A comprehensive guide to creating new microservices at FlowMart following our best practices and standards",
  { "label": 469, "order": 470 },
  [502],
  { "id": 42 },
  [504],
  { "content": 475, "backgroundColor": 476, "textColor": 476 },
  "Welcome to the FlowMart microservices creation guide. This documentation will help you understand our microservices architecture and guide you through the process of creating, deploying, and maintaining new services that align with our architectural standards and best practices.\n\n## FlowMart's Microservices Architecture\n\nAt FlowMart, we've adopted an [Event-**Driven** Architecture (EDA)](/docs/architecture-records/published/02-eda-adoption) for our e-commerce platform. Our architecture consists of domain-oriented microservices that communicate primarily through events, with Apache Kafka serving as our event backbone.\n\nKey architectural principles we follow:\n\n- **Domain-Driven Design**: **Services** are organized around business domains with clear bounded contexts\n- **Service Autonomy**: Each service owns its data and can be independently deployed\n- **Event-First Communication**: Services publish events when state changes and subscribe to events from other domains\n- **Eventual Consistency**: We prioritize availability and partition tolerance over immediate consistency\n- **API-First Development**: All services expose well-defined APIs using standardized patterns\n\n## Example service in our architecture (InventoryService)\n\u003CNodeGraph id=\"InventoryService\" version=\"0.0.2\" type=\"service\"  />\n\n## Getting Started\n\nBefore creating a new microservice, please familiarize yourself with our architectural decisions:\n\n1. [Event-Driven Architecture Adoption](/docs/architecture-records/published/02-eda-adoption)\n2. [API Gateway Pattern Adoption](/docs/architecture-records/published/01-api-gateway-pattern)\n3. [CI/CD and Deployment Strategy](/docs/architecture-records/drafts/02-cicd-deployment-strategy)\n4. [API Management and Governance](/docs/architecture-records/drafts/03-api-management-governance)\n\n## Service Creation Process\n\nThe following steps outline our microservice creation process:\n\n1. **Domain Analysis**: Identify the business domain and define the bounded context\n2. **Service Definition**: Create service specification using our templating tools\n3. **Infrastructure Provisioning**: Use our Terraform modules to provision required infrastructure\n4. **Service Implementation**: Develop the service following our technological standards\n5. **CI/CD Pipeline Setup**: Configure the service pipeline for continuous integration and deployment\n6. **Testing**: Implement comprehensive testing (unit, integration, contract, end-to-end)\n7. **Documentation**: Document API contracts, event schemas, and service behaviors\n8. **Deployment**: Deploy to production using our GitOps workflow\n\n## Available Service Templates\n\nWe provide several starter templates for new microservices:\n\n- [Node.js Service Template](/docs/guides/creating-new-microservices/node-service)\n- [TypeScript Service Template](/docs/guides/creating-new-microservices/typescript-service)\n- [Java Spring Boot Template](/docs/guides/creating-new-microservices/java-spring-boot)\n- [Python FastAPI Template](/docs/guides/creating-new-microservices/python-fastapi)\n\n## Infrastructure as Code\n\nAll FlowMart infrastructure is managed using [Terraform](/docs/guides/creating-new-microservices/terraform-modules). We maintain a set of reusable modules for common infrastructure components, which you should leverage when creating new services.\n\n## Development Workflow\n\n1. **Engage with the Platform Team**: Discuss your new service requirements with the platform team\n2. **Create Service Repository**: Use our GitLab template to create a new service repository\n3. **Setup Local Environment**: Configure your local development environment\n4. **Implement Core Functionality**: Develop the service capabilities\n5. **Review and Testing**: Submit for architecture and code review\n6. **Deploy to Production**: Use our deployment pipeline for production rollout\n\n## Support\n\nIf you need assistance creating a new microservice, please reach out to:\n\n- **Platform Engineering Team**: For infrastructure and deployment questions\n- **Architecture Team**: For design and architecture guidance\n- **DevOps Team**: For CI/CD and operational support\n\n## Next Steps\n\n- Continue to [Service Design Principles](/docs/guides/creating-new-microservices/service-design-principles)\n- Learn about [Node.js Service Creation](/docs/guides/creating-new-microservices/node-service)\n- Explore our [Terraform Modules](/docs/guides/creating-new-microservices/terraform-modules)",
  "../examples/default/docs/guides/creating-new-microservices/01-index.mdx",
  "a32065e8822c5d5b",
  "docs/guides/event-storming/03-implementation",
  {
    "id": 508,
    "data": 510,
    "body": 520,
    "filePath": 521,
    "digest": 522,
    "deferredRender": 20
  },
  {
    "title": 511,
    "summary": 512,
    "sidebar": 513,
    "owners": 516,
    "badges": 518
  },
  "From Event Storming to Implementation",
  "Learn how to transform Event Storming outcomes into concrete microservice implementations at FlowMart",
  { "label": 514, "order": 515 },
  "Implementation",
  3,
  [517],
  { "id": 42 },
  [519],
  { "content": 475, "backgroundColor": 476, "textColor": 476 },
  "# From Event Storming to Implementation\n\nThis guide will help you transform the insights gained from Event Storming sessions into concrete microservice implementations at FlowMart.\n\n## Translating Event Storming Artifacts\n\n### Domain Events to Event Schema\nConvert orange sticky notes (domain events) into event schemas:\n\n```typescript\n// Example Event Schema for \"OrderPlaced\"\ninterface OrderPlacedEvent {\n  eventType: 'OrderPlaced';\n  orderId: string;\n  customerId: string;\n  orderItems: OrderItem[];\n  totalAmount: number;\n  timestamp: string;\n}\n```\n\n### Commands to API Endpoints\nTransform blue sticky notes (commands) into API endpoints:\n\n```typescript\n// Example API endpoint for \"PlaceOrder\"\n@POST\n@Path(\"/orders\")\nasync placeOrder(orderRequest: OrderRequest): Promise\u003COrderResponse> {\n  // Implementation\n}\n```\n\n### Aggregates to Service Boundaries\nConvert yellow sticky notes (aggregates) into service definitions:\n\n```typescript\n// Example Order Service\n@Service\nclass OrderService {\n  private readonly orderRepository: OrderRepository;\n  private readonly eventPublisher: EventPublisher;\n  \n  async createOrder(order: Order): Promise\u003COrder> {\n    // Implementation\n  }\n}\n```\n\n## Implementation Steps\n\n### 1. Define Service Boundaries\n- Use bounded contexts identified during Event Storming\n- Create new service repositories using our [templates](/docs/guides/creating-new-microservices)\n- Define service interfaces and contracts\n\n### 2. Design Event Schemas\n- Create event schemas for all domain events\n- Use our [schema registry](/docs/technical-architecture-design/schema-registry)\n- Version schemas appropriately\n- Document event ownership and consumers\n\n### 3. Implement Commands\n- Create API endpoints for commands\n- Implement command handlers\n- Add validation and error handling\n- Document API contracts using OpenAPI\n\n### 4. Set Up Event Publishing\n- Implement event publishers\n- Configure Kafka topics\n- Set up dead letter queues\n- Implement retry mechanisms\n\n### 5. Create Event Subscribers\n- Implement event handlers\n- Set up consumer groups\n- Handle failure scenarios\n- Implement idempotency\n\n## Example Implementation Flow\n\nHere's a typical flow from Event Storming to implementation:\n\n1. **Event Storming Identifies**:\n   - Domain Event: \"OrderPlaced\"\n   - Command: \"PlaceOrder\"\n   - Aggregate: \"Order\"\n\n2. **Create Service Structure**:\n```typescript\n// Service structure based on Event Storming\nsrc/\n  ├── domain/\n  │   ├── Order.ts\n  │   └── OrderItem.ts\n  ├── commands/\n  │   └── PlaceOrderCommand.ts\n  ├── events/\n  │   └── OrderPlacedEvent.ts\n  └── api/\n      └── OrderController.ts\n```\n\n3. **Implement Domain Model**:\n```typescript\nclass Order {\n  private readonly items: OrderItem[];\n  private status: OrderStatus;\n\n  placeOrder(): OrderPlacedEvent {\n    // Implementation\n    return new OrderPlacedEvent(this);\n  }\n}\n```\n\n## Best Practices\n\n### Event Design\n- Use past tense for event names\n- Include all necessary context\n- Follow our [event naming conventions](/docs/technical-architecture-design/event-naming)\n- Version events appropriately\n\n### Service Design\n- Keep services focused on single bounded context\n- Implement proper error handling\n- Add comprehensive logging\n- Include monitoring and metrics\n\n### Testing Strategy\n- Unit test domain logic\n- Integration test event flows\n- Contract test event schemas\n- End-to-end test critical paths\n\n## Common Pitfalls\n\n1. **Too Many Events**\n   - Solution: Focus on meaningful state changes\n   - Combine related events when appropriate\n\n2. **Tight Coupling**\n   - Solution: Use event-driven communication\n   - Avoid direct service-to-service calls\n\n3. **Missing Context**\n   - Solution: Include necessary data in events\n   - Document event purpose and usage\n\n## Monitoring and Observability\n\nImplement proper monitoring for:\n- Event processing metrics\n- Command execution times\n- Error rates and types\n- Service health indicators\n\n## Example: Order Processing Flow\n\nHere's a complete example of implementing an order processing flow:\n\n1. **Event Storming Identified**:\n   - Command: \"PlaceOrder\"\n   - Event: \"OrderPlaced\"\n   - Event: \"PaymentProcessed\"\n   - Policy: \"Send Order Confirmation\"\n\n2. **Implementation**:\n```typescript\n// Command Handler\nasync function handlePlaceOrder(command: PlaceOrderCommand) {\n  const order = new Order(command.orderDetails);\n  const event = order.placeOrder();\n  await eventPublisher.publish('order-events', event);\n  return order;\n}\n\n// Event Handler\nasync function handleOrderPlaced(event: OrderPlacedEvent) {\n  await paymentService.processPayment(event.orderId);\n  await notificationService.sendConfirmation(event.orderId);\n}\n```\n\n## Next Steps\n\n1. Review our [service templates](/docs/guides/creating-new-microservices)\n2. Study our [event schema guidelines](/docs/technical-architecture-design/event-schemas)\n3. Explore our [monitoring setup](/docs/operations-and-support/monitoring)\n\nRemember: Event Storming is an iterative process. As you implement and learn more about the domain, you may need to revisit and refine your model. Keep the dialogue open between domain experts and developers throughout the implementation phase.",
  "../examples/default/docs/guides/event-storming/03-implementation.mdx",
  "61417f2a99a30656",
  "docs/guides/creating-new-microservices/02-service-design-principles",
  {
    "id": 523,
    "data": 525,
    "body": 530,
    "filePath": 531,
    "digest": 532,
    "deferredRender": 20
  },
  { "title": 526, "summary": 527, "sidebar": 528 },
  "Microservice Design Principles",
  "Core design principles and best practices for creating well-structured microservices at FlowMart",
  { "label": 529, "order": 487 },
  "Design Principles",
  "At FlowMart, we follow a set of design principles that guide the creation of new microservices. These principles help ensure our services are maintainable, scalable, and aligned with our overall architectural vision.\n\n## 1. Domain-Driven Design\n\nEach microservice should be aligned with a specific business domain or subdomain. We follow Domain-Driven Design (DDD) principles to identify service boundaries:\n\n### Bounded Contexts\n\n- Define clear bounded contexts for each service\n- Maintain a separate ubiquitous language within each context\n- Document domain models and context maps\n\n### Example Domains at FlowMart\n\n| Domain | Description | Example Services |\n|--------|-------------|------------------|\n| Order | Order processing and management | order-service, order-history-service |\n| Inventory | Product inventory management | inventory-service, stock-management-service |\n| Customer | Customer accounts and profiles | customer-service, authentication-service |\n| Payment | Payment processing and refunds | payment-service, refund-service |\n| Shipping | Shipping and logistics | shipping-service, tracking-service |\n| Catalog | Product information management | product-service, search-service |\n\n## 2. Single Responsibility\n\nEach microservice should have a single responsibility and a clear purpose:\n\n- **Focus on one business capability**: Services should do one thing well\n- **Right-sized services**: Not too large (mini-monolith) or too small (nano-service)\n- **Cohesive functionality**: Related functions should be grouped together\n\n## 3. Data Ownership\n\nMicroservices should own their data and maintain data autonomy:\n\n- Each service has its own database or data store\n- No direct data sharing between services\n- Data is exposed through well-defined APIs\n- Services should be the single source of truth for their domain data\n\n## 4. API Design\n\nAll FlowMart microservices must follow our [API Management and Governance Strategy](/docs/architecture-records/drafts/03-api-management-governance):\n\n### RESTful APIs\n\n- Use consistent resource naming conventions\n- Follow standard HTTP methods and status codes\n- Implement proper error handling and validation\n- Design for backward compatibility\n\n### Event-Driven Interfaces\n\n- Define clear event schemas using AsyncAPI\n- Document event ownership and responsibilities\n- Follow event versioning standards\n- Implement idempotent event consumers\n\n## 5. Resilience and Fault Tolerance\n\nMicroservices must be designed to handle failures gracefully:\n\n- Implement circuit breakers for downstream dependencies\n- Use timeouts and retries with exponential backoff\n- Design for graceful degradation of functionality\n- Implement health checks and readiness probes\n\n## 6. Observability\n\nAll services must expose monitoring and observability data:\n\n- Structured logging (using our ELK stack)\n- Metrics exposure (Prometheus format)\n- Distributed tracing support (Jaeger)\n- Health check endpoints\n\n## 7. Security by Design\n\nSecurity must be integrated into every service:\n\n- Authentication using OAuth 2.0 / OpenID Connect\n- Authorization using role-based access control\n- TLS encryption for all communications\n- Input validation and output encoding\n- No sensitive data in logs or traces\n\n## 8. Testability\n\nServices should be designed with testing in mind:\n\n- High unit test coverage (minimum 80%)\n- Integration tests for all critical paths\n- Contract tests for API interfaces\n- Easy local testing setup\n- Simulated dependencies for development\n\n## 9. Configuration Management\n\nServices should follow our configuration management approach:\n\n- Environment-specific configuration via Kubernetes ConfigMaps\n- Secrets management via HashiCorp Vault\n- Feature flags for conditional functionality\n- No hardcoded configuration values\n\n## 10. Independence and Deployability\n\nServices should be independently deployable:\n\n- No deployment coupling with other services\n- Infrastructure as Code for all resources\n- Self-contained CI/CD pipelines\n- Blue/green or canary deployment capabilities\n\n## Microservice Checklist\n\nUse this checklist when designing a new service:\n\n- [ ] Service aligns with a specific business domain\n- [ ] Clear bounded context defined\n- [ ] Service owns its data\n- [ ] APIs follow company standards\n- [ ] Event schemas are documented\n- [ ] Resilience patterns implemented\n- [ ] Observability instrumentation added\n- [ ] Security controls integrated\n- [ ] Comprehensive test suite created\n- [ ] Configuration externalized\n- [ ] Independent deployment pipeline configured\n\n## Next Steps\n\n- Learn how to [create a Node.js microservice](/docs/guides/creating-new-microservices/node-service)\n- Explore [TypeScript service implementation](/docs/guides/creating-new-microservices/typescript-service)\n- Understand our [Terraform infrastructure modules](/docs/guides/creating-new-microservices/terraform-modules)",
  "../examples/default/docs/guides/creating-new-microservices/02-service-design-principles.mdx",
  "2459b44edae636ab",
  "docs/guides/creating-new-microservices/03-database-patterns",
  {
    "id": 533,
    "data": 535,
    "body": 541,
    "filePath": 542,
    "digest": 543,
    "deferredRender": 20
  },
  { "title": 536, "summary": 537, "sidebar": 538 },
  "Database Patterns for Microservices",
  "Best practices and patterns for managing data in FlowMart microservices",
  { "label": 539, "order": 540 },
  "Database Patterns",
  5,
  "This guide outlines the database patterns and best practices for managing data in FlowMart's microservices architecture.\n\n## Database Per Service Pattern\n\nAt FlowMart, we follow the **Database Per Service** pattern as our primary data management strategy:\n\n- **Definition**: Each microservice owns and manages its database exclusively\n- **Access Pattern**: Only the service that owns the database can perform direct read/write operations\n- **Purpose**: Ensures loose coupling, independent scaling, and domain isolation\n\n![Database Per Service Diagram](/docs/images/database-per-service.png)\n\n### Why This Pattern?\n\n1. **Service Independence**: Services can be developed, deployed, and scaled independently\n2. **Technology Freedom**: Teams can choose the most appropriate database technology for their service needs\n3. **Resilience**: Database failures are isolated to individual services\n4. **Security**: Clear data ownership boundaries limit the blast radius of security incidents\n5. **Performance**: Database schemas and indexes can be optimized for specific service requirements\n\n## Supported Database Technologies\n\nFlowMart supports the following database technologies for microservices:\n\n| Database Type | Technology | Best For | Support Level |\n|--------------|------------|----------|---------------|\n| Document | MongoDB | Flexible schemas, rapid development | Primary |\n| Relational | PostgreSQL | Complex transactions, structured data | Primary |\n| Key-Value | Redis | Caching, session management, rate limiting | Primary |\n| Search | Elasticsearch | Full-text search, analytics | Secondary |\n| Graph | Neptune | Relationship-heavy domains (e.g., recommendations) | Secondary |\n| Time-Series | InfluxDB | Metrics, monitoring data | Secondary |\n\n> **Primary**: Fully supported with managed services, infrastructure modules, and dedicated support\n> \n> **Secondary**: Available but with limited tooling and support\n\n## Data Modeling Principles\n\n### 1. Model Around Bounded Contexts\n\n- Define clear domain boundaries based on business capabilities\n- Avoid modeling your database around UI needs\n- Focus on the core domain model within each service\n\n### 2. Design for Access Patterns\n\n- Consider query patterns when designing the schema\n- Optimize for the most frequent queries\n- Plan for future data growth and access needs\n\n### 3. Denormalization When Appropriate\n\n- Strategic denormalization is often necessary in distributed systems\n- Consider composite keys for efficient lookups\n- Use materialized views for read-heavy scenarios\n\n### Example: Product Service Data Model (MongoDB)\n\n```javascript\n// Product Collection\n{\n  \"_id\": ObjectId(\"5f8d0f3e1c9d440000c9a1f5\"),\n  \"name\": \"Ultra HD Smart TV\",\n  \"description\": \"55-inch Ultra HD Smart TV with voice control\",\n  \"sku\": \"TV-55UHD-2023\",\n  \"price\": {\n    \"amount\": 699.99,\n    \"currency\": \"USD\"\n  },\n  \"category\": \"electronics\",\n  \"subcategory\": \"televisions\",\n  \"attributes\": {\n    \"brand\": \"TechVision\",\n    \"model\": \"UHD-55X\",\n    \"screenSize\": \"55\",\n    \"resolution\": \"3840x2160\",\n    \"smartFeatures\": true,\n    \"voiceControl\": true,\n    \"energyRating\": \"A+\"\n  },\n  \"images\": [\n    {\n      \"url\": \"https://storage.flowmart.com/products/tv-55uhd-2023-main.jpg\",\n      \"isPrimary\": true,\n      \"alt\": \"TechVision 55-inch Smart TV front view\"\n    },\n    {\n      \"url\": \"https://storage.flowmart.com/products/tv-55uhd-2023-side.jpg\",\n      \"isPrimary\": false,\n      \"alt\": \"TechVision 55-inch Smart TV side view\"\n    }\n  ],\n  \"inventory\": {\n    \"inStock\": 127,\n    \"reserved\": 13,\n    \"available\": 114,\n    \"lowStockThreshold\": 25,\n    \"lastUpdated\": ISODate(\"2023-09-15T14:23:45.000Z\")\n  },\n  \"status\": \"active\",\n  \"createdAt\": ISODate(\"2023-05-01T09:30:00.000Z\"),\n  \"updatedAt\": ISODate(\"2023-09-15T14:23:45.000Z\")\n}\n\n// Review Collection\n{\n  \"_id\": ObjectId(\"5f8d0f3e1c9d440000c9a1f9\"),\n  \"productId\": ObjectId(\"5f8d0f3e1c9d440000c9a1f5\"),\n  \"customerId\": \"cust-12345\",\n  \"rating\": 4.5,\n  \"title\": \"Great TV for the price\",\n  \"content\": \"The picture quality is excellent and setup was easy...\",\n  \"verified\": true,\n  \"helpfulVotes\": 27,\n  \"createdAt\": ISODate(\"2023-06-15T18:22:10.000Z\")\n}\n```\n\n### Example: Order Service Data Model (PostgreSQL)\n\n```sql\n-- Orders Table\nCREATE TABLE orders (\n  id UUID PRIMARY KEY,\n  customer_id VARCHAR(255) NOT NULL,\n  order_number VARCHAR(50) NOT NULL UNIQUE,\n  status VARCHAR(50) NOT NULL,\n  total_amount DECIMAL(10, 2) NOT NULL,\n  currency VARCHAR(3) NOT NULL,\n  shipping_address_id UUID NOT NULL,\n  billing_address_id UUID NOT NULL,\n  payment_method_id UUID,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Order Items Table\nCREATE TABLE order_items (\n  id UUID PRIMARY KEY,\n  order_id UUID NOT NULL REFERENCES orders(id),\n  product_id VARCHAR(255) NOT NULL,\n  product_sku VARCHAR(100) NOT NULL,\n  product_name VARCHAR(255) NOT NULL,\n  quantity INTEGER NOT NULL,\n  unit_price DECIMAL(10, 2) NOT NULL,\n  subtotal DECIMAL(10, 2) NOT NULL,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Indexes\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\nCREATE INDEX idx_orders_status ON orders(status);\nCREATE INDEX idx_order_items_order_id ON order_items(order_id);\nCREATE INDEX idx_order_items_product_id ON order_items(product_id);\n```\n\n## Data Consistency Patterns\n\n### 1. Eventual Consistency with Event-Driven Updates\n\nAt FlowMart, we embrace eventual consistency for most inter-service data coordination:\n\n```mermaid\nsequenceDiagram\n    participant Order Service\n    participant Kafka\n    participant Inventory Service\n    participant Notification Service\n    \n    Order Service->>Order Service: Create Order\n    Order Service->>Kafka: Publish OrderCreated\n    Kafka->>Inventory Service: Consume OrderCreated\n    Inventory Service->>Inventory Service: Update Inventory\n    Kafka->>Notification Service: Consume OrderCreated\n    Notification Service->>Notification Service: Send Notification\n```\n\n### 2. Saga Pattern for Distributed Transactions\n\nFor operations that span multiple services and require transactional semantics:\n\n```mermaid\nsequenceDiagram\n    participant Order Service\n    participant Payment Service\n    participant Inventory Service\n    participant Shipping Service\n    \n    Order Service->>Payment Service: Process Payment\n    Payment Service-->>Order Service: Payment Processed\n    Order Service->>Inventory Service: Reserve Inventory\n    Inventory Service-->>Order Service: Inventory Reserved\n    Order Service->>Shipping Service: Create Shipment\n    Shipping Service-->>Order Service: Shipment Created\n    \n    alt Failure at any step\n        Note over Order Service,Shipping Service: Begin compensating transactions\n        Order Service->>Payment Service: Refund Payment\n        Order Service->>Inventory Service: Release Inventory\n        Order Service->>Shipping Service: Cancel Shipment\n    end\n```\n\n### 3. CQRS (Command Query Responsibility Segregation)\n\nFor services with complex read patterns or high read-to-write ratios:\n\n```\n┌───────────────┐     Commands     ┌───────────────┐\n│               │ ──────────────▶  │               │\n│   API Layer   │                  │ Command Model │\n│               │ ◀──────────────  │               │\n└───────────────┘                  └───────────────┘\n        │                                  │\n        │                                  │\n        │                                  ▼\n        │                          ┌───────────────┐\n        │                          │               │\n        │                          │  Event Store  │\n        │                          │               │\n        │                          └───────────────┘\n        │                                  │\n        │                                  │\n        ▼                                  ▼\n┌───────────────┐                  ┌───────────────┐\n│               │                  │               │\n│  Query Model  │ ◀──────────────  │ Event Handler │\n│               │                  │               │\n└───────────────┘                  └───────────────┘\n```\n\n## Data Migration Patterns\n\n### 1. Schema Evolution\n\nFor incremental changes to a service's schema:\n\n- Add new fields with default values\n- Make changes backward compatible\n- Use database migration tools (e.g., Flyway, Liquibase, MongoDB migrations)\n\nExample migration script (Flyway):\n\n```sql\n-- V2023.09.15.1__Add_Order_Tracking.sql\nALTER TABLE orders ADD COLUMN tracking_number VARCHAR(100);\nALTER TABLE orders ADD COLUMN shipping_carrier VARCHAR(50);\n```\n\n### 2. Expand-Contract Pattern (Parallel Change)\n\nFor significant schema changes:\n\n1. **Expand**: Add new fields/tables while maintaining old ones\n2. **Migrate**: Copy/transform data from old to new structure\n3. **Contract**: Remove old fields/tables after migration is complete\n\n### 3. Service Decomposition\n\nWhen splitting a monolithic database:\n\n1. Identify bounded contexts\n2. Create new service databases\n3. Implement dual-write mechanism temporarily\n4. Migrate historical data\n5. Switch reads to the new database\n6. Remove old tables after migration completes\n\n## Database Access Patterns\n\n### Repository Pattern\n\nOur standard approach for database access:\n\n```typescript\n// Product Repository Interface\nexport interface ProductRepository {\n  findById(id: string): Promise\u003CProduct | null>;\n  findByCategory(category: string, limit?: number, offset?: number): Promise\u003CProduct[]>;\n  findByQuery(query: ProductQuery): Promise\u003CProduct[]>;\n  save(product: Product): Promise\u003CProduct>;\n  update(id: string, updates: Partial\u003CProduct>): Promise\u003CProduct | null>;\n  delete(id: string): Promise\u003Cboolean>;\n}\n\n// MongoDB Implementation\nexport class MongoProductRepository implements ProductRepository {\n  constructor(private readonly db: Db) {}\n  \n  async findById(id: string): Promise\u003CProduct | null> {\n    const result = await this.db.collection('products').findOne({ _id: new ObjectId(id) });\n    return result ? this.mapToProduct(result) : null;\n  }\n  \n  // Additional implementation methods...\n}\n\n// PostgreSQL Implementation\nexport class PostgresProductRepository implements ProductRepository {\n  constructor(private readonly pool: Pool) {}\n  \n  async findById(id: string): Promise\u003CProduct | null> {\n    const result = await this.pool.query(\n      'SELECT * FROM products WHERE id = $1',\n      [id]\n    );\n    return result.rows.length ? this.mapToProduct(result.rows[0]) : null;\n  }\n  \n  // Additional implementation methods...\n}\n```\n\n## Connection Management\n\n### Connection Pooling\n\n```javascript\n// MongoDB Connection Pool (Node.js)\nconst { MongoClient } = require('mongodb');\n\nclass MongoDbClient {\n  constructor(config) {\n    this.client = new MongoClient(config.uri, {\n      maxPoolSize: config.maxPoolSize || 10,\n      minPoolSize: config.minPoolSize || 5,\n      maxIdleTimeMS: config.maxIdleTimeMS || 30000,\n      connectTimeoutMS: config.connectTimeoutMS || 5000\n    });\n  }\n  \n  async connect() {\n    await this.client.connect();\n    this.db = this.client.db(config.dbName);\n    console.log('Connected to MongoDB');\n    return this.db;\n  }\n  \n  async disconnect() {\n    await this.client.close();\n    console.log('Disconnected from MongoDB');\n  }\n}\n```\n\n### Health Checks\n\n```typescript\n// Database Health Check (TypeScript)\nexport class DatabaseHealthCheck {\n  constructor(private readonly dbClient: DbClient) {}\n  \n  async check(): Promise\u003CHealthStatus> {\n    try {\n      const startTime = Date.now();\n      await this.dbClient.ping();\n      const responseTime = Date.now() - startTime;\n      \n      return {\n        status: 'UP',\n        responseTime,\n        details: {\n          database: this.dbClient.getDatabaseName(),\n          connections: await this.dbClient.getConnectionStats()\n        }\n      };\n    } catch (error) {\n      return {\n        status: 'DOWN',\n        error: error.message,\n        details: {\n          database: this.dbClient.getDatabaseName()\n        }\n      };\n    }\n  }\n}\n```\n\n## Database Security Practices\n\n1. **Use IAM Roles/Service Accounts**: Avoid hardcoded credentials\n2. **Encryption**: Enable at-rest and in-transit encryption\n3. **No Direct Public Access**: Databases should never be directly exposed to the internet\n4. **Least Privilege**: Grant minimal required permissions to service accounts\n5. **Data Classification**: Tag data according to sensitivity\n6. **Audit Logging**: Enable database auditing for sensitive operations\n7. **Regular Backups**: Implement automated backup strategies\n8. **Secure Connection Strings**: Store connection information in secure vaults\n\n## Technology-Specific Guidelines\n\n### MongoDB Best Practices\n\n- Use document validation for schema enforcement\n- Create indexes for frequent query patterns\n- Limit document size (\u003C 16MB)\n- Use aggregation pipelines for complex queries\n- Implement TTL indexes for expiring data\n\n### PostgreSQL Best Practices\n\n- Use connection pooling\n- Implement database partitioning for large tables\n- Create appropriate indexes based on query patterns\n- Use prepared statements to prevent SQL injection\n- Implement row-level security for multi-tenant data\n\n### Redis Best Practices\n\n- Set appropriate key expiration times\n- Use Redis data structures (not just key-value)\n- Implement Redis Cluster for high availability\n- Monitor memory usage\n- Use Redis for its strengths: caching, rate limiting, session storage\n\n## Infrastructure as Code for Databases\n\nFlowMart provides Terraform modules for common database setups:\n\n```hcl\n# MongoDB Atlas Cluster\nmodule \"mongodb_cluster\" {\n  source = \"git::https://gitlab.flowmart.com/platform/terraform-modules//mongodb-atlas\"\n  \n  project_id      = var.atlas_project_id\n  cluster_name    = \"${var.service_name}-${var.environment}\"\n  environment     = var.environment\n  instance_size   = var.environment == \"production\" ? \"M10\" : \"M0\"\n  region          = var.region\n  \n  backup_enabled = var.environment == \"production\" ? true : false\n  \n  teams = {\n    owner     = \"team-product\",\n    developer = \"team-product-dev\"\n  }\n  \n  tags = {\n    Service     = var.service_name\n    Environment = var.environment\n    ManagedBy   = \"terraform\"\n  }\n}\n\n# PostgreSQL RDS Instance\nmodule \"postgres_db\" {\n  source = \"git::https://gitlab.flowmart.com/platform/terraform-modules//aws-rds-postgres\"\n  \n  identifier          = \"${var.service_name}-${var.environment}\"\n  allocated_storage   = var.environment == \"production\" ? 100 : 20\n  storage_type        = \"gp2\"\n  engine_version      = \"13.4\"\n  instance_class      = var.environment == \"production\" ? \"db.m5.large\" : \"db.t3.small\"\n  database_name       = replace(var.service_name, \"-\", \"_\")\n  vpc_id              = var.vpc_id\n  subnet_ids          = var.database_subnet_ids\n  multi_az            = var.environment == \"production\" ? true : false\n  deletion_protection = var.environment == \"production\" ? true : false\n  \n  backup_retention_period = var.environment == \"production\" ? 7 : 1\n  \n  tags = {\n    Service     = var.service_name\n    Environment = var.environment\n    ManagedBy   = \"terraform\"\n  }\n}\n```\n\n## Data Governance\n\n### Practices for Maintaining Data Integrity\n\n1. **Schema Registry**: Register and validate event schemas\n2. **Data Catalogs**: Document available data and owners\n3. **Data Lineage**: Track how data flows between services\n4. **Data Quality Checks**: Implement validation and integrity checks\n5. **Master Data Management**: Establish source-of-truth services\n\n### GDPR and Compliance\n\n1. **Data Minimization**: Collect only necessary data\n2. **Right to Erasure**: Implement mechanisms to delete customer data\n3. **Data Portability**: Support data export in common formats\n4. **Consent Management**: Track and honor user consent\n5. **Purpose Limitation**: Use data only for its intended purpose\n\n## Recommended Resources\n\n- **Books**:\n  - \"Database Internals\" by Alex Petrov\n  - \"Designing Data-Intensive Applications\" by Martin Kleppmann\n\n- **Courses**:\n  - MongoDB University\n  - PostgreSQL Administration by EnterpriseDB\n\n- **FlowMart Resources**:\n  - Internal Database Patterns Playbook\n  - Monthly Database Office Hours\n\n## Next Steps\n\n- Learn about [Event schema design](/docs/guides/creating-new-microservices/event-schemas)\n- Explore [API design best practices](/docs/guides/creating-new-microservices/api-design)\n- Understand [Observability in microservices](/docs/guides/creating-new-microservices/observability)",
  "../examples/default/docs/guides/creating-new-microservices/03-database-patterns.mdx",
  "e292ea0867ee5ead",
  "docs/guides/creating-new-microservices/04-event-schemas",
  {
    "id": 544,
    "data": 546,
    "body": 551,
    "filePath": 552,
    "digest": 553,
    "deferredRender": 20
  },
  { "title": 547, "summary": 548, "sidebar": 549 },
  "Event Schema Design",
  "Best practices for designing and managing event schemas in FlowMart's event-driven architecture",
  { "label": 547, "order": 550 },
  6,
  "This guide outlines best practices for designing, evolving, and managing event schemas in FlowMart's event-driven architecture.\n\n## Introduction to Events in Our Architecture\n\nEvents are the backbone of FlowMart's microservices ecosystem. They enable:\n\n- **Loose coupling** between services\n- **Asynchronous communication**\n- **Eventual consistency** across service boundaries\n- **Event sourcing** for critical business processes\n- **Audit trails** of system changes\n\n## Event Schema Fundamentals\n\n### What Is an Event Schema?\n\nAn event schema defines the structure and validation rules for events flowing through our system. Properly designed schemas ensure events can be:\n\n- **Produced** consistently by services \n- **Consumed** reliably by other services\n- **Evolved** over time without breaking consumers\n- **Validated** to prevent invalid data from propagating\n- **Documented** for developers to understand and use\n\n### Event Schema Registry\n\nFlowMart uses a centralized **Schema Registry** to:\n\n1. Store all event schemas\n2. Validate events at publish time\n3. Provide a browsable catalog of events\n4. Track schema versions and compatibility\n5. Generate client libraries and documentation\n\nAll services must register their event schemas in the central registry before publishing events.\n\n## Schema Design Principles\n\n### 1. Design for Evolution\n\nEvents should be designed to evolve over time:\n\n- **Additive Changes Only**: Add optional fields rather than modifying existing ones\n- **Required Minimal Core**: Keep required fields to essential business data\n- **Meaningful Defaults**: Provide sensible defaults for optional fields\n- **Version Awareness**: Include schema version information\n\n### 2. Event Ownership\n\nEach event type has a single owner:\n\n- The **producing service** owns the event schema\n- Only the owner can make changes to the schema\n- The owner is responsible for schema compatibility\n\n### 3. Semantic Versioning\n\nFollow semantic versioning for event schemas:\n\n- **Major Version**: Breaking changes (consumers must update)\n- **Minor Version**: Backward-compatible feature additions\n- **Patch Version**: Backward-compatible bug fixes\n\n### 4. Business-Oriented Event Naming\n\nEvents should be named using business terminology:\n\n- Use past tense verbs (e.g., `OrderPlaced`, not `CreateOrder`)\n- Follow the pattern: `[Entity][Event]` (e.g., `ProductCreated`, `PaymentProcessed`)\n- Use domain-specific terminology consistent with our ubiquitous language\n\n## Event Schema Format (JSON Schema)\n\nFlowMart uses JSON Schema as the standard format for defining event schemas:\n\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"$id\": \"https://schemas.flowmart.com/events/product/ProductCreated/1.0.0\",\n  \"title\": \"ProductCreated\",\n  \"description\": \"Represents the creation of a new product in the catalog\",\n  \"type\": \"object\",\n  \"required\": [\"eventId\", \"eventType\", \"eventVersion\", \"timestamp\", \"data\"],\n  \"properties\": {\n    \"eventId\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"Unique identifier for this event instance\"\n    },\n    \"eventType\": {\n      \"type\": \"string\",\n      \"enum\": [\"ProductCreated\"],\n      \"description\": \"Type of the event\"\n    },\n    \"eventVersion\": {\n      \"type\": \"string\",\n      \"pattern\": \"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+$\",\n      \"description\": \"Semantic version of the event schema\"\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\",\n      \"description\": \"ISO-8601 timestamp when the event was created\"\n    },\n    \"source\": {\n      \"type\": \"string\",\n      \"description\": \"Service that produced the event\"\n    },\n    \"data\": {\n      \"type\": \"object\",\n      \"required\": [\"productId\", \"name\", \"sku\", \"price\"],\n      \"properties\": {\n        \"productId\": {\n          \"type\": \"string\",\n          \"format\": \"uuid\",\n          \"description\": \"Unique identifier for the product\"\n        },\n        \"name\": {\n          \"type\": \"string\",\n          \"minLength\": 1,\n          \"maxLength\": 255,\n          \"description\": \"Name of the product\"\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"maxLength\": 2000,\n          \"description\": \"Description of the product\"\n        },\n        \"sku\": {\n          \"type\": \"string\",\n          \"pattern\": \"^[A-Z0-9-]{5,20}$\",\n          \"description\": \"Stock keeping unit - unique product identifier\"\n        },\n        \"price\": {\n          \"type\": \"object\",\n          \"required\": [\"amount\", \"currency\"],\n          \"properties\": {\n            \"amount\": {\n              \"type\": \"number\",\n              \"exclusiveMinimum\": 0,\n              \"description\": \"Price amount\"\n            },\n            \"currency\": {\n              \"type\": \"string\",\n              \"enum\": [\"USD\", \"EUR\", \"GBP\", \"CAD\"],\n              \"description\": \"Price currency code\"\n            }\n          }\n        },\n        \"categories\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"description\": \"Categories the product belongs to\"\n        },\n        \"attributes\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"type\": [\"string\", \"number\", \"boolean\"]\n          },\n          \"description\": \"Additional product attributes as key-value pairs\"\n        }\n      }\n    },\n    \"metadata\": {\n      \"type\": \"object\",\n      \"additionalProperties\": true,\n      \"description\": \"Additional contextual information about the event\"\n    },\n    \"correlationId\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"ID for correlating related events\"\n    },\n    \"causationId\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"ID of the event that caused this event\"\n    }\n  },\n  \"additionalProperties\": false\n}\n```\n\n## Standard Event Envelope\n\nAll FlowMart events follow a standard envelope structure:\n\n```json\n{\n  \"eventId\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",\n  \"eventType\": \"ProductCreated\",\n  \"eventVersion\": \"1.0.0\",\n  \"timestamp\": \"2023-09-15T13:25:47.803Z\",\n  \"source\": \"product-service\",\n  \"data\": {\n    // Event-specific payload\n  },\n  \"metadata\": {\n    // Optional contextual information\n  },\n  \"correlationId\": \"7f8d0e3c-d5f9-42e1-a11b-78ad6c0c380a\",\n  \"causationId\": \"3e4f5d6c-7b8a-9c0d-1e2f-3a4b5c6d7e8f\"\n}\n```\n\n### Required Envelope Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| eventId | UUID | Unique identifier for the event instance |\n| eventType | String | Name of the event (e.g., `ProductCreated`) |\n| eventVersion | String | Semantic version of the event schema |\n| timestamp | ISO-8601 | When the event occurred |\n| data | Object | Event-specific payload |\n\n### Optional Envelope Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| source | String | Service that produced the event |\n| metadata | Object | Additional context about the event |\n| correlationId | UUID | ID for correlating related events in a flow |\n| causationId | UUID | ID of the event that caused this event |\n\n## Event Data Types\n\n### Primitive Types\n\n- **String**: Use for text data\n- **Number**: Use for numeric values (integers or decimals)\n- **Boolean**: Use for true/false flags\n- **Array**: Use for collections of the same type\n- **Object**: Use for nested structures\n\n### Specialized Formats\n\n- **UUID**: Use for unique identifiers (format: `uuid`)\n- **ISO Date-Time**: Use for timestamps (format: `date-time`)\n- **Email**: Use for email addresses (format: `email`)\n- **URI**: Use for web addresses (format: `uri`)\n- **Decimal**: Use for currency amounts (type: `number`)\n\n### Complex Types\n\nFor complex or reusable types, create separate schema definitions:\n\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"$id\": \"https://schemas.flowmart.com/common/Address/1.0.0\",\n  \"title\": \"Address\",\n  \"type\": \"object\",\n  \"required\": [\"line1\", \"city\", \"postalCode\", \"country\"],\n  \"properties\": {\n    \"line1\": {\n      \"type\": \"string\",\n      \"maxLength\": 100\n    },\n    \"line2\": {\n      \"type\": \"string\",\n      \"maxLength\": 100\n    },\n    \"city\": {\n      \"type\": \"string\",\n      \"maxLength\": 100\n    },\n    \"region\": {\n      \"type\": \"string\",\n      \"maxLength\": 100\n    },\n    \"postalCode\": {\n      \"type\": \"string\",\n      \"maxLength\": 20\n    },\n    \"country\": {\n      \"type\": \"string\",\n      \"maxLength\": 2,\n      \"pattern\": \"^[A-Z]{2}$\"\n    }\n  }\n}\n```\n\nThen reference them in your event schemas:\n\n```json\n{\n  \"properties\": {\n    \"shippingAddress\": {\n      \"$ref\": \"https://schemas.flowmart.com/common/Address/1.0.0\"\n    }\n  }\n}\n```\n\n## Common Event Patterns\n\n### State Change Events\n\nRepresent changes to an entity's state:\n\n```json\n{\n  \"eventType\": \"OrderStatusChanged\",\n  \"data\": {\n    \"orderId\": \"61fea0a1-2ac4-4e8c-a851-d38f7c8c06f9\",\n    \"previousStatus\": \"PAYMENT_PENDING\",\n    \"newStatus\": \"PAYMENT_COMPLETED\",\n    \"reason\": \"Payment successful\",\n    \"changedBy\": \"payment-service\"\n  }\n}\n```\n\n### Resource Creation Events\n\nRepresent the creation of a new entity:\n\n```json\n{\n  \"eventType\": \"CustomerCreated\",\n  \"data\": {\n    \"customerId\": \"cust-12345\",\n    \"email\": \"john.doe@example.com\",\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\",\n    \"createdAt\": \"2023-09-15T10:30:00Z\"\n  }\n}\n```\n\n### Resource Update Events\n\nRepresent updates to an existing entity:\n\n```json\n{\n  \"eventType\": \"ProductUpdated\",\n  \"data\": {\n    \"productId\": \"b3c631a5-f7c8-4d89-a57f-dd2f069b5730\",\n    \"changes\": {\n      \"price\": {\n        \"amount\": 24.99,\n        \"currency\": \"USD\"\n      },\n      \"inventory\": {\n        \"inStock\": 250\n      }\n    },\n    \"updatedAt\": \"2023-09-15T14:22:36Z\"\n  }\n}\n```\n\n### Action Events\n\nRepresent business actions that occurred:\n\n```json\n{\n  \"eventType\": \"PaymentProcessed\",\n  \"data\": {\n    \"paymentId\": \"pay-67890\",\n    \"orderId\": \"ord-12345\",\n    \"amount\": 99.99,\n    \"currency\": \"USD\",\n    \"status\": \"SUCCESSFUL\",\n    \"paymentMethod\": \"CREDIT_CARD\",\n    \"processedAt\": \"2023-09-15T13:45:22Z\"\n  }\n}\n```\n\n## Schema Evolution\n\n### Compatibility Types\n\nWe support the following compatibility modes for schema evolution:\n\n- **Backward**: New schema can read data produced with previous schema\n- **Forward**: Previous schema can read data produced with new schema\n- **Full**: Both backward and forward compatibility\n- **None**: No compatibility guarantees (use with caution)\n\n### Backward Compatibility Rules\n\n1. **Adding optional fields** is safe\n2. **Removing optional fields** is safe\n3. **Making a required field optional** is safe\n4. **Adding new enum values** is safe\n5. **Widening numeric ranges** is safe (e.g., int to float)\n\n### Breaking Changes to Avoid\n\n1. ❌ **Removing required fields**\n2. ❌ **Adding required fields**\n3. ❌ **Changing field types**\n4. ❌ **Renaming fields**\n5. ❌ **Restricting enum values**\n\n### Handling Breaking Changes\n\nIf you must make a breaking change:\n\n1. Create a new major version of the schema\n2. Maintain both versions for a transition period\n3. Implement dual publishing for critical events\n4. Help consumers migrate to the new version\n5. Deprecate the old version with advance notice\n\n## Consuming Events\n\n### Consumer Best Practices\n\n1. **Be tolerant in what you accept**:\n   - Ignore unknown fields\n   - Provide defaults for missing optional fields\n   - Handle enum values gracefully, including unknown values\n\n2. **Validate incoming events**:\n   - Verify events against their schema\n   - Check required fields\n   - Validate business rules before processing\n\n3. **Handle versioning gracefully**:\n   - Check event version before processing\n   - Implement version-specific handlers if needed\n   - Subscribe to schema registry updates\n\n### Consumer Code Example (TypeScript)\n\n```typescript\nimport { KafkaConsumer } from '@flowmart/kafka-client';\nimport { SchemaRegistry } from '@flowmart/schema-registry';\nimport { OrderProcessingService } from './services';\n\n// Initialize schema registry client\nconst schemaRegistry = new SchemaRegistry({\n  baseUrl: 'https://schema-registry.flowmart.com',\n});\n\n// Define event interface\ninterface OrderPlacedEvent {\n  eventId: string;\n  eventType: 'OrderPlaced';\n  eventVersion: string;\n  timestamp: string;\n  data: {\n    orderId: string;\n    customerId: string;\n    items: Array\u003C{\n      productId: string;\n      quantity: number;\n      unitPrice: number;\n    }>;\n    totalAmount: number;\n    currency: string;\n    shippingAddress: {\n      // Address fields...\n    };\n  };\n  // Other envelope fields...\n}\n\n// Initialize consumer\nconst orderConsumer = new KafkaConsumer({\n  groupId: 'inventory-service',\n  brokers: ['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092'],\n});\n\n// Initialize service\nconst orderProcessor = new OrderProcessingService();\n\n// Start consuming events\nasync function startConsumer() {\n  await orderConsumer.subscribe('order-events');\n  \n  orderConsumer.on('message', async (message) => {\n    try {\n      // Parse the message\n      const rawEvent = JSON.parse(message.value.toString());\n      \n      // Skip if not the event we're interested in\n      if (rawEvent.eventType !== 'OrderPlaced') {\n        return;\n      }\n      \n      // Validate against schema\n      const isValid = await schemaRegistry.validate(\n        rawEvent, \n        'OrderPlaced', \n        rawEvent.eventVersion\n      );\n      \n      if (!isValid) {\n        console.error('Invalid event schema', rawEvent);\n        return;\n      }\n      \n      // Type-safe processing\n      const event = rawEvent as OrderPlacedEvent;\n      \n      // Process the order\n      await orderProcessor.processNewOrder(event.data);\n      \n      // Commit the offset\n      await orderConsumer.commitOffset(message);\n      \n    } catch (error) {\n      console.error('Error processing order event', error);\n      // Implement retry/dead-letter logic\n    }\n  });\n}\n\nstartConsumer().catch(console.error);\n```\n\n## Publishing Events\n\n### Producer Best Practices\n\n1. **Validate before publishing**:\n   - Ensure events comply with their schema\n   - Verify business rules and data integrity\n   - Set appropriate event headers\n\n2. **Include essential metadata**:\n   - Generate a unique event ID\n   - Set the correct event type and version\n   - Include accurate timestamp\n   - Set correlation and causation IDs\n\n3. **Handle publishing failures**:\n   - Implement retry mechanisms with backoff\n   - Store events temporarily if Kafka is unavailable\n   - Log failed events for troubleshooting\n\n### Producer Code Example (TypeScript)\n\n```typescript\nimport { v4 as uuid } from 'uuid';\nimport { KafkaProducer } from '@flowmart/kafka-client';\nimport { SchemaRegistry } from '@flowmart/schema-registry';\nimport { Product } from './models';\n\n// Initialize schema registry client\nconst schemaRegistry = new SchemaRegistry({\n  baseUrl: 'https://schema-registry.flowmart.com',\n});\n\n// Initialize producer\nconst producer = new KafkaProducer({\n  clientId: 'product-service',\n  brokers: ['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092'],\n});\n\nexport class ProductEventService {\n  async publishProductCreated(product: Product, correlationId?: string): Promise\u003Cvoid> {\n    const eventId = uuid();\n    \n    const event = {\n      eventId,\n      eventType: 'ProductCreated',\n      eventVersion: '1.0.0',\n      timestamp: new Date().toISOString(),\n      source: 'product-service',\n      data: {\n        productId: product.id,\n        name: product.name,\n        summary: product.description || null,\n        sku: product.sku,\n        price: {\n          amount: product.price,\n          currency: 'USD'  // Default to USD\n        },\n        categories: product.categories || [],\n        attributes: product.attributes || {}\n      },\n      metadata: {\n        // Add any additional metadata\n      },\n      correlationId: correlationId || eventId,\n      causationId: null  // No previous event caused this\n    };\n    \n    // Validate against schema\n    const isValid = await schemaRegistry.validate(\n      event, \n      'ProductCreated', \n      '1.0.0'\n    );\n    \n    if (!isValid) {\n      const errors = await schemaRegistry.getValidationErrors(\n        event, \n        'ProductCreated', \n        '1.0.0'\n      );\n      throw new Error(`Invalid event schema: ${JSON.stringify(errors)}`);\n    }\n    \n    // Publish event\n    await producer.send({\n      topic: 'product-events',\n      messages: [\n        {\n          key: product.id,\n          value: JSON.stringify(event),\n          headers: {\n            'eventType': 'ProductCreated',\n            'contentType': 'application/json'\n          }\n        }\n      ]\n    });\n    \n    console.log(`Published ProductCreated event: ${eventId}`);\n  }\n}\n```\n\n## Schema Registry Integration\n\n### Registering a New Schema\n\n```bash\n# Using CLI tool\nflowmart-schema register \\\n  --file ./schemas/ProductCreated.json \\\n  --compatibility BACKWARD\n\n# API endpoint\ncurl -X POST https://schema-registry.flowmart.com/subjects/ProductCreated/versions \\\n  -H \"Content-Type: application/json\" \\\n  -d @./schemas/ProductCreated.json\n```\n\n### Retrieving a Schema\n\n```javascript\n// Using JavaScript client\nconst schema = await schemaRegistry.getSchema('ProductCreated', '1.0.0');\n\n// API endpoint\ncurl https://schema-registry.flowmart.com/subjects/ProductCreated/versions/latest\n```\n\n### Checking Compatibility\n\n```javascript\n// Using JavaScript client\nconst isCompatible = await schemaRegistry.checkCompatibility(\n  newSchema, \n  'ProductCreated'\n);\n\n// API endpoint\ncurl -X POST https://schema-registry.flowmart.com/compatibility/subjects/ProductCreated/versions/latest \\\n  -H \"Content-Type: application/json\" \\\n  -d @./schemas/ProductCreated.v2.json\n```\n\n## Schema Registry UI\n\nOur Schema Registry includes a web interface at [https://schema-registry.flowmart.com/ui](https://schema-registry.flowmart.com/ui) that provides:\n\n- Browsable catalog of all event schemas\n- Schema versioning history\n- Compatibility information\n- Schema validation tools\n- Documentation generation\n\n## Testing Event Schemas\n\n### Unit Testing Schemas\n\n```javascript\nimport { validateAgainstSchema } from '@flowmart/schema-validator';\nimport productCreatedSchema from './schemas/ProductCreated.json';\n\ndescribe('ProductCreated schema', () => {\n  it('validates valid events', () => {\n    const validEvent = {\n      eventId: 'f47ac10b-58cc-4372-a567-0e02b2c3d479',\n      eventType: 'ProductCreated',\n      eventVersion: '1.0.0',\n      timestamp: '2023-09-15T13:25:47.803Z',\n      data: {\n        productId: 'b3c631a5-f7c8-4d89-a57f-dd2f069b5730',\n        name: 'Smartphone X Pro',\n        sku: 'SP-XPRO-2023',\n        price: {\n          amount: 999.99,\n          currency: 'USD'\n        }\n      }\n    };\n    \n    const result = validateAgainstSchema(validEvent, productCreatedSchema);\n    expect(result.valid).toBe(true);\n  });\n  \n  it('rejects events with missing required fields', () => {\n    const invalidEvent = {\n      eventId: 'f47ac10b-58cc-4372-a567-0e02b2c3d479',\n      eventType: 'ProductCreated',\n      eventVersion: '1.0.0',\n      timestamp: '2023-09-15T13:25:47.803Z',\n      data: {\n        // Missing required productId\n        name: 'Smartphone X Pro',\n        // Missing required sku\n        price: {\n          amount: 999.99,\n          currency: 'USD'\n        }\n      }\n    };\n    \n    const result = validateAgainstSchema(invalidEvent, productCreatedSchema);\n    expect(result.valid).toBe(false);\n    expect(result.errors.length).toBeGreaterThan(0);\n  });\n});\n```\n\n### Integration Testing with Schema Registry\n\n```javascript\ndescribe('Schema Registry Integration', () => {\n  it('registers and validates schema', async () => {\n    // Register test schema\n    await schemaRegistry.registerSchema(\n      'TestEvent',\n      testEventSchema,\n      'BACKWARD'\n    );\n    \n    // Create test event\n    const testEvent = {\n      eventId: uuid(),\n      eventType: 'TestEvent',\n      eventVersion: '1.0.0',\n      timestamp: new Date().toISOString(),\n      data: {\n        // Test data...\n      }\n    };\n    \n    // Validate against registered schema\n    const isValid = await schemaRegistry.validate(\n      testEvent,\n      'TestEvent',\n      '1.0.0'\n    );\n    \n    expect(isValid).toBe(true);\n  });\n});\n```\n\n## Event Documentation\n\n### Self-Documenting Schemas\n\nUse descriptive fields in your JSON Schema to auto-generate documentation:\n\n```json\n{\n  \"title\": \"ProductCreated\",\n  \"description\": \"Published when a new product is created in the catalog\",\n  \"properties\": {\n    \"data\": {\n      \"properties\": {\n        \"productId\": {\n          \"description\": \"Unique identifier for the product\",\n          \"examples\": [\"p-12345\"]\n        }\n      }\n    }\n  }\n}\n```\n\n### Documentation in Code\n\nDocument event handling with clear comments:\n\n```typescript\n/**\n * Handles the ProductCreated event\n * This event is triggered when a new product is added to the catalog.\n * It updates the inventory service with the new product information.\n * \n * @param event The ProductCreated event\n * @see https://schema-registry.flowmart.com/ui/schemas/ProductCreated/1.0.0\n */\nasync function handleProductCreated(event: ProductCreatedEvent): Promise\u003Cvoid> {\n  // Implementation...\n}\n```\n\n## Event Tracing and Debugging\n\n### Correlation IDs\n\nUse correlation IDs to trace requests across services:\n\n```typescript\n// When handling an API request\nconst correlationId = req.headers['x-correlation-id'] || uuid();\n\n// Include in all events\nconst event = {\n  // Other event fields...\n  correlationId,\n  // If this event was caused by another event\n  causationId: previousEvent?.eventId\n};\n```\n\n### Event Logging\n\nLog event publishing and consumption with consistent format:\n\n```typescript\n// Producer logging\nlogger.info('Publishing event', {\n  eventId: event.eventId,\n  eventType: event.eventType,\n  correlationId: event.correlationId\n});\n\n// Consumer logging\nlogger.info('Consuming event', {\n  eventId: event.eventId,\n  eventType: event.eventType,\n  correlationId: event.correlationId,\n  consumer: 'inventory-service'\n});\n```\n\n## Event Monitoring\n\nMonitor your event streams using our standard observability stack:\n\n1. **Kafka Metrics**: Lag, throughput, errors\n2. **Schema Registry Metrics**: Validation failures, compatibility checks\n3. **Service Metrics**: Event processing times, failure rates\n4. **Custom Dashboards**: Domain-specific event flows\n\nAccess dashboards at [https://grafana.flowmart.com/d/events](https://grafana.flowmart.com/d/events)\n\n## Event Schema Governance\n\n### Change Management\n\n1. **Proposal**: Document the schema change with rationale\n2. **Review**: Domain experts review for business requirements\n3. **Compatibility Check**: Verify with schema registry\n4. **Approval**: Get sign-off from service team leads\n5. **Publication**: Register schema and announce change\n\n### Schema Review Checklist\n\n✅ Schema follows naming conventions  \n✅ Required fields are truly necessary  \n✅ Field types are appropriate  \n✅ Enums have complete value lists  \n✅ Constraints (min, max, etc.) are appropriate  \n✅ Documentation is complete  \n✅ Versioning follows semantic versioning  \n✅ Compatibility type is specified  \n\n## Conclusion\n\nWell-designed event schemas are foundational to reliable event-driven systems. Following FlowMart's event schema guidelines ensures our services can communicate reliably today and evolve confidently tomorrow.\n\n## Next Steps\n\n- Explore [API design best practices](/docs/guides/creating-new-microservices/api-design)\n- Understand [Observability in microservices](/docs/guides/creating-new-microservices/observability)\n- Learn about [CI/CD for microservices](/docs/guides/creating-new-microservices/cicd-pipeline)",
  "../examples/default/docs/guides/creating-new-microservices/04-event-schemas.mdx",
  "4f8421fb132b7964",
  "docs/operations-and-support/runbooks/inventory-service-runbook",
  {
    "id": 554,
    "data": 556,
    "body": 574,
    "filePath": 575,
    "digest": 576,
    "deferredRender": 20
  },
  {
    "title": 557,
    "summary": 558,
    "sidebar": 559,
    "owners": 561,
    "badges": 563
  },
  "Inventory Service - Runbook",
  "Operational runbook for troubleshooting and maintaining the InventoryService",
  { "label": 560, "order": 487 },
  "InventoryService",
  [562],
  { "id": 297 },
  [564, 568, 571],
  { "content": 565, "backgroundColor": 566, "textColor": 567 },
  "🚀 Operational Procedures",
  "blue",
  "white",
  { "content": 569, "backgroundColor": 570, "textColor": 567 },
  "🚨 Troubleshooting Guide",
  "red",
  { "content": 572, "backgroundColor": 573, "textColor": 567 },
  "🔄 Maintenance Schedule",
  "yellow",
  "This runbook provides operational procedures for the InventoryService, which is responsible for managing product inventory and stock levels across the FlowMart e-commerce platform.\n\n## Architecture\n\nThe InventoryService is responsible for:\n- Managing product inventory and stock levels\n- Reserving inventory for pending orders\n- Tracking inventory across warehouses and locations\n- Providing real-time availability information\n- Triggering restock notifications\n\n### Service Dependencies\n\n```mermaid\nflowchart TD\n    InventoryService --> PostgresDB[(PostgreSQL Database)]\n    InventoryService --> Redis[(Redis Cache)]\n    InventoryService --> EventBus[Event Bus]\n    InventoryService --> WarehouseSystem[Warehouse Management System]\n```\n\n## Monitoring and Alerting\n\n### Key Metrics\n\n| Metric | Description | Warning Threshold | Critical Threshold |\n|--------|-------------|-------------------|-------------------|\n| `inventory_check_rate` | Inventory availability checks per minute | > 1000 | > 5000 |\n| `inventory_check_latency` | Time to check inventory availability | > 100ms | > 500ms |\n| `inventory_update_latency` | Time to update inventory levels | > 200ms | > 1s |\n| `low_stock_items` | Number of items with low stock | > 50 | > 100 |\n| `connection_pool_usage` | Database connection pool utilization | > 70% | > 90% |\n| `redis_hit_rate` | Cache hit rate | \u003C 80% | \u003C 60% |\n\n### Dashboards\n\n- [InventoryService Overview](https://grafana.flowmart.com/d/inventory-overview)\n- [Stock Level Alerts](https://grafana.flowmart.com/d/inventory-stock-alerts)\n- [Database Performance](https://grafana.flowmart.com/d/inventory-database)\n\n### Common Alerts\n\n| Alert | Description | Troubleshooting Steps |\n|-------|-------------|----------------------|\n| `InventoryServiceHighLatency` | API latency exceeds thresholds | See [High Latency](#high-latency) |\n| `InventoryServiceDatabaseIssues` | Database connection or performance issues | See [Database Issues](#database-issues) |\n| `InventoryServiceCacheFailure` | Redis cache unavailable or performance degraded | See [Cache Issues](#cache-issues) |\n| `InventoryServiceOutOfStock` | Critical products out of stock | See [Stock Management](#stock-management) |\n\n## Troubleshooting Guides\n\n### High Latency\n\nIf the service is experiencing high latency:\n\n1. **Check system resource usage**:\n   ```bash\n   kubectl top pods -n inventory\n   ```\n\n2. **Check database connection pool**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl localhost:8080/actuator/metrics/hikaricp.connections.usage\n   ```\n\n3. **Check cache hit rate**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl localhost:8080/actuator/metrics/cache.gets | grep \"hit_ratio\"\n   ```\n\n4. **Check for slow queries** in the database:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -c \"SELECT query, calls, mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;\"\n   ```\n\n5. **Scale the service** if needed:\n   ```bash\n   kubectl scale deployment inventory-service -n inventory --replicas=5\n   ```\n\n### Database Issues\n\nIf there are database connection or performance issues:\n\n1. **Check PostgreSQL status**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- pg_isready -U postgres\n   ```\n\n2. **Check for long-running transactions**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -c \"SELECT pid, now() - xact_start AS duration, state, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC;\"\n   ```\n\n3. **Check for table bloat**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -c \"SELECT schemaname, relname, n_live_tup, n_dead_tup, (n_dead_tup::float / n_live_tup::float) AS dead_ratio FROM pg_stat_user_tables WHERE n_live_tup > 1000 ORDER BY dead_ratio DESC;\"\n   ```\n\n4. **Restart database connections** in the application if needed:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl -X POST localhost:8080/actuator/restart-db-connections\n   ```\n\n### Cache Issues\n\nIf there are Redis cache issues:\n\n1. **Check Redis status**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=redis -n data -o jsonpath='{.items[0].metadata.name}') -n data -- redis-cli ping\n   ```\n\n2. **Check Redis memory usage**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=redis -n data -o jsonpath='{.items[0].metadata.name}') -n data -- redis-cli info memory\n   ```\n\n3. **Check cache hit rate**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=redis -n data -o jsonpath='{.items[0].metadata.name}') -n data -- redis-cli info stats | grep hit_rate\n   ```\n\n4. **Clear cache** if necessary:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl -X POST localhost:8080/actuator/caches/clearAll\n   ```\n\n### Stock Management\n\nFor critical stock issues:\n\n1. **Identify products with low or no stock**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl localhost:8080/internal/api/inventory/low-stock\n   ```\n\n2. **Check for stuck inventory reservations**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl localhost:8080/internal/api/inventory/stuck-reservations\n   ```\n\n3. **Release expired reservations** if necessary:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl -X POST localhost:8080/internal/api/inventory/release-expired-reservations\n   ```\n\n4. **Manually update inventory levels** for emergency corrections:\n   ```bash\n   curl -X PUT https://api.internal.flowmart.com/inventory/products/{productId}/stock \\\n     -H \"Authorization: Bearer $ADMIN_TOKEN\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"warehouseId\": \"WAREHOUSE_ID\", \"quantity\": 100, \"reason\": \"Manual correction\"}'\n   ```\n\n## Common Operational Tasks\n\n### Scaling the Service\n\nTo scale the service horizontally:\n\n```bash\nkubectl scale deployment inventory-service -n inventory --replicas=\u003Cnumber>\n```\n\n### Restarting the Service\n\nTo restart all pods:\n\n```bash\nkubectl rollout restart deployment inventory-service -n inventory\n```\n\n### Database Maintenance\n\nFor routine database maintenance:\n\n1. **Run VACUUM ANALYZE to optimize tables**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -c \"VACUUM ANALYZE inventory_items;\"\n   ```\n\n2. **Update database statistics**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -c \"ANALYZE;\"\n   ```\n\n### Reconcile Inventory\n\nTo reconcile inventory with the warehouse management system:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl -X POST localhost:8080/internal/api/inventory/reconcile\n```\n\n### Manually Trigger Restock Notifications\n\nTo trigger restock notifications for low stock items:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl -X POST localhost:8080/internal/api/inventory/trigger-restock-notifications\n```\n\n## Recovery Procedures\n\n### Database Failure Recovery\n\nIf the PostgreSQL database becomes unavailable:\n\n1. Verify the status of the PostgreSQL cluster:\n   ```bash\n   kubectl get pods -l app=postgresql -n data\n   ```\n\n2. If the primary instance is down, check if automatic failover has occurred:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql-patroni -n data -o jsonpath='{.items[0].metadata.name}') -n data -- patronictl list\n   ```\n\n3. If automatic failover has not occurred, initiate manual failover:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql-patroni -n data -o jsonpath='{.items[0].metadata.name}') -n data -- patronictl failover\n   ```\n\n4. Once database availability is restored, validate the InventoryService functionality:\n   ```bash\n   curl -X GET https://api.internal.flowmart.com/inventory/health\n   ```\n\n### Cache Failure Recovery\n\nIf the Redis cache becomes unavailable:\n\n1. Verify Redis cluster status:\n   ```bash\n   kubectl get pods -l app=redis -n data\n   ```\n\n2. If needed, restart the Redis cluster:\n   ```bash\n   kubectl rollout restart statefulset redis -n data\n   ```\n\n3. The InventoryService will fall back to database queries when the cache is unavailable.\n\n4. When the cache is restored, you can warm it up:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=inventory-service -n inventory -o jsonpath='{.items[0].metadata.name}') -n inventory -- curl -X POST localhost:8080/internal/api/inventory/warm-cache\n   ```\n\n## Disaster Recovery\n\n### Complete Service Failure\n\nIn case of a complete service failure:\n\n1. Initiate incident response by notifying the on-call team through PagerDuty.\n\n2. Verify the deployment status:\n   ```bash\n   kubectl describe deployment inventory-service -n inventory\n   ```\n\n3. If necessary, restore from a previous version:\n   ```bash\n   kubectl rollout undo deployment inventory-service -n inventory\n   ```\n\n4. If the primary region is experiencing issues, fail over to the secondary region:\n   ```bash\n   ./scripts/dr-failover.sh inventory-service\n   ```\n\n5. Verify the service is functioning in the secondary region:\n   ```bash\n   curl -X GET https://api-dr.internal.flowmart.com/inventory/health\n   ```\n\n## Maintenance Tasks\n\n### Deploying New Versions\n\n```bash\nkubectl set image deployment/inventory-service -n inventory inventory-service=ecr.aws/flowmart/inventory-service:$VERSION\n```\n\n### Database Schema Updates\n\nFor database schema updates:\n\n1. Notify stakeholders through the #maintenance Slack channel.\n\n2. Set InventoryService to maintenance mode:\n   ```bash\n   curl -X POST https://api.internal.flowmart.com/inventory/admin/maintenance -H \"Authorization: Bearer $ADMIN_TOKEN\" -H \"Content-Type: application/json\" -d '{\"maintenanceMode\": true, \"message\": \"Database schema update\"}'\n   ```\n\n3. Apply the database migrations:\n   ```bash\n   kubectl apply -f inventory-flyway-job.yaml\n   ```\n\n4. Verify migration completion:\n   ```bash\n   kubectl logs -l job-name=inventory-flyway-migration -n inventory\n   ```\n\n5. Turn off maintenance mode:\n   ```bash\n   curl -X POST https://api.internal.flowmart.com/inventory/admin/maintenance -H \"Authorization: Bearer $ADMIN_TOKEN\" -H \"Content-Type: application/json\" -d '{\"maintenanceMode\": false}'\n   ```\n\n## Contact Information\n\n**Primary On-Call:** Inventory Team (rotating schedule)  \n**Secondary On-Call:** Platform Team  \n**Escalation Path:** Inventory Team Lead > Engineering Manager > CTO\n\n**Slack Channels:**\n- #inventory-support (primary support channel)\n- #inventory-alerts (automated alerts)\n- #incident-response (for major incidents)\n\n## Reference Information\n\n- [InventoryService API Documentation](https://docs.internal.flowmart.com/inventory/api)\n- [Architecture Diagram](https://docs.internal.flowmart.com/architecture/inventory)\n- [Service Level Objectives (SLOs)](https://docs.internal.flowmart.com/slo/inventory)\n- [Database Schema](https://docs.internal.flowmart.com/inventory/database-schema)",
  "../examples/default/docs/operations-and-support/runbooks/inventory-service-runbook.mdx",
  "840c138ba368cf77",
  "docs/guides/creating-new-microservices/06-typescript-service",
  {
    "id": 577,
    "data": 579,
    "body": 585,
    "filePath": 586,
    "digest": 587,
    "deferredRender": 20
  },
  { "title": 580, "summary": 581, "sidebar": 582 },
  "New TypeScript service",
  "Guide to implementing microservices using TypeScript at FlowMart",
  { "label": 583, "order": 584 },
  "TypeScript Service",
  4,
  "This guide details the recommended patterns, practices, and tools for implementing TypeScript-based microservices at FlowMart.\n\n\n## Why TypeScript?\n\nAt FlowMart, we recommend TypeScript for new microservices because it offers:\n\n- **Type Safety**: Catch errors during development instead of at runtime\n- **Better IDE Support**: Enhanced auto-completion, navigation, and refactoring\n- **Self-Documenting Code**: Types serve as documentation for your codebase\n- **Enterprise-Ready**: Better maintainability for large codebases and teams\n- **Ecosystem Compatibility**: Full access to the Node.js ecosystem\n\n## Prerequisites\n\nBefore you begin:\n\n- Install Node.js (v18 or later)\n- Familiarize yourself with [our Node.js service guide](/docs/guides/creating-new-microservices/node-service)\n- Have basic TypeScript knowledge\n\n## Scaffolding a TypeScript Service\n\nUse our service generator to create a TypeScript service:\n\n```bash\n# Install the FlowMart service generator\nnpm install -g @flowmart/service-generator\n\n# Create a new TypeScript service\nflowmart-create-service my-service --type typescript\n```\n\n## TypeScript Project Structure\n\nThe generated service follows our standard TypeScript structure:\n\n```\nmy-service/\n├── src/\n│   ├── api/                 # API definition and controllers\n│   ├── config/              # Configuration management\n│   ├── domain/              # Domain models and business logic\n│   │   ├── models/          # Domain entities and value objects\n│   │   └── services/        # Domain services\n│   ├── events/              # Event producers and consumers\n│   ├── infrastructure/      # External dependencies and adapters\n│   ├── repositories/        # Data access layer\n│   ├── types/               # TypeScript type definitions\n│   ├── utils/               # Utility functions\n│   └── app.ts               # Application entry point\n├── test/\n│   ├── unit/                # Unit tests\n│   ├── integration/         # Integration tests\n│   └── contract/            # Contract tests\n├── terraform/               # Infrastructure as code\n├── .github/                 # GitHub Actions workflows\n├── Dockerfile               # Container definition\n├── docker-compose.yml       # Local development setup\n├── tsconfig.json            # TypeScript configuration\n├── package.json             # Dependencies and scripts\n└── README.md                # Service documentation\n```\n\n## TypeScript Configuration\n\nOur template includes a pre-configured `tsconfig.json`:\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"NodeNext\",\n    \"moduleResolution\": \"NodeNext\",\n    \"lib\": [\"ES2020\"],\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true,\n    \"sourceMap\": true,\n    \"declaration\": true,\n    \"experimentalDecorators\": true,\n    \"emitDecoratorMetadata\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\", \"test\"]\n}\n```\n\n## Domain Modeling with TypeScript\n\nTypeScript enables us to model our domain with strong types:\n\n### Example Domain Model\n\n```typescript\n// src/domain/models/product.ts\nexport enum ProductCategory {\n  ELECTRONICS = 'electronics',\n  CLOTHING = 'clothing',\n  GROCERY = 'grocery',\n  HOME = 'home',\n  BEAUTY = 'beauty'\n}\n\nexport interface ProductAttributes {\n  [key: string]: string | number | boolean;\n}\n\nexport class Product {\n  constructor(\n    public readonly id: string,\n    public name: string,\n    public summary: string,\n    public price: number,\n    public category: ProductCategory,\n    public sku: string,\n    public inventoryCount: number,\n    public attributes: ProductAttributes = {},\n    public isActive: boolean = true,\n    public createdAt: Date = new Date(),\n    public updatedAt: Date = new Date()\n  ) {}\n\n  updateInventory(count: number): void {\n    if (count \u003C 0) {\n      throw new Error('Inventory count cannot be negative');\n    }\n    this.inventoryCount = count;\n    this.updatedAt = new Date();\n  }\n\n  updatePrice(price: number): void {\n    if (price \u003C 0) {\n      throw new Error('Price cannot be negative');\n    }\n    this.price = price;\n    this.updatedAt = new Date();\n  }\n\n  deactivate(): void {\n    this.isActive = false;\n    this.updatedAt = new Date();\n  }\n\n  activate(): void {\n    this.isActive = true;\n    this.updatedAt = new Date();\n  }\n\n  isInStock(): boolean {\n    return this.inventoryCount > 0;\n  }\n}\n```\n\n## Repository Pattern with TypeScript\n\nUse interfaces to define repositories:\n\n```typescript\n// src/repositories/product-repository.ts\nimport { Product } from '../domain/models/product';\n\nexport interface ProductRepository {\n  findById(id: string): Promise\u003CProduct | null>;\n  findAll(limit?: number, offset?: number): Promise\u003CProduct[]>;\n  findByCategory(category: string, limit?: number, offset?: number): Promise\u003CProduct[]>;\n  save(product: Product): Promise\u003CProduct>;\n  update(product: Product): Promise\u003CProduct>;\n  delete(id: string): Promise\u003Cboolean>;\n}\n\n// src/repositories/mongodb-product-repository.ts\nimport { Collection, MongoClient, ObjectId } from 'mongodb';\nimport { Product, ProductCategory } from '../domain/models/product';\nimport { ProductRepository } from './product-repository';\nimport { logger } from '../infrastructure/observability';\n\nexport class MongoDBProductRepository implements ProductRepository {\n  private collection: Collection;\n\n  constructor(client: MongoClient) {\n    this.collection = client.db('ecommerce').collection('products');\n  }\n\n  async findById(id: string): Promise\u003CProduct | null> {\n    try {\n      const result = await this.collection.findOne({ _id: new ObjectId(id) });\n      if (!result) return null;\n      return this.mapToProduct(result);\n    } catch (error) {\n      logger.error('Error finding product by id', { error, id });\n      throw error;\n    }\n  }\n\n  async findAll(limit = 100, offset = 0): Promise\u003CProduct[]> {\n    try {\n      const results = await this.collection\n        .find({})\n        .skip(offset)\n        .limit(limit)\n        .toArray();\n      return results.map(this.mapToProduct);\n    } catch (error) {\n      logger.error('Error finding all products', { error });\n      throw error;\n    }\n  }\n\n  async findByCategory(category: string, limit = 100, offset = 0): Promise\u003CProduct[]> {\n    try {\n      const results = await this.collection\n        .find({ category })\n        .skip(offset)\n        .limit(limit)\n        .toArray();\n      return results.map(this.mapToProduct);\n    } catch (error) {\n      logger.error('Error finding products by category', { error, category });\n      throw error;\n    }\n  }\n\n  async save(product: Product): Promise\u003CProduct> {\n    try {\n      const productDoc = {\n        name: product.name,\n        summary: product.description,\n        price: product.price,\n        category: product.category,\n        sku: product.sku,\n        inventoryCount: product.inventoryCount,\n        attributes: product.attributes,\n        isActive: product.isActive,\n        createdAt: product.createdAt,\n        updatedAt: product.updatedAt\n      };\n      \n      const result = await this.collection.insertOne(productDoc);\n      return {\n        ...product,\n        id: result.insertedId.toString()\n      };\n    } catch (error) {\n      logger.error('Error saving product', { error, product });\n      throw error;\n    }\n  }\n\n  async update(product: Product): Promise\u003CProduct> {\n    try {\n      const result = await this.collection.updateOne(\n        { _id: new ObjectId(product.id) },\n        {\n          $set: {\n            name: product.name,\n            summary: product.description,\n            price: product.price,\n            category: product.category,\n            sku: product.sku,\n            inventoryCount: product.inventoryCount,\n            attributes: product.attributes,\n            isActive: product.isActive,\n            updatedAt: product.updatedAt\n          }\n        }\n      );\n      \n      if (result.matchedCount === 0) {\n        throw new Error(`Product with id ${product.id} not found`);\n      }\n      \n      return product;\n    } catch (error) {\n      logger.error('Error updating product', { error, productId: product.id });\n      throw error;\n    }\n  }\n\n  async delete(id: string): Promise\u003Cboolean> {\n    try {\n      const result = await this.collection.deleteOne({ _id: new ObjectId(id) });\n      return result.deletedCount === 1;\n    } catch (error) {\n      logger.error('Error deleting product', { error, id });\n      throw error;\n    }\n  }\n\n  private mapToProduct(doc: any): Product {\n    return new Product(\n      doc._id.toString(),\n      doc.name,\n      doc.description,\n      doc.price,\n      doc.category as ProductCategory,\n      doc.sku,\n      doc.inventoryCount,\n      doc.attributes,\n      doc.isActive,\n      new Date(doc.createdAt),\n      new Date(doc.updatedAt)\n    );\n  }\n}\n```\n\n## Type-safe API Controllers\n\nTypeScript enables type-safe API controllers with Express:\n\n```typescript\n// src/api/controllers/product-controller.ts\nimport { Router, Request, Response, NextFunction } from 'express';\nimport { ProductService } from '../../domain/services/product-service';\nimport { validateProduct } from '../middleware/product-validator';\nimport { tracing } from '../../infrastructure/observability';\nimport { Product, ProductCategory } from '../../domain/models/product';\n\nexport interface CreateProductRequest {\n  name: string;\n  summary: string;\n  price: number;\n  category: ProductCategory;\n  sku: string;\n  inventoryCount: number;\n  attributes?: { [key: string]: string | number | boolean };\n}\n\nexport interface UpdateProductRequest {\n  name?: string;\n  description?: string;\n  price?: number;\n  category?: ProductCategory;\n  sku?: string;\n  inventoryCount?: number;\n  attributes?: { [key: string]: string | number | boolean };\n  isActive?: boolean;\n}\n\nexport class ProductController {\n  private router: Router;\n  \n  constructor(private productService: ProductService) {\n    this.router = Router();\n    this.setupRoutes();\n  }\n  \n  private setupRoutes(): void {\n    this.router.get('/', tracing.middleware('get-all-products'), this.getAllProducts.bind(this));\n    this.router.get('/:id', tracing.middleware('get-product'), this.getProductById.bind(this));\n    this.router.post('/', tracing.middleware('create-product'), validateProduct, this.createProduct.bind(this));\n    this.router.put('/:id', tracing.middleware('update-product'), this.updateProduct.bind(this));\n    this.router.delete('/:id', tracing.middleware('delete-product'), this.deleteProduct.bind(this));\n  }\n  \n  getRouter(): Router {\n    return this.router;\n  }\n  \n  private async getAllProducts(req: Request, res: Response, next: NextFunction): Promise\u003Cvoid> {\n    try {\n      const limit = req.query.limit ? parseInt(req.query.limit as string, 10) : 100;\n      const offset = req.query.offset ? parseInt(req.query.offset as string, 10) : 0;\n      const products = await this.productService.getAllProducts(limit, offset);\n      res.json(products);\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  private async getProductById(req: Request, res: Response, next: NextFunction): Promise\u003Cvoid> {\n    try {\n      const product = await this.productService.getProductById(req.params.id);\n      if (!product) {\n        res.status(404).json({ error: 'Product not found' });\n        return;\n      }\n      res.json(product);\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  private async createProduct(req: Request, res: Response, next: NextFunction): Promise\u003Cvoid> {\n    try {\n      const productData = req.body as CreateProductRequest;\n      const product = await this.productService.createProduct(productData);\n      res.status(201).json(product);\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  private async updateProduct(req: Request, res: Response, next: NextFunction): Promise\u003Cvoid> {\n    try {\n      const productData = req.body as UpdateProductRequest;\n      const product = await this.productService.updateProduct(req.params.id, productData);\n      if (!product) {\n        res.status(404).json({ error: 'Product not found' });\n        return;\n      }\n      res.json(product);\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  private async deleteProduct(req: Request, res: Response, next: NextFunction): Promise\u003Cvoid> {\n    try {\n      const success = await this.productService.deleteProduct(req.params.id);\n      if (!success) {\n        res.status(404).json({ error: 'Product not found' });\n        return;\n      }\n      res.status(204).send();\n    } catch (error) {\n      next(error);\n    }\n  }\n}\n```\n\n## Strongly Typed Event Handling\n\nCreate strongly typed event producers and consumers:\n\n```typescript\n// src/events/types/event-types.ts\nexport interface EventMetadata {\n  eventId: string;\n  timestamp: string;\n  service: string;\n  correlationId?: string;\n  causationId?: string;\n}\n\nexport interface Event\u003CT> {\n  type: string;\n  data: T;\n  metadata: EventMetadata;\n}\n\nexport interface ProductCreatedEvent extends Event\u003C{\n  id: string;\n  name: string;\n  price: number;\n  category: string;\n  sku: string;\n  inventoryCount: number;\n}> {\n  type: 'PRODUCT_CREATED';\n}\n\nexport interface ProductUpdatedEvent extends Event\u003C{\n  id: string;\n  changes: {\n    name?: string;\n    price?: number;\n    category?: string;\n    inventoryCount?: number;\n  };\n}> {\n  type: 'PRODUCT_UPDATED';\n}\n\nexport interface InventoryUpdatedEvent extends Event\u003C{\n  productId: string;\n  quantity: number;\n  warehouseId: string;\n}> {\n  type: 'INVENTORY_UPDATED';\n}\n\n// src/events/producers/product-event-producer.ts\nimport { v4 as uuidv4 } from 'uuid';\nimport { KafkaClient } from '../../infrastructure/kafka';\nimport { Product } from '../../domain/models/product';\nimport { ProductCreatedEvent, ProductUpdatedEvent } from '../types/event-types';\n\nexport class ProductEventProducer {\n  private readonly topic = 'product-events';\n  \n  constructor(private kafkaClient: KafkaClient) {}\n\n  async productCreated(product: Product): Promise\u003Cvoid> {\n    const event: ProductCreatedEvent = {\n      type: 'PRODUCT_CREATED',\n      data: {\n        id: product.id,\n        name: product.name,\n        price: product.price,\n        category: product.category,\n        sku: product.sku,\n        inventoryCount: product.inventoryCount\n      },\n      metadata: {\n        eventId: uuidv4(),\n        timestamp: new Date().toISOString(),\n        service: 'product-service'\n      }\n    };\n\n    await this.kafkaClient.produce({\n      topic: this.topic,\n      key: product.id,\n      value: JSON.stringify(event)\n    });\n  }\n\n  async productUpdated(product: Product, changes: Partial\u003CProduct>): Promise\u003Cvoid> {\n    const event: ProductUpdatedEvent = {\n      type: 'PRODUCT_UPDATED',\n      data: {\n        id: product.id,\n        changes: {\n          name: changes.name,\n          price: changes.price,\n          category: changes.category,\n          inventoryCount: changes.inventoryCount\n        }\n      },\n      metadata: {\n        eventId: uuidv4(),\n        timestamp: new Date().toISOString(),\n        service: 'product-service'\n      }\n    };\n\n    await this.kafkaClient.produce({\n      topic: this.topic,\n      key: product.id,\n      value: JSON.stringify(event)\n    });\n  }\n}\n```\n\n## Dependency Injection with TypeScript\n\nWe use the `inversify` library for dependency injection:\n\n```typescript\n// src/infrastructure/ioc/container.ts\nimport { Container } from 'inversify';\nimport { MongoClient } from 'mongodb';\nimport { KafkaClient } from '../kafka';\nimport { ProductRepository } from '../../repositories/product-repository';\nimport { MongoDBProductRepository } from '../../repositories/mongodb-product-repository';\nimport { ProductService } from '../../domain/services/product-service';\nimport { ProductEventProducer } from '../../events/producers/product-event-producer';\nimport { ProductController } from '../../api/controllers/product-controller';\nimport { AppConfig } from '../../config/app-config';\nimport TYPES from './types';\n\nconst container = new Container();\n\n// Config\ncontainer.bind\u003CAppConfig>(TYPES.AppConfig).to(AppConfig).inSingletonScope();\n\n// Infrastructure\ncontainer.bind\u003CMongoClient>(TYPES.MongoClient).toDynamicValue((context) => {\n  const config = context.container.get\u003CAppConfig>(TYPES.AppConfig);\n  return new MongoClient(config.mongoDbUri);\n}).inSingletonScope();\n\ncontainer.bind\u003CKafkaClient>(TYPES.KafkaClient).toDynamicValue((context) => {\n  const config = context.container.get\u003CAppConfig>(TYPES.AppConfig);\n  return new KafkaClient(config.kafkaBrokers);\n}).inSingletonScope();\n\n// Repositories\ncontainer.bind\u003CProductRepository>(TYPES.ProductRepository).toDynamicValue((context) => {\n  const mongoClient = context.container.get\u003CMongoClient>(TYPES.MongoClient);\n  return new MongoDBProductRepository(mongoClient);\n}).inSingletonScope();\n\n// Event Producers\ncontainer.bind\u003CProductEventProducer>(TYPES.ProductEventProducer).toDynamicValue((context) => {\n  const kafkaClient = context.container.get\u003CKafkaClient>(TYPES.KafkaClient);\n  return new ProductEventProducer(kafkaClient);\n}).inSingletonScope();\n\n// Services\ncontainer.bind\u003CProductService>(TYPES.ProductService).toDynamicValue((context) => {\n  const repository = context.container.get\u003CProductRepository>(TYPES.ProductRepository);\n  const eventProducer = context.container.get\u003CProductEventProducer>(TYPES.ProductEventProducer);\n  return new ProductService(repository, eventProducer);\n}).inSingletonScope();\n\n// Controllers\ncontainer.bind\u003CProductController>(TYPES.ProductController).toDynamicValue((context) => {\n  const service = context.container.get\u003CProductService>(TYPES.ProductService);\n  return new ProductController(service);\n}).inSingletonScope();\n\nexport default container;\n\n// src/infrastructure/ioc/types.ts\nconst TYPES = {\n  AppConfig: Symbol.for('AppConfig'),\n  MongoClient: Symbol.for('MongoClient'),\n  KafkaClient: Symbol.for('KafkaClient'),\n  ProductRepository: Symbol.for('ProductRepository'),\n  ProductEventProducer: Symbol.for('ProductEventProducer'),\n  ProductService: Symbol.for('ProductService'),\n  ProductController: Symbol.for('ProductController')\n};\n\nexport default TYPES;\n```\n\n## Error Handling with TypeScript\n\nUse custom error classes:\n\n```typescript\n// src/utils/errors.ts\nexport class AppError extends Error {\n  constructor(\n    public readonly message: string,\n    public readonly statusCode: number = 500,\n    public readonly code: string = 'INTERNAL_ERROR',\n    public readonly details?: any\n  ) {\n    super(message);\n    this.name = this.constructor.name;\n    Error.captureStackTrace(this, this.constructor);\n  }\n}\n\nexport class NotFoundError extends AppError {\n  constructor(resource: string, id: string) {\n    super(`${resource} with id ${id} not found`, 404, 'NOT_FOUND');\n    this.name = this.constructor.name;\n  }\n}\n\nexport class ValidationError extends AppError {\n  constructor(message: string, details?: any) {\n    super(message, 400, 'VALIDATION_ERROR', details);\n    this.name = this.constructor.name;\n  }\n}\n\nexport class ConflictError extends AppError {\n  constructor(message: string) {\n    super(message, 409, 'CONFLICT_ERROR');\n    this.name = this.constructor.name;\n  }\n}\n\nexport class AuthorizationError extends AppError {\n  constructor(message: string = 'Unauthorized') {\n    super(message, 401, 'UNAUTHORIZED');\n    this.name = this.constructor.name;\n  }\n}\n\n// src/api/middleware/error-handler.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { AppError } from '../../utils/errors';\nimport { logger } from '../../infrastructure/observability';\n\nexport function errorHandler(\n  error: Error, \n  req: Request, \n  res: Response, \n  next: NextFunction\n): void {\n  logger.error('Request error', {\n    error: error.message,\n    stack: error.stack,\n    path: req.path,\n    method: req.method\n  });\n\n  if (error instanceof AppError) {\n    res.status(error.statusCode).json({\n      error: {\n        code: error.code,\n        message: error.message,\n        details: error.details\n      }\n    });\n    return;\n  }\n\n  res.status(500).json({\n    error: {\n      code: 'INTERNAL_SERVER_ERROR',\n      message: 'An unexpected error occurred'\n    }\n  });\n}\n```\n\n## Testing TypeScript Services\n\nWe use Jest for testing TypeScript services:\n\n```typescript\n// test/unit/domain/services/product-service.test.ts\nimport { ProductService } from '../../../../src/domain/services/product-service';\nimport { Product, ProductCategory } from '../../../../src/domain/models/product';\nimport { NotFoundError } from '../../../../src/utils/errors';\n\ndescribe('ProductService', () => {\n  const mockProduct = new Product(\n    '1',\n    'Test Product',\n    'Test Description',\n    9.99,\n    ProductCategory.ELECTRONICS,\n    'TEST-123',\n    100\n  );\n\n  const mockRepository = {\n    findById: jest.fn(),\n    findAll: jest.fn(),\n    findByCategory: jest.fn(),\n    save: jest.fn(),\n    update: jest.fn(),\n    delete: jest.fn()\n  };\n\n  const mockEventProducer = {\n    productCreated: jest.fn(),\n    productUpdated: jest.fn()\n  };\n\n  const productService = new ProductService(mockRepository, mockEventProducer);\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  describe('getProductById', () => {\n    it('should return a product when found', async () => {\n      mockRepository.findById.mockResolvedValue(mockProduct);\n      \n      const result = await productService.getProductById('1');\n      \n      expect(result).toEqual(mockProduct);\n      expect(mockRepository.findById).toHaveBeenCalledWith('1');\n    });\n\n    it('should return null when product not found', async () => {\n      mockRepository.findById.mockResolvedValue(null);\n      \n      const result = await productService.getProductById('999');\n      \n      expect(result).toBeNull();\n      expect(mockRepository.findById).toHaveBeenCalledWith('999');\n    });\n  });\n\n  describe('createProduct', () => {\n    it('should create and return a new product', async () => {\n      const productData = {\n        name: 'New Product',\n        summary: 'New Description',\n        price: 19.99,\n        category: ProductCategory.CLOTHING,\n        sku: 'NEW-123',\n        inventoryCount: 50\n      };\n      \n      mockRepository.save.mockImplementation(product => Promise.resolve(product));\n      \n      const result = await productService.createProduct(productData);\n      \n      expect(result).toMatchObject(productData);\n      expect(mockRepository.save).toHaveBeenCalledTimes(1);\n      expect(mockEventProducer.productCreated).toHaveBeenCalledWith(expect.objectContaining(productData));\n    });\n  });\n\n  describe('updateProduct', () => {\n    it('should update and return the product', async () => {\n      const changes = { price: 29.99, inventoryCount: 75 };\n      const updatedProduct = { ...mockProduct, ...changes };\n      \n      mockRepository.findById.mockResolvedValue(mockProduct);\n      mockRepository.update.mockResolvedValue(updatedProduct);\n      \n      const result = await productService.updateProduct('1', changes);\n      \n      expect(result).toEqual(updatedProduct);\n      expect(mockRepository.findById).toHaveBeenCalledWith('1');\n      expect(mockRepository.update).toHaveBeenCalledWith(expect.objectContaining(changes));\n      expect(mockEventProducer.productUpdated).toHaveBeenCalledWith(\n        expect.anything(),\n        expect.objectContaining(changes)\n      );\n    });\n\n    it('should throw NotFoundError when product not found', async () => {\n      mockRepository.findById.mockResolvedValue(null);\n      \n      await expect(productService.updateProduct('999', { price: 29.99 }))\n        .rejects\n        .toThrow(NotFoundError);\n      \n      expect(mockRepository.update).not.toHaveBeenCalled();\n      expect(mockEventProducer.productUpdated).not.toHaveBeenCalled();\n    });\n  });\n});\n```\n\n## Building and Packaging TypeScript Services\n\nOur template includes optimized build scripts:\n\n```json\n// package.json (excerpt)\n{\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node dist/app.js\",\n    \"dev\": \"ts-node-dev --respawn --transpile-only src/app.ts\",\n    \"lint\": \"eslint src --ext .ts\",\n    \"test\": \"jest\",\n    \"test:watch\": \"jest --watch\",\n    \"test:coverage\": \"jest --coverage\"\n  }\n}\n```\n\n## Dockerizing TypeScript Services\n\nOur Docker setup uses multi-stage builds for optimal container size:\n\n```dockerfile\n# Dockerfile\nFROM node:18-alpine AS builder\n\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm ci\n\nCOPY tsconfig.json ./\nCOPY src/ ./src/\n\nRUN npm run build\n\nFROM node:18-alpine\n\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm ci --production\n\nCOPY --from=builder /app/dist ./dist\n\nENV NODE_ENV=production\nEXPOSE 3000\n\nHEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \\\n  CMD wget -qO- http://localhost:3000/health || exit 1\n\nCMD [\"node\", \"dist/app.js\"]\n```\n\n## CI/CD for TypeScript Services\n\nOur CI/CD workflow ensures proper TypeScript builds:\n\n```yaml\n# .github/workflows/main.yml (excerpt)\njobs:\n  test:\n    name: Test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm test\n      \n  build:\n    name: Build and Push\n    needs: test\n    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run build\n      # Then Docker build and push steps...\n```\n\n## TypeScript Best Practices at FlowMart\n\n1. **Use Interfaces for Public APIs**: Define interfaces for repositories, services, and controllers.\n\n2. **Embrace Type Inference**: Let TypeScript infer types where it makes sense to reduce verbosity.\n\n3. **Use Discriminated Unions**: For handling different event types or command patterns.\n\n4. **Leverage Utility Types**: Use built-in utility types like `Partial\u003CT>`, `Readonly\u003CT>`, `Pick\u003CT>`, etc.\n\n5. **Strict Null Checks**: Always enable `strictNullChecks` to prevent null/undefined errors.\n\n6. **Immutability**: Use `readonly` properties and `const` variables to enforce immutability.\n\n7. **Error Handling**: Use custom error classes with type information.\n\n8. **Asynchronous Code**: Use `async/await` consistently with proper error handling.\n\n9. **Use Enums for Constants**: Define related constants as TypeScript enums.\n\n10. **Module Structure**: Organize code into cohesive modules with clear interfaces.\n\n## Recommended Libraries\n\n- **Express.js**: Web framework\n- **inversify**: Dependency injection\n- **zod**: Runtime validation\n- **winston**: Logging\n- **prom-client**: Prometheus metrics\n- **jaeger-client**: Distributed tracing\n- **mongodb**: Database driver\n- **kafkajs**: Kafka client\n- **jest**: Testing framework\n- **supertest**: API testing\n- **ts-node-dev**: Development server\n\n## Next Steps\n\n- Learn about [Database patterns for microservices](/docs/guides/creating-new-microservices/database-patterns)\n- Understand [Event schema design](/docs/guides/creating-new-microservices/event-schemas)\n- Explore [API design best practices](/docs/guides/creating-new-microservices/api-design)",
  "../examples/default/docs/guides/creating-new-microservices/06-typescript-service.mdx",
  "b1621456cd4d21af",
  "docs/operations-and-support/runbooks/orders-service-runbook",
  {
    "id": 588,
    "data": 590,
    "body": 595,
    "filePath": 596,
    "digest": 597,
    "deferredRender": 20
  },
  { "title": 591, "summary": 592, "sidebar": 593 },
  "OrdersService Runbook",
  "Operational runbook for troubleshooting and maintaining the OrdersService",
  { "label": 594, "order": 470 },
  "OrdersService",
  "This runbook provides operational procedures for the OrdersService, which is responsible for managing the entire lifecycle of customer orders in the FlowMart e-commerce platform.\n\n\n## Architecture\n\nThe OrdersService is responsible for:\n- Creating and processing customer orders\n- Tracking order status throughout fulfillment\n- Coordinating with other services (Inventory, Payment, Shipping)\n- Managing order history and amendments\n\n### Service Dependencies\n\n```mermaid\nflowchart TD\n    OrdersService --> InventoryService[Inventory Service]\n    OrdersService --> PaymentService[Payment Service]\n    OrdersService --> ShippingService[Shipping Service]\n    OrdersService --> NotificationService[Notification Service]\n    OrdersService --> OrdersDB[(Orders Database)]\n    OrdersService --> EventBus[Event Bus]\n```\n\n## Monitoring and Alerting\n\n### Key Metrics\n\n| Metric | Description | Warning Threshold | Critical Threshold |\n|--------|-------------|-------------------|-------------------|\n| `order_creation_rate` | Orders created per minute | \u003C 5 | \u003C 1 |\n| `order_creation_latency` | Time to create an order | > 2s | > 5s |\n| `order_error_rate` | Percentage of failed orders | > 1% | > 5% |\n| `database_connection_pool` | Database connection pool utilization | > 70% | > 90% |\n| `memory_usage` | Container memory usage | > 80% | > 90% |\n| `cpu_usage` | Container CPU usage | > 70% | > 85% |\n\n### Dashboards\n\n- [OrdersService Overview](https://grafana.flowmart.com/d/orders-overview)\n- [OrdersService API Metrics](https://grafana.flowmart.com/d/orders-api)\n- [OrdersService Error Tracking](https://grafana.flowmart.com/d/orders-errors)\n\n### Common Alerts\n\n| Alert | Description | Troubleshooting Steps |\n|-------|-------------|----------------------|\n| `OrdersServiceHighLatency` | API latency exceeds thresholds | See [High Latency](#high-latency) |\n| `OrdersServiceHighErrorRate` | Error rate exceeds thresholds | See [High Error Rate](#high-error-rate) |\n| `OrdersServiceDatabaseConnectionIssues` | Database connection issues | See [Database Issues](#database-issues) |\n\n## Troubleshooting Guides\n\n### High Latency\n\nIf the service is experiencing high latency:\n\n1. **Check system metrics**:\n   ```bash\n   kubectl top pods -n orders\n   ```\n\n2. **Check database metrics** in the MongoDB dashboard to identify slow queries.\n\n3. **Check dependent services** to see if delays are caused by downstream systems:\n   ```bash\n   curl -X GET https://api.internal.flowmart.com/inventory/health\n   curl -X GET https://api.internal.flowmart.com/payment/health\n   ```\n\n4. **Analyze recent changes** that might have impacted performance.\n\n5. **Scale the service** if needed:\n   ```bash\n   kubectl scale deployment orders-service -n orders --replicas=5\n   ```\n\n### High Error Rate\n\nIf the service is experiencing a high error rate:\n\n1. **Check application logs**:\n   ```bash\n   kubectl logs -l app=orders-service -n orders --tail=100\n   ```\n\n2. **Check for recent deployments** that might have introduced issues:\n   ```bash\n   kubectl rollout history deployment/orders-service -n orders\n   ```\n\n3. **Verify database connectivity**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=orders-service -n orders -o jsonpath='{.items[0].metadata.name}') -n orders -- node -e \"const mongoose = require('mongoose'); mongoose.connect(process.env.MONGODB_URI).then(() => console.log('Connected!')).catch(err => console.error('Connection error', err));\"\n   ```\n\n4. **Check dependent services** for failures:\n   ```bash\n   curl -X GET https://api.internal.flowmart.com/inventory/health\n   curl -X GET https://api.internal.flowmart.com/payment/health\n   ```\n\n5. **Consider rolling back** if issues persist:\n   ```bash\n   kubectl rollout undo deployment/orders-service -n orders\n   ```\n\n### Database Issues\n\nIf there are database connection issues:\n\n1. **Check MongoDB status**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo admin -u admin -p $MONGODB_PASSWORD --eval \"db.serverStatus()\"\n   ```\n\n2. **Verify network connectivity**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=orders-service -n orders -o jsonpath='{.items[0].metadata.name}') -n orders -- ping mongodb.data.svc.cluster.local\n   ```\n\n3. **Check MongoDB resource usage**:\n   ```bash\n   kubectl top pods -l app=mongodb -n data\n   ```\n\n4. **Review MongoDB logs**:\n   ```bash\n   kubectl logs -l app=mongodb -n data --tail=100\n   ```\n\n## Common Operational Tasks\n\n### Scaling the Service\n\nTo scale the service horizontally:\n\n```bash\nkubectl scale deployment orders-service -n orders --replicas=\u003Cnumber>\n```\n\n### Restarting the Service\n\nTo restart all pods:\n\n```bash\nkubectl rollout restart deployment orders-service -n orders\n```\n\n### Viewing Recent Orders\n\nTo view recent orders in the database:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=orders-service -n orders -o jsonpath='{.items[0].metadata.name}') -n orders -- node -e \"const mongoose = require('mongoose'); const Order = require('./models/order'); mongoose.connect(process.env.MONGODB_URI).then(async () => { const orders = await Order.find().sort({createdAt: -1}).limit(10); console.log(JSON.stringify(orders, null, 2)); process.exit(0); });\"\n```\n\n### Manually Processing Stuck Orders\n\nIf orders are stuck in a particular state:\n\n1. Identify stuck orders:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=orders-service -n orders -o jsonpath='{.items[0].metadata.name}') -n orders -- node -e \"const mongoose = require('mongoose'); const Order = require('./models/order'); mongoose.connect(process.env.MONGODB_URI).then(async () => { const stuckOrders = await Order.find({status: 'PROCESSING', updatedAt: {$lt: new Date(Date.now() - 30*60*1000)}}); console.log(JSON.stringify(stuckOrders, null, 2)); process.exit(0); });\"\n   ```\n\n2. Manually trigger processing for a specific order:\n   ```bash\n   curl -X POST https://api.internal.flowmart.com/orders/process -H \"Content-Type: application/json\" -d '{\"orderId\": \"ORDER_ID\", \"force\": true}'\n   ```\n\n## Recovery Procedures\n\n### Database Failure Recovery\n\nIf the MongoDB database becomes unavailable:\n\n1. Verify the status of the MongoDB cluster:\n   ```bash\n   kubectl get pods -l app=mongodb -n data\n   ```\n\n2. If the primary node is down, initiate a manual failover if necessary:\n   ```bash\n   kubectl exec -it mongodb-0 -n data -- mongo admin -u admin -p $MONGODB_PASSWORD --eval \"rs.stepDown()\"\n   ```\n\n3. If the entire cluster is unavailable, create an incident and notify the Database Team.\n\n4. Once database availability is restored, validate the OrdersService functionality:\n   ```bash\n   curl -X GET https://api.internal.flowmart.com/orders/health\n   ```\n\n### Event Bus Failure Recovery\n\nIf the Event Bus is unavailable:\n\n1. The OrdersService implements the Circuit Breaker pattern and will queue messages locally.\n\n2. When the Event Bus is restored, check the backlog of events:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=orders-service -n orders -o jsonpath='{.items[0].metadata.name}') -n orders -- curl localhost:9090/metrics | grep event_queue\n   ```\n\n3. Manually trigger event processing if necessary:\n   ```bash\n   curl -X POST https://api.internal.flowmart.com/orders/admin/process-event-queue -H \"Authorization: Bearer $ADMIN_TOKEN\"\n   ```\n\n## Disaster Recovery\n\n### Complete Service Failure\n\nIn case of a complete service failure:\n\n1. Initiate incident response by notifying the on-call team through PagerDuty.\n\n2. Check for region-wide AWS issues on the AWS Status page.\n\n3. If necessary, trigger the DR plan to fail over to the secondary region:\n   ```bash\n   ./scripts/dr-failover.sh orders-service\n   ```\n\n4. Update Route53 DNS to point to the secondary region if global failover is needed:\n   ```bash\n   aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://dr-dns-change.json\n   ```\n\n## Maintenance Tasks\n\n### Deploying New Versions\n\n```bash\nkubectl set image deployment/orders-service -n orders orders-service=ecr.aws/flowmart/orders-service:$VERSION\n```\n\n### Database Maintenance\n\nScheduled database maintenance should be performed during off-peak hours:\n\n1. Notify stakeholders through the #maintenance Slack channel.\n\n2. Set OrdersService to maintenance mode:\n   ```bash\n   curl -X POST https://api.internal.flowmart.com/orders/admin/maintenance -H \"Authorization: Bearer $ADMIN_TOKEN\" -H \"Content-Type: application/json\" -d '{\"maintenanceMode\": true, \"message\": \"Scheduled maintenance\"}'\n   ```\n\n3. Perform database maintenance operations.\n\n4. Turn off maintenance mode:\n   ```bash\n   curl -X POST https://api.internal.flowmart.com/orders/admin/maintenance -H \"Authorization: Bearer $ADMIN_TOKEN\" -H \"Content-Type: application/json\" -d '{\"maintenanceMode\": false}'\n   ```\n\n## Contact Information\n\n**Primary On-Call:** Orders Team (rotating schedule)  \n**Secondary On-Call:** Platform Team  \n**Escalation Path:** Orders Team Lead > Engineering Manager > CTO\n\n**Slack Channels:**\n- #orders-support (primary support channel)\n- #orders-alerts (automated alerts)\n- #incident-response (for major incidents)\n\n## Reference Information\n\n- [OrdersService API Documentation](https://docs.internal.flowmart.com/orders/api)\n- [Architecture Diagram](https://docs.internal.flowmart.com/architecture/orders)\n- [Service Level Objectives (SLOs)](https://docs.internal.flowmart.com/slo/orders)",
  "../examples/default/docs/operations-and-support/runbooks/orders-service-runbook.mdx",
  "c73c9760a675e516",
  "docs/operations-and-support/runbooks/payment-service-runbook",
  {
    "id": 598,
    "data": 600,
    "body": 605,
    "filePath": 606,
    "digest": 607,
    "deferredRender": 20
  },
  { "title": 601, "summary": 602, "sidebar": 603 },
  "PaymentService Runbook",
  "Operational runbook for troubleshooting and maintaining the PaymentService",
  { "label": 604, "order": 515 },
  "PaymentService",
  "This runbook provides operational procedures for the PaymentService, which is responsible for processing payments, refunds, and managing financial transactions in the FlowMart e-commerce platform.\n\n## Architecture\n\nThe PaymentService is responsible for:\n- Processing customer payments\n- Managing refunds and chargebacks\n- Integrating with external payment gateways\n- Storing payment transactions\n- Handling subscription billing\n\n### Service Dependencies\n\n```mermaid\nflowchart TD\n    PaymentService --> PostgresDB[(PostgreSQL Database)]\n    PaymentService --> EventBus[Event Bus]\n    PaymentService --> StripeGateway[Stripe Payment Gateway]\n    PaymentService --> PayPalGateway[PayPal Gateway]\n    PaymentService --> VaultService[Vault - Secret Management]\n```\n\n## Monitoring and Alerting\n\n### Key Metrics\n\n| Metric | Description | Warning Threshold | Critical Threshold |\n|--------|-------------|-------------------|-------------------|\n| `payment_processing_rate` | Payments processed per minute | \u003C 5 | \u003C 1 |\n| `payment_success_rate` | Percentage of successful payments | \u003C 95% | \u003C 90% |\n| `payment_processing_latency` | Time to process a payment | > 3s | > 8s |\n| `refund_processing_latency` | Time to process a refund | > 5s | > 15s |\n| `gateway_error_rate` | Payment gateway errors | > 2% | > 5% |\n| `fraud_detection_latency` | Time for fraud checks | > 1s | > 3s |\n\n### Dashboards\n\n- [PaymentService Overview](https://grafana.flowmart.com/d/payment-overview)\n- [Payment Gateway Status](https://grafana.flowmart.com/d/payment-gateways)\n- [Transaction Success Rates](https://grafana.flowmart.com/d/payment-success-rates)\n\n### Common Alerts\n\n| Alert | Description | Troubleshooting Steps |\n|-------|-------------|----------------------|\n| `PaymentServiceHighErrorRate` | Payment failure rate above threshold | See [High Error Rate](#high-error-rate) |\n| `PaymentServiceGatewayFailure` | Payment gateway connection issues | See [Gateway Issues](#payment-gateway-issues) |\n| `PaymentServiceHighLatency` | Payment processing latency issues | See [High Latency](#high-latency) |\n| `PaymentServiceDatabaseIssues` | Database connection issues | See [Database Issues](#database-issues) |\n\n## Troubleshooting Guides\n\n### High Error Rate\n\nIf the service is experiencing a high payment error rate:\n\n1. **Check application logs** for error patterns:\n   ```bash\n   kubectl logs -l app=payment-service -n payment --tail=100\n   ```\n\n2. **Check payment gateway status** on their status pages:\n   - [Stripe Status](https://status.stripe.com/)\n   - [PayPal Status](https://status.paypal.com/)\n\n3. **Check for patterns in failed transactions**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/query-failed-transactions.js --last-hour\n   ```\n\n4. **Check for recent deployments** that might have introduced issues:\n   ```bash\n   kubectl rollout history deployment/payment-service -n payment\n   ```\n\n5. **Verify if the issue is specific to a payment method** (credit card, PayPal, etc.):\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/payment-method-success-rates.js\n   ```\n\n### Payment Gateway Issues\n\nIf there are issues with payment gateways:\n\n1. **Check gateway connectivity**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -o /dev/null -s -w \"%{http_code}\\n\" https://api.stripe.com/v1/charges -H \"Authorization: Bearer $STRIPE_TEST_KEY\"\n   ```\n\n2. **Check payment gateway API keys** rotation status:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/check-api-key-rotation.js\n   ```\n\n3. **Check gateway timeouts** in application logs:\n   ```bash\n   kubectl logs -l app=payment-service -n payment | grep \"gateway timeout\"\n   ```\n\n4. **Verify if the issue is isolated to a specific gateway**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/gateway-health-check.js\n   ```\n\n5. **Switch to backup payment gateway** if primary is down:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -X POST localhost:3000/internal/api/payment/switch-gateway -H \"Content-Type: application/json\" -d '{\"primaryGateway\": \"paypal\", \"reason\": \"Stripe outage\"}'\n   ```\n\n### High Latency\n\nIf the service is experiencing high latency:\n\n1. **Check system metrics**:\n   ```bash\n   kubectl top pods -n payment\n   ```\n\n2. **Check database connection pool**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/db-pool-stats.js\n   ```\n\n3. **Check slow queries** in the payment database:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -d payments -c \"SELECT query, calls, mean_exec_time, max_exec_time FROM pg_stat_statements WHERE mean_exec_time > 100 ORDER BY mean_exec_time DESC LIMIT 10;\"\n   ```\n\n4. **Check payment gateway response times**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/gateway-latency-check.js\n   ```\n\n5. **Scale the service** if needed:\n   ```bash\n   kubectl scale deployment payment-service -n payment --replicas=5\n   ```\n\n### Database Issues\n\nIf there are database issues:\n\n1. **Check PostgreSQL status**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- pg_isready -U postgres -d payments\n   ```\n\n2. **Check for long-running transactions**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -d payments -c \"SELECT pid, now() - xact_start AS duration, state, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC LIMIT 10;\"\n   ```\n\n3. **Check for database locks**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- psql -U postgres -d payments -c \"SELECT relation::regclass, mode, pid, granted FROM pg_locks l JOIN pg_stat_activity a ON l.pid = a.pid WHERE relation = 'payments.transactions'::regclass;\"\n   ```\n\n4. **Restart database connections** if needed:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -X POST localhost:3000/internal/api/system/refresh-db-connections\n   ```\n\n## Common Operational Tasks\n\n### Managing API Keys\n\n#### Rotating Payment Gateway API Keys\n\n1. **Generate new API keys** in the payment gateway admin portal.\n\n2. **Store the new keys** in AWS Secrets Manager:\n   ```bash\n   aws secretsmanager update-secret --secret-id flowmart/payment/stripe-api-key --secret-string '{\"api_key\": \"sk_live_NEW_KEY\", \"webhook_secret\": \"whsec_NEW_SECRET\"}'\n   ```\n\n3. **Trigger key rotation** in the service:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -X POST localhost:3000/internal/api/system/reload-api-keys\n   ```\n\n4. **Verify the new keys are active**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/verify-api-keys.js\n   ```\n\n### Managing Refunds\n\n#### Processing Manual Refunds\n\nFor special cases requiring manual intervention:\n\n```bash\ncurl -X POST https://api.internal.flowmart.com/payment/transactions/{transactionId}/refund \\\n  -H \"Authorization: Bearer $ADMIN_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"amount\": 1999, \"reason\": \"Customer service request\", \"refundToOriginalMethod\": true}'\n```\n\n#### Finding Failed Refunds\n\nTo identify and retry failed refunds:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/list-failed-refunds.js --last-24h\n```\n\n### Handling Chargebacks\n\nTo record and process a new chargeback:\n\n```bash\ncurl -X POST https://api.internal.flowmart.com/payment/transactions/{transactionId}/chargeback \\\n  -H \"Authorization: Bearer $ADMIN_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"chargebackReference\": \"CB12345\", \"amount\": 1999, \"reason\": \"Unauthorized transaction\"}'\n```\n\n### Payment Reconciliation\n\nTo trigger payment reconciliation with payment gateway:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/reconcile-payments.js --gateway=stripe --date=2023-05-15\n```\n\n## Recovery Procedures\n\n### Failed Transactions Recovery\n\nIf transactions are stuck or failed:\n\n1. **Identify stuck transactions**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/find-stuck-transactions.js\n   ```\n\n2. **Check transaction status** with the payment gateway:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/check-gateway-transaction.js --transaction-id=TXN123456\n   ```\n\n3. **Resolve transactions** that completed at gateway but failed in our system:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/resolve-stuck-transaction.js --transaction-id=TXN123456 --status=completed\n   ```\n\n### Payment Gateway Failure Recovery\n\nIf a payment gateway is unavailable:\n\n1. **Enable fallback gateway** mode:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -X POST localhost:3000/internal/api/system/enable-fallback-gateway\n   ```\n\n2. **Monitor gateway status** for recovery:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/monitor-gateway-health.js --gateway=stripe\n   ```\n\n3. **Disable fallback mode** once the primary gateway is restored:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -X POST localhost:3000/internal/api/system/disable-fallback-gateway\n   ```\n\n### Database Failure Recovery\n\nIf the PostgreSQL database becomes unavailable:\n\n1. Verify the status of the PostgreSQL cluster:\n   ```bash\n   kubectl get pods -l app=postgresql -n data\n   ```\n\n2. Check if automatic failover has occurred:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql-patroni -n data -o jsonpath='{.items[0].metadata.name}') -n data -- patronictl list\n   ```\n\n3. Once database availability is restored, validate the PaymentService functionality:\n   ```bash\n   curl -X GET https://api.internal.flowmart.com/payment/health\n   ```\n\n## Disaster Recovery\n\n### Complete Service Failure\n\nIn case of a complete service failure:\n\n1. Initiate incident response by notifying the on-call team through PagerDuty.\n\n2. If necessary, deploy to the disaster recovery environment:\n   ```bash\n   ./scripts/dr-failover.sh payment-service\n   ```\n\n3. Update DNS records to point to the DR environment:\n   ```bash\n   aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://dr-dns-change.json\n   ```\n\n4. Enable simplified payment flow (if necessary):\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- curl -X POST localhost:3000/internal/api/system/enable-simplified-flow\n   ```\n\n5. Regularly check primary environment recovery status.\n\n## Maintenance Tasks\n\n### Deploying New Versions\n\n```bash\nkubectl set image deployment/payment-service -n payment payment-service=ecr.aws/flowmart/payment-service:$VERSION\n```\n\n### Database Migrations\n\nFor database schema updates:\n\n1. Notify stakeholders through the #maintenance Slack channel.\n\n2. Create a migration plan and backup the database:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=postgresql -n data -o jsonpath='{.items[0].metadata.name}') -n data -- pg_dump -U postgres -d payments > payments_backup_$(date +%Y%m%d).sql\n   ```\n\n3. Apply database migrations:\n   ```bash\n   kubectl apply -f payment-migration-job.yaml\n   ```\n\n4. Verify migration completion:\n   ```bash\n   kubectl logs -l job-name=payment-db-migration -n payment\n   ```\n\n### Compliance and Auditing\n\nTo generate PCI compliance reports:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=payment-service -n payment -o jsonpath='{.items[0].metadata.name}') -n payment -- node scripts/generate-pci-audit-report.js --month=2023-05\n```\n\n## Contact Information\n\n**Primary On-Call:** Payments Team (rotating schedule)  \n**Secondary On-Call:** Platform Team  \n**Escalation Path:** Payments Team Lead > Engineering Manager > CTO\n\n**Slack Channels:**\n- #payments-support (primary support channel)\n- #payments-alerts (automated alerts)\n- #incident-response (for major incidents)\n\n**External Contacts:**\n- Stripe Support: support@stripe.com, 1-888-555-1234\n- PayPal Support: merchant-support@paypal.com, 1-888-555-5678\n\n## Reference Information\n\n- [PaymentService API Documentation](https://docs.internal.flowmart.com/payment/api)\n- [Architecture Diagram](https://docs.internal.flowmart.com/architecture/payment)\n- [Service Level Objectives (SLOs)](https://docs.internal.flowmart.com/slo/payment)\n- [PCI Compliance Documentation](https://docs.internal.flowmart.com/security/payment-pci)\n- [Payment Gateway Integration Guides](https://docs.internal.flowmart.com/payment/gateway-integration)",
  "../examples/default/docs/operations-and-support/runbooks/payment-service-runbook.mdx",
  "f168443fe095b088",
  "docs/technical-architecture-design/infrastructure-as-code/02-terraform-implementation",
  {
    "id": 608,
    "data": 610,
    "body": 615,
    "filePath": 616,
    "digest": 617,
    "deferredRender": 20
  },
  { "title": 611, "summary": 612, "sidebar": 613 },
  "Terraform Implementation",
  "Details of how Terraform is used to provision and manage the FlowMart infrastructure",
  { "label": 614, "order": 487 },
  "02 - Terraform Implementation",
  "# Terraform Implementation\n\nThis document describes how we use Terraform to manage and provision the infrastructure for the FlowMart e-commerce platform.\n\n## Why Terraform?\n\nWe chose Terraform as our primary IaC tool for the following reasons:\n\n- **Cloud-agnostic**: While we primarily use AWS, Terraform gives us the flexibility to work with multiple cloud providers if needed\n- **Declarative syntax**: Define what you want, not how to get there\n- **State management**: Tracks the current state of infrastructure to plan changes\n- **Modular approach**: Supports reusable components through modules\n- **Strong community support**: Wide adoption means better documentation and resources\n- **Extensible**: Can be extended through providers and modules\n\n## Terraform Structure\n\nOur Terraform code follows a structured approach:\n\n### Directory Structure\n\n```\nterraform/\n│\n├── environments/                 # Environment-specific configurations\n│   ├── dev/\n│   │   ├── main.tf              # Main configuration file\n│   │   ├── variables.tf         # Input variables\n│   │   ├── outputs.tf           # Output variables\n│   │   └── terraform.tfvars     # Variable values\n│   ├── staging/\n│   │   └── ...\n│   └── production/\n│       └── ...\n│\n├── modules/                      # Reusable Terraform modules\n│   ├── networking/               # VPC, subnets, routing\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   ├── eks/                      # EKS cluster configuration\n│   │   └── ...\n│   ├── rds/                      # Database configurations\n│   │   └── ...\n│   ├── lambda/                   # Serverless functions\n│   │   └── ...\n│   └── monitoring/               # Monitoring resources\n│       └── ...\n│\n└── global/                       # Global resources\n    ├── iam/                      # IAM roles and policies\n    │   └── ...\n    └── route53/                  # DNS configurations\n        └── ...\n```\n\n### Module Design\n\nEach module follows a consistent pattern:\n\n- **Inputs**: Defined in `variables.tf`\n- **Resources**: Defined in `main.tf`\n- **Outputs**: Defined in `outputs.tf`\n\n## Core Infrastructure Components\n\nHere's an overview of our main Terraform modules and what they provision:\n\n### Networking Module\n\nThe networking module provisions our VPC infrastructure:\n\n```hcl\nmodule \"vpc\" {\n  source = \"../../modules/networking\"\n\n  name                 = \"flowmart-${var.environment}\"\n  cidr                 = \"10.0.0.0/16\"\n  azs                  = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n  private_subnets      = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets       = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n  database_subnets     = [\"10.0.201.0/24\", \"10.0.202.0/24\", \"10.0.203.0/24\"]\n  \n  enable_nat_gateway   = true\n  single_nat_gateway   = var.environment != \"production\"\n  \n  tags = {\n    Environment = var.environment\n    Project     = \"FlowMart\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n```\n\n### EKS Module\n\nThe EKS module provisions our Kubernetes cluster:\n\n```hcl\nmodule \"eks\" {\n  source = \"../../modules/eks\"\n\n  cluster_name    = \"flowmart-${var.environment}\"\n  cluster_version = \"1.23\"\n  \n  vpc_id          = module.vpc.vpc_id\n  subnet_ids      = module.vpc.private_subnets\n  \n  node_groups = {\n    application = {\n      desired_capacity = 3\n      max_capacity     = 10\n      min_capacity     = 2\n      instance_types   = [\"t3.large\"]\n      disk_size        = 50\n    }\n    \n    system = {\n      desired_capacity = 2\n      max_capacity     = 4\n      min_capacity     = 2\n      instance_types   = [\"t3.medium\"]\n      disk_size        = 20\n    }\n  }\n  \n  tags = {\n    Environment = var.environment\n    Project     = \"FlowMart\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n```\n\n### Database Module\n\nThe database module provisions our RDS instances:\n\n```hcl\nmodule \"database\" {\n  source = \"../../modules/rds\"\n\n  identifier           = \"flowmart-${var.environment}\"\n  engine               = \"postgres\"\n  engine_version       = \"13.4\"\n  instance_class       = var.environment == \"production\" ? \"db.r5.large\" : \"db.t3.medium\"\n  allocated_storage    = 100\n  \n  name                 = \"flowmart\"\n  username             = \"flowmart_admin\"\n  password             = var.db_password\n  \n  vpc_security_group_ids = [module.security_groups.database_sg_id]\n  subnet_ids             = module.vpc.database_subnets\n  \n  backup_retention_period = var.environment == \"production\" ? 30 : 7\n  \n  tags = {\n    Environment = var.environment\n    Project     = \"FlowMart\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n```\n\n### DynamoDB Module\n\nFor NoSQL database needs, we use DynamoDB:\n\n```hcl\nmodule \"dynamodb\" {\n  source = \"../../modules/dynamodb\"\n\n  tables = {\n    inventory = {\n      name           = \"inventory-${var.environment}\"\n      billing_mode   = \"PROVISIONED\"\n      read_capacity  = var.environment == \"production\" ? 20 : 5\n      write_capacity = var.environment == \"production\" ? 20 : 5\n      hash_key       = \"productId\"\n      attributes = [\n        {\n          name = \"productId\"\n          type = \"S\"\n        }\n      ]\n    },\n    \n    shopping_cart = {\n      name           = \"shopping-cart-${var.environment}\"\n      billing_mode   = \"PROVISIONED\"\n      read_capacity  = var.environment == \"production\" ? 20 : 5\n      write_capacity = var.environment == \"production\" ? 20 : 5\n      hash_key       = \"sessionId\"\n      attributes = [\n        {\n          name = \"sessionId\"\n          type = \"S\"\n        }\n      ]\n    }\n  }\n  \n  tags = {\n    Environment = var.environment\n    Project     = \"FlowMart\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n```\n\n## Terraform Workflows\n\nWe follow these workflows when making infrastructure changes:\n\n### Development Workflow\n\n```mermaid\nflowchart LR\n    classDef highlightStep fill:#d5e8d4,stroke:#82b366,stroke-width:2px\n\n    Init[terraform init] --> Plan[terraform plan]\n    Plan --> Review[Review plan]\n    Review --> Apply[terraform apply]\n    Apply --> Validate[Validate changes]\n    \n    class Plan,Review highlightStep\n```\n\n### Terraform in CI/CD Pipeline\n\n```mermaid\nflowchart TD\n    Code[Code changes pushed] --> PR[Pull request created]\n    PR --> Init[terraform init]\n    Init --> Plan[terraform plan]\n    Plan --> AutoReview[Automated plan review]\n    AutoReview --> HumanReview[Human review]\n    HumanReview --> Approved{Approved?}\n    Approved -->|Yes| Merge[Merge PR]\n    Approved -->|No| Revise[Revise changes]\n    Revise --> PR\n    Merge --> CI[CI/CD pipeline]\n    CI --> Apply[terraform apply]\n    Apply --> PostCheck[Post-apply validation]\n```\n\n## Terraform State Management\n\nWe manage Terraform state using a remote backend with the following characteristics:\n\n- **S3 Bucket**: For state storage\n- **DynamoDB Table**: For state locking\n- **IAM Roles**: For secure access to state files\n- **State Encryption**: For security of sensitive data\n\nExample remote backend configuration:\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket         = \"flowmart-terraform-state\"\n    key            = \"environments/${var.environment}/terraform.tfstate\"\n    region         = \"us-west-2\"\n    dynamodb_table = \"terraform-lock\"\n    encrypt        = true\n    role_arn       = \"arn:aws:iam::ACCOUNT_ID:role/TerraformStateManager\"\n  }\n}\n```\n\n## Terraform Best Practices\n\nWe follow these best practices for our Terraform codebase:\n\n1. **Version Pinning**: Lock provider and module versions to ensure reproducibility\n\n```hcl\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.16.0\"\n    }\n  }\n  required_version = \">= 1.2.0\"\n}\n```\n\n2. **Resource Tagging**: All resources are tagged for billing and management purposes\n\n```hcl\ntags = {\n  Environment = var.environment\n  Project     = \"FlowMart\"\n  ManagedBy   = \"Terraform\"\n  Service     = \"Orders\"\n}\n```\n\n3. **Variable Validation**: Validate inputs to prevent errors\n\n```hcl\nvariable \"environment\" {\n  description = \"The deployment environment (e.g., dev, staging, production)\"\n  type        = string\n  \n  validation {\n    condition     = contains([\"dev\", \"staging\", \"production\"], var.environment)\n    error_message = \"Environment must be one of: dev, staging, production.\"\n  }\n}\n```\n\n4. **Modular Design**: Use modules for reusable components\n\n5. **Minimal Permissions**: Follow the principle of least privilege for IAM roles\n\n## Next Steps\n\nFor more information about our IaC implementation, please refer to:\n\n- [Environment Setups](./03-environment-setups.mdx)\n- [CI/CD Pipelines](./04-cicd-pipelines.mdx)",
  "../examples/default/docs/technical-architecture-design/infrastructure-as-code/02-terraform-implementation.mdx",
  "eab1c91b6b8a70a9",
  "docs/operations-and-support/runbooks/shipping-service-runbook",
  {
    "id": 618,
    "data": 620,
    "body": 625,
    "filePath": 626,
    "digest": 627,
    "deferredRender": 20
  },
  { "title": 621, "summary": 622, "sidebar": 623 },
  "ShippingService Runbook",
  "Operational runbook for troubleshooting and maintaining the ShippingService",
  { "label": 624, "order": 584 },
  "ShippingService",
  "This runbook provides operational procedures for the ShippingService, which is responsible for managing shipping options, carrier integration, and delivery tracking in the FlowMart e-commerce platform.\n\n\n## Architecture\n\nThe ShippingService is responsible for:\n- Calculating shipping costs and delivery estimates\n- Managing shipping carriers and integration\n- Generating shipping labels\n- Tracking shipments\n- Handling delivery exceptions and returns\n\n### Service Dependencies\n\n```mermaid\nflowchart TD\n    ShippingService --> MongoDB[(MongoDB Database)]\n    ShippingService --> EventBus[Event Bus]\n    ShippingService --> Redis[(Redis Cache)]\n    ShippingService --> FedExAPI[FedEx API]\n    ShippingService --> UPSApi[UPS API]\n    ShippingService --> USPSApi[USPS API]\n    ShippingService --> DHLApi[DHL API]\n    ShippingService --> VaultService[Vault - Secret Management]\n```\n\n## Monitoring and Alerting\n\n### Key Metrics\n\n| Metric | Description | Warning Threshold | Critical Threshold |\n|--------|-------------|-------------------|-------------------|\n| `shipping_rate_calculation_rate` | Rate calculations per minute | \u003C 10 | \u003C 2 |\n| `shipping_label_generation_success` | Label generation success % | \u003C 98% | \u003C 95% |\n| `carrier_api_response_time` | Carrier API response time | > 2s | > 5s |\n| `carrier_api_error_rate` | Carrier API errors % | > 2% | > 5% |\n| `tracking_update_processing_rate` | Tracking updates processed per minute | \u003C 50 | \u003C 10 |\n| `shipment_tracking_lag` | Delay in tracking information | > 15m | > 1h |\n\n### Dashboards\n\n- [Shipping Service Overview](https://grafana.flowmart.com/d/shipping-overview)\n- [Carrier API Status](https://grafana.flowmart.com/d/shipping-carriers)\n- [Delivery Performance](https://grafana.flowmart.com/d/shipping-delivery-performance)\n\n### Common Alerts\n\n| Alert | Description | Troubleshooting Steps |\n|-------|-------------|----------------------|\n| `ShippingServiceHighErrorRate` | Shipping API error rate above threshold | See [High Error Rate](#high-error-rate) |\n| `ShippingCarrierAPIDown` | Carrier API connection issues | See [Carrier API Issues](#carrier-api-issues) |\n| `ShippingServiceHighLatency` | Shipping service latency issues | See [High Latency](#high-latency) |\n| `ShippingServiceDatabaseIssues` | Database connection issues | See [Database Issues](#database-issues) |\n\n## Troubleshooting Guides\n\n### High Error Rate\n\nIf the service is experiencing a high error rate:\n\n1. **Check application logs** for error patterns:\n   ```bash\n   kubectl logs -l app=shipping-service -n shipping --tail=100\n   ```\n\n2. **Check specific error types**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/error-analyzer.jar --last-hour\n   ```\n\n3. **Check for patterns in failed shipments**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/failed-shipments-analyzer.jar\n   ```\n\n4. **Check for recent deployments** that might have introduced issues:\n   ```bash\n   kubectl rollout history deployment/shipping-service -n shipping\n   ```\n\n5. **Verify if the issue is specific to a carrier** (FedEx, UPS, etc.):\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/carrier-success-rates.jar\n   ```\n\n### Carrier API Issues\n\nIf there are issues with carrier APIs:\n\n1. **Check carrier API connectivity**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/carrier-health-check.jar\n   ```\n\n2. **Check carrier API credentials** and rotation status:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/check-carrier-credentials.jar\n   ```\n\n3. **Check carrier status pages** for announced outages:\n   - [FedEx Status](https://www.fedex.com/en-us/service-alerts.html)\n   - [UPS Status](https://www.ups.com/service-alerts)\n   - [USPS Status](https://about.usps.com/newsroom/service-alerts/)\n\n4. **Check carrier timeouts** in application logs:\n   ```bash\n   kubectl logs -l app=shipping-service -n shipping | grep \"carrier timeout\"\n   ```\n\n5. **Enable fallback shipping carrier**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/shipping/enable-fallback-carrier -H \"Content-Type: application/json\" -d '{\"primaryCarrier\": \"fedex\", \"fallbackCarrier\": \"ups\", \"reason\": \"FedEx API outage\"}'\n   ```\n\n### High Latency\n\nIf the service is experiencing high latency:\n\n1. **Check system metrics**:\n   ```bash\n   kubectl top pods -n shipping\n   ```\n\n2. **Check JVM memory and GC metrics**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/jvm-metrics.jar\n   ```\n\n3. **Check MongoDB performance**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo --eval \"db.currentOp()\"\n   ```\n\n4. **Check carrier API response times**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/carrier-response-times.jar\n   ```\n\n5. **Scale the service** if needed:\n   ```bash\n   kubectl scale deployment shipping-service -n shipping --replicas=5\n   ```\n\n### Database Issues\n\nIf there are MongoDB issues:\n\n1. **Check MongoDB status**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo --eval \"rs.status()\"\n   ```\n\n2. **Check for slow queries**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo --eval \"db.currentOp({ 'active': true, 'secs_running': { '$gt': 5 } })\"\n   ```\n\n3. **Check database connection pool**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/db-pool-stats.jar\n   ```\n\n4. **Restart database connections** if needed:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/system/refresh-db-connections\n   ```\n\n## Common Operational Tasks\n\n### Managing Carrier API Credentials\n\n#### Rotating Carrier API Keys\n\n1. **Generate new API keys** in the carrier portal:\n   - FedEx Developer Portal: [https://developer.fedex.com](https://developer.fedex.com)\n   - UPS Developer Portal: [https://developer.ups.com](https://developer.ups.com)\n   - USPS Web Tools: [https://www.usps.com/business/web-tools-apis](https://www.usps.com/business/web-tools-apis)\n\n2. **Store the new keys** in AWS Secrets Manager:\n   ```bash\n   aws secretsmanager update-secret --secret-id flowmart/shipping/fedex-api-key --secret-string '{\"api_key\": \"NEW_KEY\", \"password\": \"NEW_PASSWORD\", \"account_number\": \"ACCOUNT_NUMBER\"}'\n   ```\n\n3. **Trigger key rotation** in the service:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/system/reload-carrier-credentials\n   ```\n\n4. **Verify the new keys are working** by testing label generation:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/test-label-generation.jar --carrier=fedex\n   ```\n\n### Managing Shipping Rates\n\n#### Updating Shipping Rate Tables\n\nWhen carrier rates change:\n\n1. **Prepare the new rate table** in the required JSON format.\n\n2. **Upload the rate table** to S3:\n   ```bash\n   aws s3 cp new-fedex-rates.json s3://flowmart-configs/shipping/rates/\n   ```\n\n3. **Trigger rate table reload**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/shipping/reload-rate-tables -H \"Content-Type: application/json\" -d '{\"carrier\": \"fedex\"}'\n   ```\n\n4. **Verify rate calculations** with test scenarios:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/test-rate-calculation.jar\n   ```\n\n### Shipping Label Generation Troubleshooting\n\n#### Debugging Failed Label Generation\n\nIf labels are failing to generate:\n\n```bash\n# Find recent failed label generation attempts\nkubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/find-failed-labels.jar --hours=2\n\n# Get detailed error for a specific shipment\nkubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/label-error-details.jar --shipment-id=SHIP123456\n```\n\n#### Manual Label Generation\n\nFor special cases requiring manual intervention:\n\n```bash\ncurl -X POST https://api.internal.flowmart.com/shipping/shipments/{shipmentId}/generate-label \\\n  -H \"Authorization: Bearer $ADMIN_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"carrier\": \"fedex\", \"service\": \"PRIORITY_OVERNIGHT\", \"forceGeneration\": true}'\n```\n\n### Tracking Updates\n\n#### Triggering Manual Tracking Updates\n\nTo manually trigger tracking updates:\n\n```bash\nkubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/sync-tracking.jar --shipment-id=SHIP123456\n\n# For bulk tracking updates\nkubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/sync-tracking.jar --status=in_transit --hours=24\n```\n\n#### Tracking Webhook Troubleshooting\n\nIf tracking webhooks from carriers are failing:\n\n```bash\n# Check recent webhook failures\nkubectl logs -l app=shipping-webhook-service -n shipping | grep \"Webhook failure\"\n\n# Replay failed webhooks\nkubectl exec -it $(kubectl get pods -l app=shipping-webhook-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/replay-webhooks.jar --hours=2\n```\n\n## Recovery Procedures\n\n### Failed Shipment Recovery\n\nIf shipments are stuck or failed:\n\n1. **Identify stuck shipments**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/find-stuck-shipments.jar\n   ```\n\n2. **Check shipment status** with the carrier:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/check-carrier-shipment.jar --shipment-id=SHIP123456\n   ```\n\n3. **Resolve shipments** that completed at carrier but failed in our system:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/resolve-shipment.jar --shipment-id=SHIP123456 --tracking-number=1Z999AA10123456784 --status=label_created\n   ```\n\n### Carrier API Failure Recovery\n\nIf a carrier API is unavailable:\n\n1. **Enable automatic carrier fallback**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/system/enable-carrier-fallback\n   ```\n\n2. **Monitor carrier API status** for recovery:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/monitor-carrier-health.jar --carrier=fedex\n   ```\n\n3. **Switch back to primary carrier** once it's restored:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/system/disable-carrier-fallback\n   ```\n\n### Database Failure Recovery\n\nIf the MongoDB database becomes unavailable:\n\n1. **Verify the status of the MongoDB cluster**:\n   ```bash\n   kubectl get pods -l app=mongodb -n data\n   ```\n\n2. **Check if automatic failover has occurred**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo --eval \"rs.status()\"\n   ```\n\n3. **Once database availability is restored, validate ShippingService functionality**:\n   ```bash\n   curl -X GET https://api.internal.flowmart.com/shipping/health\n   ```\n\n## Disaster Recovery\n\n### Complete Service Failure\n\nIn case of a complete service failure:\n\n1. **Initiate incident response** by notifying the on-call team through PagerDuty.\n\n2. **Deploy to the disaster recovery environment** if necessary:\n   ```bash\n   ./scripts/dr-failover.sh shipping-service\n   ```\n\n3. **Update DNS records** to point to the DR environment:\n   ```bash\n   aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://dr-dns-change.json\n   ```\n\n4. **Enable simplified shipping flow** (if necessary):\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- curl -X POST localhost:8080/internal/api/system/enable-simplified-flow\n   ```\n\n5. **Regularly check primary environment recovery status**.\n\n## Maintenance Tasks\n\n### Deploying New Versions\n\n```bash\nkubectl set image deployment/shipping-service -n shipping shipping-service=ecr.aws/flowmart/shipping-service:$VERSION\n```\n\n### Database Maintenance\n\n#### MongoDB Index Maintenance\n\nPeriodically verify and optimize MongoDB indexes:\n\n```bash\n# Check current indexes\nkubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo --eval \"db.shipments.getIndexes()\"\n\n# Add new index (example)\nkubectl exec -it $(kubectl get pods -l app=mongodb -n data -o jsonpath='{.items[0].metadata.name}') -n data -- mongo --eval \"db.shipments.createIndex({carrier: 1, status: 1, createdAt: -1})\"\n```\n\n#### Database Backups\n\nVerify scheduled MongoDB backups:\n\n```bash\n# Check recent backups\naws s3 ls s3://flowmart-mongodb-backups/shipping/ --human-readable\n\n# Trigger manual backup if needed\nkubectl apply -f shipping-db-backup-job.yaml\n```\n\n### Carrier Integration Updates\n\nWhen a carrier updates their API:\n\n1. **Test the API changes** in the staging environment:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service-staging -n shipping-staging -o jsonpath='{.items[0].metadata.name}') -n shipping-staging -- java -jar /app/tools/test-carrier-integration.jar --carrier=fedex --mode=new\n   ```\n\n2. **Update integration configuration** if needed:\n   ```bash\n   kubectl apply -f updated-fedex-integration-config.yaml\n   ```\n\n3. **Validate the updated integration**:\n   ```bash\n   kubectl exec -it $(kubectl get pods -l app=shipping-service -n shipping -o jsonpath='{.items[0].metadata.name}') -n shipping -- java -jar /app/tools/validate-carrier-integration.jar --carrier=fedex\n   ```\n\n## Contact Information\n\n**Primary On-Call:** Logistics Team (rotating schedule)  \n**Secondary On-Call:** Platform Team  \n**Escalation Path:** Logistics Team Lead > Engineering Manager > CTO\n\n**Slack Channels:**\n- #shipping-support (primary support channel)\n- #shipping-alerts (automated alerts)\n- #incident-response (for major incidents)\n\n**External Contacts:**\n- FedEx API Support: apisupport@fedex.com, 1-800-555-1234\n- UPS Developer Support: developer@ups.com, 1-800-555-5678\n- USPS Web Tools Support: uspstechsupport@usps.gov, 1-800-555-9012\n\n## Reference Information\n\n- [ShippingService API Documentation](https://docs.internal.flowmart.com/shipping/api)\n- [Architecture Diagram](https://docs.internal.flowmart.com/architecture/shipping)\n- [Service Level Objectives (SLOs)](https://docs.internal.flowmart.com/slo/shipping)\n- [Carrier Integration Guides](https://docs.internal.flowmart.com/shipping/carrier-integration)\n- [Rate Calculation Documentation](https://docs.internal.flowmart.com/shipping/rate-calculation)",
  "../examples/default/docs/operations-and-support/runbooks/shipping-service-runbook.mdx",
  "23dc0e444db22d73",
  "docs/technical-architecture-design/architecture-decision-records/01-architecture-desicion-record",
  {
    "id": 628,
    "data": 630,
    "body": 635,
    "filePath": 636,
    "digest": 637,
    "deferredRender": 20
  },
  { "title": 631, "summary": 632, "sidebar": 633 },
  "Architecture Decision Records (ADR)",
  "A document that captures important architectural decisions and their context",
  { "label": 634, "order": 470 },
  "ADR Template",
  "## What is an Architecture Decision Record (ADR)?\n\n\u003CTiles columns={2}>\n    \u003CTile icon=\"CommandLineIcon\" iconColor=\"black\" href=\"/docs\" title=\"Explore examples on GitHub\" description=\"Delve into our documentation for more insights.\" />\n    \u003CTile icon=\"UsersIcon\" iconColor=\"black\" href=\"/docs\" title=\"Request a review\" description=\"Request a review from the team. Reviews for ADRs are from the design authority, and can take a week.\" />\n\u003C/Tiles>\n\nArchitecture Decision Records (ADRs) are documents that capture important architectural decisions made during the development of a software system. Each ADR describes a choice the team has made, the context in which it was made, and the consequences of that choice.\n\n## Why ADRs are Important\n\n:::tip\nUse this template to create your own ADRs. Once you read this page you can submit new ADRs to the [Architecture Decision Records](https://github.com/eventcatalog/eventcatalog/issues/new?assignees=&labels=architecture&template=architecture-decision-record.mdx&title=ADR%3A+) repository.\n:::\n\nADRs help teams:\n- Document decisions for future reference\n- Communicate architectural choices across the organization\n- Onboard new team members by providing insight into past decisions\n- Track the evolution of the system architecture over time\n- Establish a process for making and documenting significant technical decisions\n\n## ADR Format\n\nOur ADRs follow this structure:\n- **Title**: A descriptive name for the decision\n- **Status**: Current status (Proposed, Accepted, Superseded, etc.)\n- **Context**: The factors that influenced the decision\n- **Decision**: The choice that was made\n- **Consequences**: The resulting outcomes, both positive and negative\n- **Compliance Requirements**: Any regulatory or policy requirements that must be met\n- **Implementation Details**: How and when the decision will be implemented\n- **Alternatives**: Other options that were considered\n- **References**: Resources that support or provide more information\n- **Decision History**: The record of changes to the ADR\n\n```mermaid\nflowchart TD\n    A[Identify Architectural Decision Point] --> B[Discuss Options]\n    B --> C[Draft ADR]\n    C --> D[Review with Stakeholders]\n    D --> E{Approved?}\n    E -->|Yes| F[Update Status to Approved]\n    E -->|No| G[Revise ADR]\n    G --> D\n    F --> H[Implement Decision]\n    H --> I[Monitor & Evaluate]\n    I --> J{Changes Needed?}\n    J -->|Yes| K[Create New or Update ADR]\n    K --> B\n    J -->|No| L[Document Learnings]\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/01-architecture-desicion-record.mdx",
  "fa47cf139e1aa8b0",
  "docs/technical-architecture-design/infrastructure-as-code/03-environment-setups",
  {
    "id": 638,
    "data": 640,
    "body": 645,
    "filePath": 646,
    "digest": 647,
    "deferredRender": 20
  },
  { "title": 641, "summary": 642, "sidebar": 643 },
  "Environment Setups",
  "Detailed information about the different environments for the FlowMart e-commerce platform",
  { "label": 644, "order": 515 },
  "03 - Environment Setups",
  "# Environment Setups\n\nThis document describes the different environments used in the FlowMart e-commerce platform, their purposes, configurations, and the promotion process between them.\n\n## Environment Strategy\n\nWe follow a multi-environment strategy with clear separation and purposes:\n\n```mermaid\nflowchart TD\n    classDef devEnv fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px\n    classDef stagingEnv fill:#d5e8d4,stroke:#82b366,stroke-width:2px\n    classDef prodEnv fill:#f8cecc,stroke:#b85450,stroke-width:2px\n    classDef dataEnv fill:#fff2cc,stroke:#d6b656,stroke-width:2px\n\n    Dev[Development Environment] --> Staging[Staging Environment]\n    Staging --> Production[Production Environment]\n    \n    Dev --- Sandbox[Sandbox Environments]\n    Production --- DR[Disaster Recovery]\n    \n    class Dev,Sandbox devEnv\n    class Staging stagingEnv\n    class Production,DR prodEnv\n    class Data dataEnv\n```\n\n## Environment Descriptions\n\n### Development Environment\n\nThe development environment is used by developers to test changes and new features.\n\n- **Purpose**: Development, testing, and integration\n- **Access**: Development team\n- **Data**: Subset of anonymized production data or synthetic data\n- **Infrastructure Scale**: Minimal, cost-optimized\n- **Deployment Frequency**: Multiple times per day\n- **Automated Testing**: Unit tests, API tests\n\n**Key Characteristics**:\n- Shared development environment\n- Reduced redundancy (e.g., single NAT gateway, smaller instances)\n- Feature flags enabled for work-in-progress features\n- Debug and verbose logging enabled\n- Daily database refresh from anonymized production data\n\n### Staging Environment\n\nThe staging environment is a pre-production environment that closely mirrors production.\n\n- **Purpose**: System testing, performance testing, UAT\n- **Access**: Development team, QA, selected stakeholders\n- **Data**: Full anonymized copy of production data\n- **Infrastructure Scale**: Nearly identical to production, but smaller scale\n- **Deployment Frequency**: Once per release (multiple times per week)\n- **Automated Testing**: Integration tests, performance tests, security scans\n\n**Key Characteristics**:\n- Configuration as close to production as possible\n- Full feature set enabled\n- Production-level logging\n- Staged rollout of new features\n- Regular (weekly) database refresh from anonymized production data\n\n### Production Environment\n\nThe production environment serves real customers and processes real transactions.\n\n- **Purpose**: Serving end-users\n- **Access**: Limited access via break-glass procedures\n- **Data**: Real customer data\n- **Infrastructure Scale**: Full scale, highly available\n- **Deployment Frequency**: Multiple times per week, during designated windows\n- **Automated Testing**: Smoke tests, canary tests\n\n**Key Characteristics**:\n- High availability across multiple availability zones\n- Auto-scaling based on demand\n- Enhanced security controls\n- Full monitoring and alerting\n- Regular backups\n- Blue/green deployment strategy\n\n### Sandbox Environments\n\nEphemeral environments for developers to test specific features or changes.\n\n- **Purpose**: Feature development, experimentation\n- **Access**: Individual developers or teams\n- **Data**: Synthetic data\n- **Infrastructure Scale**: Minimal\n- **Deployment Frequency**: On-demand\n- **Lifetime**: Temporary (hours to days)\n\n### Disaster Recovery Environment\n\nA standby environment that can be activated in case of a major outage in the production environment.\n\n- **Purpose**: Business continuity\n- **State**: Warm standby\n- **Data**: Regular replication from production\n- **Region**: Different from primary production region\n- **Activation**: Automated with manual approval\n\n## Environment Configuration\n\nWe manage environment-specific configurations through:\n\n1. **Terraform Variables**: Each environment has its own `terraform.tfvars` file\n2. **Kubernetes ConfigMaps**: Environment-specific Kubernetes configurations\n3. **Environment Variables**: Set at the pod or container level\n4. **Feature Flags**: Application-level feature toggles\n\nExample Terraform Variable Differences:\n\n| Variable | Dev | Staging | Production |\n|----------|-----|---------|------------|\n| `vpc_cidr` | 10.0.0.0/16 | 10.1.0.0/16 | 10.2.0.0/16 |\n| `eks_node_count` | 2 | 3 | 5-10 (auto-scaling) |\n| `rds_instance_type` | db.t3.medium | db.r5.large | db.r5.2xlarge |\n| `rds_multi_az` | false | true | true |\n| `enable_waf` | false | true | true |\n\n## Environment Promotion Process\n\nWe follow a structured promotion process for changes moving through environments:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Development\n    participant Staging as Staging\n    participant Prod as Production\n    \n    Note over Dev: Feature development complete\n    Dev->>Staging: Promote code\n    Note over Staging: Run integration tests\n    Note over Staging: Performance testing\n    Note over Staging: Security scanning\n    Note over Staging: UAT approval\n    Staging->>Prod: Promote code\n    Note over Prod: Canary deployment\n    Note over Prod: Monitoring\n    Note over Prod: Full rollout\n```\n\n### Promotion Guidelines\n\n1. **Development to Staging**:\n   - All unit tests pass\n   - Code review completed\n   - Feature implementation verified in development\n   - Feature documentation completed\n\n2. **Staging to Production**:\n   - All integration tests pass\n   - Performance tests meet SLAs\n   - Security scans show no critical or high vulnerabilities\n   - UAT completed and signed off\n   - Release notes prepared\n\n## Environment Variables Management\n\nWe manage environment variables securely using:\n\n1. **AWS Parameter Store**: For non-secret configuration\n2. **AWS Secrets Manager**: For sensitive values\n3. **Kubernetes Secrets**: Mounted into containers at runtime\n\nExample parameter naming convention:\n```\n/flowmart/{environment}/{service}/{parameter-name}\n```\n\nExample secret access in application code:\n```javascript\nconst AWS = require('aws-sdk');\nconst ssm = new AWS.SSM();\n\nasync function getDatabaseConfig() {\n  const params = {\n    Name: `/flowmart/${process.env.ENVIRONMENT}/orders-service/db-connection-string`,\n    WithDecryption: true\n  };\n  \n  const result = await ssm.getParameter(params).promise();\n  return result.Parameter.Value;\n}\n```\n\n## Network Isolation\n\nOur environments are isolated from each other:\n\n```mermaid\nflowchart TB\n    subgraph \"AWS Account: Production\"\n        ProdVPC[Production VPC]\n    end\n    \n    subgraph \"AWS Account: Non-Production\"\n        StagingVPC[Staging VPC]\n        DevVPC[Development VPC]\n    end\n    \n    Internet[Internet] --> PVPN[Production VPN]\n    Internet --> NPVPN[Non-Production VPN]\n    \n    PVPN --> ProdVPC\n    NPVPN --> StagingVPC\n    NPVPN --> DevVPC\n```\n\nKey security controls:\n- Separate AWS accounts for production and non-production\n- VPC isolation for each environment\n- Separate VPN access for production and non-production\n- Restricted traffic between environments\n- Different IAM roles for each environment\n\n## Data Management Across Environments\n\nWe handle data carefully across environments:\n\n1. **Production**: Real customer data with full security controls\n2. **Staging**: Anonymized production data, refreshed weekly\n3. **Development**: Subset of anonymized data or synthetic data\n4. **Sandbox**: Synthetic data only\n\nData anonymization process:\n```mermaid\nflowchart LR\n    ProdDB[(Production Database)] --> Extract[Extract Data]\n    Extract --> Anonymize[Anonymize Sensitive Data]\n    Anonymize --> Load[Load to Non-Prod Environments]\n    Load --> StageDB[(Staging Database)]\n    Load --> DevDB[(Development Database)]\n```\n\n## Monitoring and Observability\n\nEach environment has appropriate monitoring:\n\n| Monitoring Aspect | Development | Staging | Production |\n|-------------------|-------------|---------|------------|\n| Metrics Collection | Basic | Full | Full |\n| Logs Retention | 7 days | 14 days | 90 days |\n| Alerting | Critical only | High and Critical | All severities |\n| Dashboards | Basic | Full | Full with extended |\n| Tracing | Sampled (50%) | Sampled (75%) | Sampled (10%) |\n\n## Next Steps\n\nFor more information about our environment management and deployment processes, please refer to:\n\n- [CI/CD Pipelines](./04-cicd-pipelines.mdx)",
  "../examples/default/docs/technical-architecture-design/infrastructure-as-code/03-environment-setups.mdx",
  "f3c167d2a0e14381",
  "docs/technical-architecture-design/infrastructure-as-code/01-iac-overview",
  {
    "id": 648,
    "data": 650,
    "body": 655,
    "filePath": 656,
    "digest": 657,
    "deferredRender": 20
  },
  { "title": 651, "summary": 652, "sidebar": 653 },
  "Infrastructure as Code (IaC) Overview",
  "An overview of the infrastructure-as-code approach used in the FlowMart e-commerce platform",
  { "label": 654, "order": 470 },
  "01 - IaC Overview",
  "# Infrastructure as Code (IaC) Overview\n\nThis document provides an overview of the Infrastructure as Code (IaC) approach used to manage and provision the infrastructure for the FlowMart e-commerce platform.\n\n## What is Infrastructure as Code?\n\nInfrastructure as Code (IaC) is an approach to infrastructure management where infrastructure resources are defined and provisioned through machine-readable definition files, rather than through manual processes or interactive configuration tools. This approach allows us to:\n\n- **Version control** our infrastructure definitions alongside our application code\n- **Automate** the provisioning and management of infrastructure\n- **Standardize** configurations across different environments\n- **Document** our infrastructure setup as living code rather than static documentation\n- **Test** infrastructure changes before deploying to production\n\n## Our IaC Tech Stack\n\nFor the FlowMart e-commerce platform, we use the following technologies for our infrastructure management:\n\n### Primary Tools\n\n| Tool | Purpose |\n|------|---------|\n| **Terraform** | Infrastructure provisioning across cloud providers (primary tool) |\n| **Kubernetes (K8s)** | Container orchestration |\n| **Helm Charts** | Kubernetes application deployment packaging |\n| **GitHub Actions** | CI/CD pipeline automation |\n| **AWS CloudFormation** | Specific AWS infrastructure components |\n\n### Additional Supporting Tools\n\n| Tool | Purpose |\n|------|---------|\n| **Terragrunt** | Terraform code organization and management |\n| **Packer** | Virtual machine image building |\n| **Ansible** | Configuration management |\n| **Prometheus & Grafana** | Monitoring and alerting |\n| **ELK Stack** | Logging |\n\n## Infrastructure Architecture\n\nOur infrastructure is organized into the following logical components:\n\n```mermaid\nflowchart TD\n    classDef networkInfra fill:#e1d5e7,stroke:#9673a6,stroke-width:2px\n    classDef computeInfra fill:#d5e8d4,stroke:#82b366,stroke-width:2px\n    classDef datastoreInfra fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px\n    classDef securityInfra fill:#f8cecc,stroke:#b85450,stroke-width:2px\n    classDef monitoringInfra fill:#fff2cc,stroke:#d6b656,stroke-width:2px\n\n    subgraph \"Networking Infrastructure\"\n        VPC[VPC & Subnets]\n        IGW[Internet Gateway]\n        NLB[Network Load Balancer]\n        Route53[DNS - Route53]\n    end\n    \n    subgraph \"Compute Infrastructure\"\n        EKS[EKS Cluster]\n        EC2[EC2 Instances]\n        Lambda[Lambda Functions]\n    end\n    \n    subgraph \"Data Storage\"\n        RDS[RDS Databases]\n        DynamoDB[DynamoDB Tables]\n        S3[S3 Buckets]\n        ElastiCache[ElastiCache Redis]\n    end\n    \n    subgraph \"Security\"\n        IAM[IAM Roles & Policies]\n        SG[Security Groups]\n        WAF[WAF & Shield]\n        Secrets[Secrets Manager]\n    end\n    \n    subgraph \"Monitoring & Logging\"\n        CloudWatch[CloudWatch]\n        Prometheus[Prometheus]\n        Grafana[Grafana Dashboards]\n        ELK[ELK Stack]\n    end\n    \n    %% Connections\n    VPC --> IGW\n    IGW --> NLB\n    NLB --> EKS\n    Route53 --> NLB\n    \n    EKS --> SG\n    EC2 --> SG\n    Lambda --> SG\n    \n    EKS --> RDS\n    EKS --> DynamoDB\n    EKS --> S3\n    EKS --> ElastiCache\n    \n    IAM --> EKS\n    IAM --> Lambda\n    IAM --> RDS\n    \n    CloudWatch --> EKS\n    CloudWatch --> RDS\n    CloudWatch --> Lambda\n    Prometheus --> EKS\n    Grafana --> Prometheus\n    ELK --> EKS\n    \n    %% Apply styles\n    class VPC,IGW,NLB,Route53 networkInfra\n    class EKS,EC2,Lambda computeInfra\n    class RDS,DynamoDB,S3,ElastiCache datastoreInfra\n    class IAM,SG,WAF,Secrets securityInfra\n    class CloudWatch,Prometheus,Grafana,ELK monitoringInfra\n```\n\n## Repository Structure\n\nOur infrastructure code is organized as follows:\n\n```\ninfrastructure/\n│\n├── terraform/                  # Terraform configuration\n│   ├── environments/           # Environment-specific configurations\n│   │   ├── dev/\n│   │   ├── staging/\n│   │   └── production/\n│   ├── modules/                # Reusable Terraform modules\n│   │   ├── networking/\n│   │   ├── compute/\n│   │   ├── database/\n│   │   └── monitoring/\n│   └── global/                 # Global resources (e.g., Route53)\n│\n├── kubernetes/                 # Kubernetes manifests\n│   ├── base/                   # Base configurations\n│   └── overlays/               # Environment-specific overlays (Kustomize)\n│\n├── helm-charts/                # Helm charts for application deployment\n│\n├── scripts/                    # Utility scripts\n│\n└── packer/                     # Packer templates for image building\n```\n\n## Deployment Principles\n\n1. **Infrastructure Changes via Pull Requests**: All infrastructure changes must go through a pull request process, with automated testing and reviews.\n\n2. **Environment Promotion**: Changes are first deployed to development, then staging, and finally production, with appropriate testing at each stage.\n\n3. **Immutable Infrastructure**: We prefer to replace rather than modify infrastructure components.\n\n4. **Least Privilege**: We follow the principle of least privilege for all IAM roles and security groups.\n\n5. **Automated Rollbacks**: Our CI/CD pipelines include automated rollback capabilities if deployments fail.\n\n## Next Steps\n\nFor more detailed information about our infrastructure as code setup, please refer to the following documents:\n\n- [Terraform Implementation](./02-terraform-implementation.mdx)\n- [Environment Setups](./03-environment-setups.mdx)\n- [CI/CD Pipelines](./04-cicd-pipelines.mdx)",
  "../examples/default/docs/technical-architecture-design/infrastructure-as-code/01-iac-overview.mdx",
  "9ccba682a8f30dce",
  "docs/technical-architecture-design/infrastructure-as-code/04-cicd-pipelines",
  {
    "id": 658,
    "data": 660,
    "body": 665,
    "filePath": 666,
    "digest": 667,
    "deferredRender": 20
  },
  { "title": 661, "summary": 662, "sidebar": 663 },
  "CI/CD Pipelines",
  "Detailed overview of the CI/CD pipelines used to deploy and maintain the FlowMart e-commerce platform",
  { "label": 664, "order": 584 },
  "04 - CI/CD Pipelines",
  "# CI/CD Pipelines\n\nThis document provides an overview of the Continuous Integration (CI) and Continuous Deployment (CD) pipelines used to build, test, and deploy the FlowMart e-commerce platform.\n\n## CI/CD Philosophy\n\nOur CI/CD approach follows these key principles:\n\n1. **Automation First**: Automate everything that can be automated\n2. **Fast Feedback**: Provide developers with quick feedback on their changes\n3. **Consistency**: Ensure consistent builds and deployments across all environments\n4. **Security**: Integrate security testing throughout the pipeline\n5. **Observability**: Monitor and track all deployments and their impacts\n6. **Self-service**: Enable teams to deploy independently, but safely\n\n## CI/CD Technology Stack\n\nOur CI/CD pipeline utilizes the following key technologies:\n\n| Technology | Purpose |\n|------------|---------|\n| GitHub Actions | Main CI/CD orchestration platform |\n| ArgoCD | Kubernetes GitOps deployment tool |\n| Helm | Kubernetes package management |\n| Docker | Container building and registry |\n| Terraform | Infrastructure as Code deployment |\n| SonarQube | Code quality and security analysis |\n| Jest, JUnit, pytest | Unit testing frameworks |\n| Playwright | End-to-end testing |\n| K6 | Performance testing |\n| OWASP ZAP | Security scanning |\n| Snyk | Dependency vulnerability scanning |\n| AWS ECR | Container registry |\n\n## CI/CD Pipeline Overview\n\nOur CI/CD pipeline consists of multiple stages with specific responsibilities:\n\n```mermaid\nflowchart TD\n    classDef build fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px\n    classDef test fill:#d5e8d4,stroke:#82b366,stroke-width:2px\n    classDef deploy fill:#ffe6cc,stroke:#d79b00,stroke-width:2px\n    classDef release fill:#f8cecc,stroke:#b85450,stroke-width:2px\n\n    CodePush[Code Push] --> Build[Build & Package]\n    Build --> UnitTest[Unit Tests]\n    UnitTest --> CodeQuality[Code Quality Analysis]\n    CodeQuality --> SecurityScan[Security Scanning]\n    SecurityScan --> DockerBuild[Docker Build]\n    DockerBuild --> PushRegistry[Push to Registry]\n    PushRegistry --> DeployDev[Deploy to Dev]\n    DeployDev --> IntegrationTest[Integration Tests]\n    IntegrationTest --> DeployStaging[Deploy to Staging]\n    DeployStaging --> PerformanceTest[Performance Tests]\n    DeployStaging --> E2ETest[End-to-End Tests]\n    PerformanceTest --> Approval{Approval}\n    E2ETest --> Approval\n    Approval -->|Approved| DeployProd[Deploy to Production]\n    Approval -->|Rejected| Feedback[Feedback Loop]\n    DeployProd --> SmokeTest[Smoke Tests]\n    SmokeTest --> Monitoring[Monitoring & Observability]\n    \n    class Build,DockerBuild build\n    class UnitTest,CodeQuality,SecurityScan,IntegrationTest,PerformanceTest,E2ETest,SmokeTest test\n    class DeployDev,DeployStaging,DeployProd deploy\n    class Approval,Monitoring release\n```\n\n## Pipeline Stages in Detail\n\n### 1. Build & Package\n\n- Triggered by code push or pull request\n- Compiles application code\n- Installs dependencies using package managers (npm, Maven, pip)\n- Generates build artifacts\n- Built in isolated environments with cached dependencies\n\n```yaml\n# Example GitHub Actions code snippet\nbuild:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v3\n    - name: Set up Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n        cache: 'npm'\n    - name: Install dependencies\n      run: npm ci\n    - name: Build\n      run: npm run build\n    - name: Upload build artifacts\n      uses: actions/upload-artifact@v3\n      with:\n        name: build-artifacts\n        path: build/\n```\n\n### 2. Test\n\nMultiple types of tests run in parallel to provide rapid feedback:\n\n- **Unit Tests**: Test individual components in isolation\n- **Integration Tests**: Test component interactions\n- **End-to-End Tests**: Test complete user flows\n- **Performance Tests**: Test application performance under load\n- **Security Tests**: Scan for vulnerabilities in code and dependencies\n\n```mermaid\nflowchart LR\n    subgraph \"Test Phase\"\n        direction TB\n        Unit[Unit Tests]\n        Integration[Integration Tests]\n        E2E[End-to-End Tests]\n        Performance[Performance Tests]\n        Security[Security Tests]\n    end\n    \n    CodeBase[Code Base] --> Unit\n    CodeBase --> Integration\n    CodeBase --> E2E\n    CodeBase --> Performance\n    CodeBase --> Security\n    \n    Unit --> Results[Test Results]\n    Integration --> Results\n    E2E --> Results\n    Performance --> Results\n    Security --> Results\n```\n\n### 3. Docker Build & Registry Push\n\n- Builds Docker images for all services\n- Tags images with git commit SHA and environment\n- Pushes images to AWS ECR\n- Scans images for vulnerabilities before pushing\n\n```yaml\n# Example GitHub Actions code snippet\ndocker-build:\n  runs-on: ubuntu-latest\n  needs: [build, test]\n  steps:\n    - uses: actions/checkout@v3\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v1\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: us-west-2\n    - name: Login to Amazon ECR\n      id: login-ecr\n      uses: aws-actions/amazon-ecr-login@v1\n    - name: Build and push Docker image\n      uses: docker/build-push-action@v4\n      with:\n        context: .\n        push: true\n        tags: ${{ steps.login-ecr.outputs.registry }}/flowmart-orders-service:${{ github.sha }}\n```\n\n### 4. Deployment\n\nWe use GitOps with ArgoCD for managing deployments:\n\n- **Development**: Automatic deployment on successful build\n- **Staging**: Automatic deployment after integration tests pass\n- **Production**: Manual approval required, then automated deployment\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub Actions\n    participant Reg as Container Registry\n    participant Git as Git Repo (Manifests)\n    participant Argo as ArgoCD\n    participant K8s as Kubernetes Cluster\n    \n    Dev->>GH: Push Code\n    GH->>GH: Build & Test\n    GH->>Reg: Push Container Image\n    GH->>Git: Update Image Tag in Manifests\n    Git->>Argo: Sync (Automatic or Manual)\n    Argo->>K8s: Apply Manifests\n    K8s->>K8s: Deploy Application\n    K8s->>Argo: Report Status\n    Argo->>Git: Update Deployment Status\n```\n\n### 5. Post-Deployment Verification\n\n- **Smoke Tests**: Quick tests to verify basic functionality\n- **Canary Deployment**: Rolling deployment with traffic shifting\n- **Monitoring**: Performance and error tracking during and after deployment\n\n## Infrastructure Pipeline\n\nFor infrastructure changes, we have a separate pipeline:\n\n```mermaid\nflowchart TD\n    InfraChange[Infrastructure Change] --> TerraformPlan[Terraform Plan]\n    TerraformPlan --> AutoReview[Automated Review]\n    AutoReview --> HumanReview[Human Review]\n    HumanReview --> Approval{Approved?}\n    Approval -->|Yes| TerraformApply[Terraform Apply]\n    Approval -->|No| Feedback[Feedback Loop]\n    TerraformApply --> Verification[Infrastructure Verification]\n    Verification --> Documentation[Update Documentation]\n```\n\n## Feature Branch Workflow\n\nWe follow a feature branch workflow for development:\n\n```mermaid\ngitGraph\n    commit\n    commit\n    branch feature/order-tracking\n    checkout feature/order-tracking\n    commit\n    commit\n    commit\n    checkout main\n    merge feature/order-tracking\n    branch feature/payment-gateway\n    checkout feature/payment-gateway\n    commit\n    commit\n    checkout main\n    merge feature/payment-gateway\n    commit\n```\n\n## Deployment to Multiple Environments\n\nOur pipeline handles deployments to multiple environments:\n\n```mermaid\nflowchart TD\n    Build[Build & Test] --> DevDeploy[Deploy to Dev]\n    \n    DevDeploy --> IntegrationTest[Run Integration Tests]\n    IntegrationTest -->|Pass| StagingDeploy[Deploy to Staging]\n    IntegrationTest -->|Fail| FixIssues[Fix Issues]\n    FixIssues --> Build\n    \n    StagingDeploy --> StagingTests[Run E2E & Performance Tests]\n    StagingTests -->|Pass| ApprovalGate{Approval Gate}\n    StagingTests -->|Fail| FixIssues\n    \n    ApprovalGate -->|Approved| ProdDeploy[Deploy to Production]\n    ApprovalGate -->|Rejected| FixIssues\n    \n    ProdDeploy --> CanaryDeploy[Canary Deployment]\n    CanaryDeploy --> Monitor[Monitor]\n    Monitor -->|Healthy| FullRollout[Full Rollout]\n    Monitor -->|Issues| Rollback[Rollback]\n```\n\n## Rollback Strategy\n\nIn case of deployment issues, we have an automated rollback strategy:\n\n1. **Immediate Automated Rollback**: Triggered by health checks or error rate spikes\n2. **One-Click Manual Rollback**: Available through the deployment dashboard\n3. **Previous Version Restoration**: Reverts to the last known good state\n\n```mermaid\nsequenceDiagram\n    participant Metrics as Metrics System\n    participant CD as CD Pipeline\n    participant Git as Git Repository\n    participant K8s as Kubernetes\n    \n    Note over Metrics,K8s: New deployment shows issues\n    Metrics->>CD: Alert on error threshold exceeded\n    CD->>CD: Trigger rollback process\n    CD->>Git: Revert to previous manifest version\n    Git->>K8s: Apply previous manifests\n    K8s->>K8s: Restore previous deployment\n    K8s->>CD: Report successful rollback\n    CD->>Metrics: Verify metrics returning to normal\n```\n\n## Pipeline Security\n\nSecurity is integrated throughout our pipeline:\n\n- **Secrets Management**: Secrets stored in AWS Secrets Manager and injected at runtime\n- **SAST**: Static Application Security Testing integrated in build phase\n- **DAST**: Dynamic Application Security Testing during staging deployment\n- **Dependency Scanning**: Checks for vulnerable dependencies\n- **Container Scanning**: Scans container images for vulnerabilities\n- **Infrastructure Scanning**: Checks IaC for security misconfigurations\n\n## Observability\n\nOur pipeline provides comprehensive observability:\n\n- **Deployment Tracking**: Each deployment is traced from commit to production\n- **Metrics Collection**: Performance metrics before and after deployment\n- **Log Aggregation**: Centralized logging for all pipeline stages\n- **Alerting**: Automated alerts for pipeline failures or anomalies\n- **Dashboards**: Visual representation of pipeline health and history\n\nExample deployment dashboard:\n\n```mermaid\ngantt\n    title Recent Deployments Timeline\n    dateFormat  YYYY-MM-DD HH:mm\n    axisFormat %H:%M\n    \n    section Orders Service\n    Build #452           :a1, 2023-05-10 09:00, 5m\n    Deploy to Dev        :a2, after a1, 10m\n    Integration Tests    :a3, after a2, 15m\n    Deploy to Staging    :a4, after a3, 10m\n    E2E Tests            :a5, after a4, 30m\n    Deploy to Prod       :a6, after a5, 15m\n    \n    section Inventory Service\n    Build #389           :b1, 2023-05-10 10:00, 5m\n    Deploy to Dev        :b2, after b1, 10m\n    Integration Tests    :b3, after b2, 15m\n    Deploy to Staging    :b4, after b3, 10m\n    E2E Tests            :b5, after b4, 30m\n    \n    section Payment Service\n    Build #421           :c1, 2023-05-10 08:30, 5m\n    Deploy to Dev        :c2, after c1, 10m\n    Integration Tests    :c3, after c2, 15m\n    Deploy to Staging    :c4, after c3, 10m\n    E2E Tests            :c5, after c4, 30m\n    Deploy to Prod       :c6, after c5, 15m\n```\n\n## Continuous Improvement\n\nWe continuously improve our pipeline through:\n\n1. **Pipeline Metrics**: Track build times, success rates, and deployment frequency\n2. **Postmortems**: Document and learn from deployment failures\n3. **Automation Improvements**: Regularly identify manual steps for automation\n4. **Cross-team Learning**: Share best practices across development teams\n\n## Conclusion\n\nOur CI/CD pipeline provides a robust, secure, and efficient process for deploying changes to the FlowMart platform. By automating the build, test, and deployment processes, we can deliver new features and bug fixes to users quickly and reliably while maintaining high quality standards.",
  "../examples/default/docs/technical-architecture-design/infrastructure-as-code/04-cicd-pipelines.mdx",
  "b7e4e43b32243a8e",
  "docs/technical-architecture-design/system-architecture-diagrams/01-high-level-system-overview",
  {
    "id": 668,
    "data": 670,
    "body": 675,
    "filePath": 676,
    "digest": 677,
    "deferredRender": 20
  },
  { "title": 671, "summary": 672, "sidebar": 673 },
  "High-Level FlowMart System Architecture Overview",
  "A high-level overview of the FlowMart e-commerce system architecture",
  { "label": 674, "order": 470 },
  "01 - High-Level Overview",
  "# FlowMart System Architecture: High-Level Overview\n\nThis diagram provides a high-level overview of the FlowMart e-commerce platform architecture. It illustrates the main components and how they interact to deliver our online shopping experience.\n\n## Business Context\n\nFlowMart is an e-commerce platform that enables customers to browse products, place orders, make payments, and track shipments. The architecture is designed to be scalable, resilient, and maintainable, following domain-driven design and event-driven architecture principles.\n\n## High-Level Architecture Diagram\n\n```mermaid\nflowchart TB\n    subgraph \"Customer Channels\"\n        Web[\"Web Application\"]\n        Mobile[\"Mobile Apps\"]\n        ThirdParty[\"Third-Party Integrations\"]\n    end\n\n    subgraph \"API Gateway Layer\"\n        APIGateway[\"API Gateway / BFF\"]\n    end\n\n    subgraph \"Core Business Domains\"\n        Orders[\"Orders Domain\"]\n        Inventory[\"Inventory Domain\"]\n        Payment[\"Payment Domain\"]\n        Shipping[\"Shipping Domain\"]\n        Subscription[\"Subscription Domain\"]\n        Notification[\"Notification Domain\"]\n    end\n\n    subgraph \"Data Stores\"\n        OrdersDB[(Orders Database)]\n        InventoryDB[(Inventory Database)]\n        PaymentDB[(Payment Database)]\n        ShippingDB[(Shipping Database)]\n        SubscriptionDB[(Subscription Database)]\n    end\n\n    subgraph \"External Systems\"\n        PaymentGateways[\"Payment Gateways\"]\n        LogisticsProviders[\"Logistics Providers\"]\n        EmailSMS[\"Email/SMS Providers\"]\n    end\n\n    Web --> APIGateway\n    Mobile --> APIGateway\n    ThirdParty --> APIGateway\n    \n    APIGateway --> Orders\n    APIGateway --> Inventory\n    APIGateway --> Payment\n    APIGateway --> Shipping\n    APIGateway --> Subscription\n    \n    Orders \u003C--> OrdersDB\n    Inventory \u003C--> InventoryDB\n    Payment \u003C--> PaymentDB\n    Shipping \u003C--> ShippingDB\n    Subscription \u003C--> SubscriptionDB\n    \n    Orders \u003C--> Inventory\n    Orders \u003C--> Payment\n    Orders \u003C--> Shipping\n    Orders \u003C--> Notification\n    Payment \u003C--> Subscription\n    \n    Payment \u003C--> PaymentGateways\n    Shipping \u003C--> LogisticsProviders\n    Notification \u003C--> EmailSMS\n```\n\n## Component Descriptions\n\n### Customer Channels\n- **Web Application**: A responsive web interface for customers to browse products and place orders\n- **Mobile Apps**: Native iOS and Android applications for mobile shopping\n- **Third-Party Integrations**: External platforms that integrate with our services\n\n### API Gateway Layer\n- **API Gateway / BFF**: Backend for Frontend that routes requests, handles authentication, and optimizes responses for different clients\n\n### Core Business Domains\n- **Orders Domain**: Manages the order lifecycle from creation to fulfillment\n- **Inventory Domain**: Tracks product availability and stock levels\n- **Payment Domain**: Processes payments and manages financial transactions\n- **Shipping Domain**: Handles order delivery and shipment tracking\n- **Subscription Domain**: Manages recurring subscriptions and memberships\n- **Notification Domain**: Delivers notifications to customers across different channels\n\n### Data Stores\n- Each domain has its dedicated database to ensure domain isolation and independent scalability\n\n### External Systems\n- **Payment Gateways**: Third-party services for processing credit card payments\n- **Logistics Providers**: External shipping and delivery services\n- **Email/SMS Providers**: Services for sending notifications to customers\n\n## Key Architectural Principles\n\n1. **Domain-Driven Design**: Our system is organized around business domains and their bounded contexts\n2. **Event-Driven Architecture**: Services communicate primarily through events, enhancing decoupling and scalability\n3. **Microservices**: Each domain is implemented as one or more microservices with clear boundaries\n4. **API-First Approach**: All functionality is exposed through well-defined APIs\n5. **Polyglot Persistence**: Each domain can choose the most appropriate database technology\n\n## Next Steps\n\nFor a more detailed view of our architecture, refer to the following documents:\n- [Domain-Level Architecture](./02-domain-level-architecture.mdx)\n- [Service-Level Architecture](./03-service-level-architecture.mdx)\n- [Data Flow Architecture](./04-data-flow-architecture.mdx)",
  "../examples/default/docs/technical-architecture-design/system-architecture-diagrams/01-high-level-system-overview.mdx",
  "814d3e923d0f25f6",
  "docs/technical-architecture-design/system-architecture-diagrams/03-service-level-architecture",
  {
    "id": 678,
    "data": 680,
    "body": 685,
    "filePath": 686,
    "digest": 687,
    "deferredRender": 20
  },
  { "title": 681, "summary": 682, "sidebar": 683 },
  "FlowMart Service-Level Architecture",
  "A detailed view of the services within each domain and their interactions",
  { "label": 684, "order": 515 },
  "03 - Service Architecture",
  "# FlowMart Service-Level Architecture\n\nThis document provides a detailed view of the individual services within FlowMart's e-commerce platform, their responsibilities, and how they interact.\n\n## Service Architecture Overview\n\nThe following diagram illustrates the services that make up our platform and how they communicate:\n\n```mermaid\nflowchart TD\n    classDef apiGateway fill:#f8cecc,stroke:#b85450,stroke-width:2px\n    classDef orderService fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px\n    classDef inventoryService fill:#d5e8d4,stroke:#82b366,stroke-width:2px\n    classDef paymentService fill:#ffe6cc,stroke:#d79b00,stroke-width:2px\n    classDef shippingService fill:#e1d5e7,stroke:#9673a6,stroke-width:2px\n    classDef notificationService fill:#fff2cc,stroke:#d6b656,stroke-width:2px\n    classDef subscriptionService fill:#f8cecc,stroke:#b85450,stroke-width:2px\n    classDef datastore fill:#f5f5f5,stroke:#666666,stroke-width:1px\n    classDef messagebroker fill:#e6ffcc,stroke:#36b37e,stroke-width:2px\n    classDef externalSystem fill:#f5f5f5,stroke:#666666,stroke-width:1px,stroke-dasharray: 5 5\n\n    APIGateway[API Gateway]\n    EventBus[Event Bus / Message Broker]\n    \n    subgraph OrdersDomain[\"Orders Domain\"]\n        OrdersService[Orders Service]\n        OrdersDB[(Orders Database)]\n    end\n    \n    subgraph InventoryDomain[\"Inventory Domain\"]\n        InventoryService[Inventory Service]\n        InventoryDB[(Inventory Database)]\n    end\n    \n    subgraph PaymentDomain[\"Payment Domain\"]\n        PaymentService[Payment Service]\n        PaymentDB[(Payment Database)]\n    end\n    \n    subgraph ShippingDomain[\"Shipping Domain\"]\n        ShippingService[Shipping Service]\n        ShippingDB[(Shipping Database)]\n    end\n    \n    subgraph NotificationDomain[\"Notification Domain\"]\n        NotificationService[Notification Service]\n    end\n    \n    subgraph SubscriptionDomain[\"Subscription Domain\"]\n        SubscriptionService[Subscription Service]\n        SubscriptionDB[(Subscription Database)]\n    end\n    \n    subgraph ExternalSystems[\"External Systems\"]\n        PaymentGateway[Payment Gateway]\n        EmailProvider[Email Provider]\n        SMSProvider[SMS Provider]\n        LogisticsPartner[Logistics Partner]\n    end\n    \n    %% API Gateway connections\n    APIGateway --> OrdersService\n    APIGateway --> InventoryService\n    APIGateway --> PaymentService\n    APIGateway --> ShippingService\n    APIGateway --> SubscriptionService\n    \n    %% Database connections\n    OrdersService \u003C--> OrdersDB\n    InventoryService \u003C--> InventoryDB\n    PaymentService \u003C--> PaymentDB\n    ShippingService \u003C--> ShippingDB\n    SubscriptionService \u003C--> SubscriptionDB\n    \n    %% Event bus connections\n    OrdersService \u003C--> EventBus\n    InventoryService \u003C--> EventBus\n    PaymentService \u003C--> EventBus\n    ShippingService \u003C--> EventBus\n    NotificationService \u003C--> EventBus\n    SubscriptionService \u003C--> EventBus\n    \n    %% External system connections\n    PaymentService --> PaymentGateway\n    NotificationService --> EmailProvider\n    NotificationService --> SMSProvider\n    ShippingService --> LogisticsPartner\n    \n    %% Apply styles\n    class APIGateway apiGateway\n    class OrdersService orderService\n    class InventoryService inventoryService\n    class PaymentService paymentService\n    class ShippingService shippingService\n    class NotificationService notificationService\n    class SubscriptionService subscriptionService\n    class OrdersDB,InventoryDB,PaymentDB,ShippingDB,SubscriptionDB datastore\n    class EventBus messagebroker\n    class PaymentGateway,EmailProvider,SMSProvider,LogisticsPartner externalSystem\n```\n\n## Service Component Details\n\n### API Gateway\n- **Description**: Entry point for all client requests, handles routing, authentication, and load balancing\n- **Technologies**: AWS API Gateway, Kong, or Nginx\n- **Key Responsibilities**:\n  - Route requests to appropriate services\n  - Handle authentication and authorization\n  - API rate limiting and throttling\n  - Request validation\n  - Response caching\n\n### Orders Service\n- **Description**: Manages the entire lifecycle of customer orders\n- **Technologies**: Node.js, Express, MongoDB\n- **Key Responsibilities**:\n  - Create and manage orders\n  - Process order amendments and cancellations\n  - Coordinate with other services for order fulfillment\n  - Maintain order history and status\n- **Key Events**:\n  - Publishes: OrderConfirmed, OrderCancelled, OrderAmended\n  - Consumes: InventoryAdjusted, PaymentProcessed, UserSubscriptionCancelled\n\n### Inventory Service\n- **Description**: Tracks and manages product inventory across warehouses\n- **Technologies**: Java, Spring Boot, PostgreSQL\n- **Key Responsibilities**:\n  - Maintain accurate inventory levels\n  - Process inventory adjustments\n  - Monitor stock levels and trigger alerts\n  - Support inventory queries\n- **Key Events**:\n  - Publishes: InventoryAdjusted, OutOfStock\n  - Consumes: OrderConfirmed, OrderCancelled, OrderAmended\n\n### Payment Service\n- **Description**: Handles all payment processing and financial transactions\n- **Technologies**: Node.js, Express, PostgreSQL\n- **Key Responsibilities**:\n  - Process customer payments\n  - Manage refunds and chargebacks\n  - Integrate with payment gateways\n  - Track payment status\n- **Key Events**:\n  - Publishes: PaymentProcessed\n  - Consumes: PaymentInitiated, UserSubscriptionStarted, InventoryAdjusted\n\n### Shipping Service\n- **Description**: Manages logistics and shipment of orders\n- **Technologies**: Python, Flask, MongoDB\n- **Key Responsibilities**:\n  - Create and track shipments\n  - Integrate with logistics providers\n  - Process returns and exchanges\n  - Calculate shipping costs\n- **Key Events**:\n  - Publishes: ShipmentCreated, ShipmentDispatched, ShipmentInTransit, ShipmentDelivered, DeliveryFailed, ReturnInitiated\n  - Consumes: OrderConfirmed, PaymentProcessed\n\n### Notification Service\n- **Description**: Delivers notifications to customers through various channels\n- **Technologies**: Node.js, Express, Redis\n- **Key Responsibilities**:\n  - Send transactional emails\n  - Deliver SMS notifications\n  - Push mobile app notifications\n  - Store notification history\n- **Key Events**:\n  - Consumes: InventoryAdjusted, OutOfStock, PaymentProcessed\n\n### Subscription Service\n- **Description**: Manages recurring subscriptions and memberships\n- **Technologies**: Java, Spring Boot, MySQL\n- **Key Responsibilities**:\n  - Handle subscription lifecycle\n  - Process recurring billing\n  - Manage subscription plans and tiers\n  - Track subscription status\n- **Key Events**:\n  - Publishes: UserSubscriptionStarted, UserSubscriptionCancelled\n  - Consumes: PaymentProcessed\n\n### Event Bus / Message Broker\n- **Description**: Central messaging system that enables asynchronous communication between services\n- **Technologies**: Apache Kafka, RabbitMQ, or AWS SNS/SQS\n- **Key Responsibilities**:\n  - Reliable event delivery\n  - Support for event persistence\n  - Message routing\n  - Scalable message throughput\n\n## Service Interaction Patterns\n\n### Synchronous Communication\nServices communicate directly with each other through REST APIs for operations that require immediate responses.\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant OrdersService\n    participant InventoryService\n    participant PaymentService\n    \n    Client->>OrdersService: Place Order\n    OrdersService->>InventoryService: Check Inventory Availability\n    InventoryService-->>OrdersService: Inventory Available\n    OrdersService->>PaymentService: Process Payment\n    PaymentService-->>OrdersService: Payment Successful\n    OrdersService-->>Client: Order Confirmation\n```\n\n### Asynchronous Communication\nServices communicate through events published to the event bus, allowing for loose coupling.\n\n```mermaid\nsequenceDiagram\n    participant OrdersService\n    participant EventBus\n    participant InventoryService\n    participant ShippingService\n    participant NotificationService\n    \n    OrdersService->>EventBus: Publish OrderConfirmed Event\n    EventBus->>InventoryService: OrderConfirmed Event\n    InventoryService->>EventBus: Publish InventoryAdjusted Event\n    EventBus->>ShippingService: OrderConfirmed Event\n    ShippingService->>EventBus: Publish ShipmentCreated Event\n    EventBus->>NotificationService: ShipmentCreated Event\n    NotificationService->>Customer: Send Shipment Notification\n```\n\n## Infrastructure Considerations\n\n- All services are containerized using Docker and orchestrated with Kubernetes\n- Services are deployed across multiple availability zones for high availability\n- Auto-scaling is configured based on CPU and memory metrics\n- Database replication and backups are implemented for data durability\n- Centralized logging and monitoring using Prometheus and Grafana\n\n## Next Steps\n\nFor more information about data flows within the system, refer to:\n- [Data Flow Architecture](./04-data-flow-architecture.mdx)",
  "../examples/default/docs/technical-architecture-design/system-architecture-diagrams/03-service-level-architecture.mdx",
  "93c1fe4134f46a1e",
  "docs/technical-architecture-design/system-architecture-diagrams/02-domain-level-architecture",
  {
    "id": 688,
    "data": 690,
    "body": 695,
    "filePath": 696,
    "digest": 697,
    "deferredRender": 20
  },
  { "title": 691, "summary": 692, "sidebar": 693 },
  "FlowMart Domain-Level Architecture",
  "A detailed view of FlowMart's domain architecture showing bounded contexts and domain interactions",
  { "label": 694, "order": 487 },
  "02 - Domain Architecture",
  "# FlowMart Domain-Level Architecture\n\nThis document describes the domain-level architecture of the FlowMart e-commerce platform, showing how different domains interact with each other through well-defined interfaces and events.\n\n## Domain Architecture Diagram\n\nThe following diagram illustrates the bounded contexts of each domain and how they communicate with each other:\n\n```mermaid\nflowchart TD\n    classDef domainStyle fill:#f9f9f9,stroke:#333,stroke-width:2px\n    classDef eventStyle fill:#e1f5fe,stroke:#0288d1,stroke-width:1px,color:#01579b\n    classDef commandStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:1px,color:#4a148c\n    classDef queryStyle fill:#e8f5e9,stroke:#388e3c,stroke-width:1px,color:#1b5e20\n\n    subgraph Orders[\"Orders Domain\"]\n        class Orders domainStyle\n        OrdersService[\"Orders Service\"]\n        \n        subgraph OrderEvents[\" \"]\n            OrderConfirmed[\"OrderConfirmed Event\"]\n            OrderCancelled[\"OrderCancelled Event\"]\n            OrderAmended[\"OrderAmended Event\"]\n        end\n        \n        subgraph OrderCommands[\" \"]\n            PlaceOrder[\"PlaceOrder Command\"]\n        end\n        \n        subgraph OrderQueries[\" \"]\n            GetOrder[\"GetOrder Query\"]\n        end\n    end\n    \n    subgraph Inventory[\"Inventory Domain\"]\n        class Inventory domainStyle\n        InventoryService[\"Inventory Service\"]\n        \n        subgraph InventoryEvents[\" \"]\n            InventoryAdjusted[\"InventoryAdjusted Event\"]\n            OutOfStock[\"OutOfStock Event\"]\n        end\n        \n        subgraph InventoryCommands[\" \"]\n            AddInventory[\"AddInventory Command\"]\n            UpdateInventory[\"UpdateInventory Command\"]\n            DeleteInventory[\"DeleteInventory Command\"]\n        end\n        \n        subgraph InventoryQueries[\" \"]\n            GetInventoryStatus[\"GetInventoryStatus Query\"]\n            GetInventoryList[\"GetInventoryList Query\"]\n        end\n    end\n    \n    subgraph Payment[\"Payment Domain\"]\n        class Payment domainStyle\n        PaymentService[\"Payment Service\"]\n        \n        subgraph PaymentEvents[\" \"]\n            PaymentProcessed[\"PaymentProcessed Event\"]\n        end\n        \n        subgraph PaymentCommands[\" \"]\n            PaymentInitiated[\"PaymentInitiated Command\"]\n        end\n        \n        subgraph PaymentQueries[\" \"]\n            GetPaymentStatus[\"GetPaymentStatus Query\"]\n        end\n    end\n    \n    subgraph Shipping[\"Shipping Domain\"]\n        class Shipping domainStyle\n        ShippingService[\"Shipping Service\"]\n        \n        subgraph ShippingEvents[\" \"]\n            ShipmentCreated[\"ShipmentCreated Event\"]\n            ShipmentDispatched[\"ShipmentDispatched Event\"]\n            ShipmentInTransit[\"ShipmentInTransit Event\"]\n            ShipmentDelivered[\"ShipmentDelivered Event\"]\n            DeliveryFailed[\"DeliveryFailed Event\"]\n            ReturnInitiated[\"ReturnInitiated Event\"]\n        end\n        \n        subgraph ShippingCommands[\" \"]\n            CreateShipment[\"CreateShipment Command\"]\n            UpdateShipmentStatus[\"UpdateShipmentStatus Command\"]\n            CancelShipment[\"CancelShipment Command\"]\n            CreateReturnLabel[\"CreateReturnLabel Command\"]\n        end\n    end\n    \n    subgraph Subscription[\"Subscription Domain\"]\n        class Subscription domainStyle\n        SubscriptionService[\"Subscription Service\"]\n        \n        subgraph SubscriptionEvents[\" \"]\n            UserSubscriptionStarted[\"UserSubscriptionStarted Event\"]\n            UserSubscriptionCancelled[\"UserSubscriptionCancelled Event\"]\n        end\n        \n        subgraph SubscriptionCommands[\" \"]\n            SubscribeUser[\"SubscribeUser Command\"]\n            CancelSubscription[\"CancelSubscription Command\"]\n        end\n        \n        subgraph SubscriptionQueries[\" \"]\n            GetSubscriptionStatus[\"GetSubscriptionStatus Query\"]\n        end\n    end\n    \n    subgraph Notification[\"Notification Domain\"]\n        class Notification domainStyle\n        NotificationService[\"Notification Service\"]\n        \n        subgraph NotificationQueries[\" \"]\n            GetUserNotifications[\"GetUserNotifications Query\"]\n            GetNotificationDetails[\"GetNotificationDetails Query\"]\n        end\n    end\n\n    %% Domain Interactions through Events\n    OrderConfirmed -->|Consumed by| InventoryService\n    OrderConfirmed -->|Consumed by| ShippingService\n    OrderCancelled -->|Consumed by| InventoryService\n    OrderAmended -->|Consumed by| InventoryService\n    \n    InventoryAdjusted -->|Consumed by| OrdersService\n    InventoryAdjusted -->|Consumed by| NotificationService\n    OutOfStock -->|Consumed by| NotificationService\n    \n    PaymentProcessed -->|Consumed by| OrdersService\n    PaymentProcessed -->|Consumed by| ShippingService\n    PaymentProcessed -->|Consumed by| SubscriptionService\n    \n    UserSubscriptionCancelled -->|Consumed by| OrdersService\n\n    %% Apply styles to events, commands and queries\n    class OrderConfirmed,OrderCancelled,OrderAmended,InventoryAdjusted,OutOfStock,PaymentProcessed,ShipmentCreated,ShipmentDispatched,ShipmentInTransit,ShipmentDelivered,DeliveryFailed,ReturnInitiated,UserSubscriptionStarted,UserSubscriptionCancelled eventStyle\n    \n    class PlaceOrder,AddInventory,UpdateInventory,DeleteInventory,PaymentInitiated,CreateShipment,UpdateShipmentStatus,CancelShipment,CreateReturnLabel,SubscribeUser,CancelSubscription commandStyle\n    \n    class GetOrder,GetInventoryStatus,GetInventoryList,GetPaymentStatus,GetSubscriptionStatus,GetUserNotifications,GetNotificationDetails queryStyle\n```\n\n## Domain Descriptions\n\n### Orders Domain\nThe Orders Domain is the central domain of our e-commerce platform. It manages the entire lifecycle of customer orders, from creation to fulfillment. It communicates with other domains to check inventory availability, process payments, and arrange shipping.\n\n**Key Events Published:**\n- OrderConfirmed\n- OrderCancelled \n- OrderAmended\n\n**Commands:**\n- PlaceOrder\n\n**Queries:**\n- GetOrder\n\n### Inventory Domain\nThe Inventory Domain manages product stock levels across all warehouses. It ensures accurate inventory tracking and provides real-time stock information to other domains.\n\n**Key Events Published:**\n- InventoryAdjusted\n- OutOfStock\n\n**Commands:**\n- AddInventory\n- UpdateInventory\n- DeleteInventory\n\n**Queries:**\n- GetInventoryStatus\n- GetInventoryList\n\n### Payment Domain\nThe Payment Domain handles all financial transactions within the platform. It processes customer payments, manages refunds, and communicates with external payment gateways.\n\n**Key Events Published:**\n- PaymentProcessed\n\n**Commands:**\n- PaymentInitiated\n\n**Queries:**\n- GetPaymentStatus\n\n### Shipping Domain\nThe Shipping Domain manages order delivery to customers. It tracks shipment status, handles returns, and integrates with external logistics providers.\n\n**Key Events Published:**\n- ShipmentCreated\n- ShipmentDispatched\n- ShipmentInTransit\n- ShipmentDelivered\n- DeliveryFailed\n- ReturnInitiated\n\n**Commands:**\n- CreateShipment\n- UpdateShipmentStatus\n- CancelShipment\n- CreateReturnLabel\n\n### Subscription Domain\nThe Subscription Domain manages recurring customer subscriptions. It handles subscription lifecycle, billing cycles, and renewal processes.\n\n**Key Events Published:**\n- UserSubscriptionStarted\n- UserSubscriptionCancelled\n\n**Commands:**\n- SubscribeUser\n- CancelSubscription\n\n**Queries:**\n- GetSubscriptionStatus\n\n### Notification Domain\nThe Notification Domain is responsible for sending notifications to customers through various channels like email, SMS, and push notifications.\n\n**Queries:**\n- GetUserNotifications\n- GetNotificationDetails\n\n## Domain Integration Patterns\n\nThe domains in FlowMart's architecture interact using several integration patterns:\n\n1. **Event-Driven Communication**: Most domain interactions occur through asynchronous events published to a message broker, promoting loose coupling.\n\n2. **Synchronous API Calls**: For operations requiring immediate responses, domains expose REST APIs that can be called synchronously.\n\n3. **Saga Pattern**: For complex workflows spanning multiple domains, we use choreography-based sagas where services respond to events to maintain data consistency.\n\n4. **CQRS**: We separate command (write) and query (read) operations, allowing for optimization of each path.\n\n## Next Steps\n\nFor more detailed architectural information, see:\n- [Service-Level Architecture](./03-service-level-architecture.mdx)\n- [Data Flow Architecture](./04-data-flow-architecture.mdx)",
  "../examples/default/docs/technical-architecture-design/system-architecture-diagrams/02-domain-level-architecture.mdx",
  "073b6ae659757b56",
  "docs/technical-architecture-design/system-architecture-diagrams/04-data-flow-architecture",
  {
    "id": 698,
    "data": 700,
    "body": 705,
    "filePath": 706,
    "digest": 707,
    "deferredRender": 20
  },
  { "title": 701, "summary": 702, "sidebar": 703 },
  "FlowMart Data Flow Architecture",
  "A detailed view of key data flows and business processes within the FlowMart e-commerce platform",
  { "label": 704, "order": 584 },
  "04 - Data Flow Architecture",
  "# FlowMart Data Flow Architecture\n\nThis document illustrates the key data flows within the FlowMart e-commerce platform, focusing on the most important business processes and how data moves through the system.\n\n## Key Business Process Flows\n\n### Order Placement and Fulfillment Flow\n\nThis diagram shows the complete flow from order placement to order fulfillment:\n\n```mermaid\nflowchart TD\n    classDef customerAction fill:#ffcccc,stroke:#ff0000\n    classDef systemProcess fill:#ccffcc,stroke:#00aa00\n    classDef decisionPoint fill:#ffffcc,stroke:#ffcc00\n    classDef externalSystem fill:#ccccff,stroke:#0000ff\n    classDef dataStore fill:#f5f5f5,stroke:#333333\n\n    Start([Customer starts checkout]) --> ValidateCart[Validate Shopping Cart]\n    class Start customerAction\n    \n    ValidateCart --> CartValid{Is cart valid?}\n    class ValidateCart systemProcess\n    class CartValid decisionPoint\n    \n    CartValid -->|No| UpdateCart[Customer updates cart]\n    class UpdateCart customerAction\n    \n    UpdateCart --> ValidateCart\n    \n    CartValid -->|Yes| CheckInventory[Check Inventory Availability]\n    class CheckInventory systemProcess\n    \n    CheckInventory --> InventoryAvailable{Inventory available?}\n    class InventoryAvailable decisionPoint\n    \n    InventoryAvailable -->|No| NotifyCustomer[Notify Customer of Unavailability]\n    class NotifyCustomer systemProcess\n    \n    NotifyCustomer --> UpdateCart\n    \n    InventoryAvailable -->|Yes| ProcessPayment[Process Payment]\n    class ProcessPayment systemProcess\n    \n    ProcessPayment --> PaymentGateway[Payment Gateway]\n    class PaymentGateway externalSystem\n    \n    PaymentGateway --> PaymentSuccessful{Payment successful?}\n    class PaymentSuccessful decisionPoint\n    \n    PaymentSuccessful -->|No| NotifyPaymentFailure[Notify Payment Failure]\n    class NotifyPaymentFailure systemProcess\n    \n    NotifyPaymentFailure --> UpdatePayment[Customer updates payment]\n    class UpdatePayment customerAction\n    \n    UpdatePayment --> ProcessPayment\n    \n    PaymentSuccessful -->|Yes| CreateOrder[Create Order]\n    class CreateOrder systemProcess\n    \n    CreateOrder --> OrderDB[(Orders Database)]\n    class OrderDB dataStore\n    \n    CreateOrder --> ReserveInventory[Reserve Inventory]\n    class ReserveInventory systemProcess\n    \n    ReserveInventory --> InventoryDB[(Inventory Database)]\n    class InventoryDB dataStore\n    \n    ReserveInventory --> CreateShipment[Create Shipment]\n    class CreateShipment systemProcess\n    \n    CreateShipment --> ShipmentDB[(Shipment Database)]\n    class ShipmentDB dataStore\n    \n    CreateShipment --> NotifyCustomerOrder[Send Order Confirmation]\n    class NotifyCustomerOrder systemProcess\n    \n    NotifyCustomerOrder --> End([Order Placement Complete])\n    class End customerAction\n```\n\n### Payment Processing Flow\n\nThis diagram details the payment processing flow:\n\n```mermaid\nsequenceDiagram\n    participant Customer\n    participant OrdersService\n    participant PaymentService\n    participant PaymentGateway\n    participant PaymentDB\n    participant EventBus\n    participant NotificationService\n\n    Customer->>OrdersService: Place Order\n    OrdersService->>PaymentService: Request Payment\n    PaymentService->>PaymentGateway: Process Payment\n    \n    alt Payment Successful\n        PaymentGateway->>PaymentService: Confirm Payment\n        PaymentService->>PaymentDB: Store Payment Information\n        PaymentService->>EventBus: Publish PaymentProcessed Event\n        EventBus->>OrdersService: PaymentProcessed Event\n        EventBus->>NotificationService: PaymentProcessed Event\n        NotificationService->>Customer: Send Payment Confirmation\n        OrdersService->>Customer: Complete Order\n    else Payment Failed\n        PaymentGateway->>PaymentService: Payment Failure\n        PaymentService->>PaymentDB: Log Failed Transaction\n        PaymentService->>OrdersService: Payment Failed Response\n        OrdersService->>Customer: Request Different Payment Method\n    end\n```\n\n### Inventory Management Flow\n\nThis diagram shows how inventory is managed across the system:\n\n```mermaid\nstateDiagram-v2\n    [*] --> Available: Initial Stock\n    \n    Available --> Reserved: Customer Places Order\n    Reserved --> Allocated: Order Confirmed\n    Reserved --> Available: Order Cancelled\n    \n    Allocated --> Shipped: Order Shipped\n    Shipped --> Delivered: Order Delivered\n    Delivered --> [*]\n    \n    Available --> Replenished: Inventory Low\n    Replenished --> Available: Stock Received\n    \n    Available --> OutOfStock: All Units Reserved\n    OutOfStock --> Available: Replenishment\n```\n\n### Subscription Processing Flow\n\nThis diagram illustrates the subscription management flow:\n\n```mermaid\nflowchart TD\n    classDef process fill:#d5e8d4,stroke:#82b366\n    classDef event fill:#dae8fc,stroke:#6c8ebf\n    classDef externalSystem fill:#f5f5f5,stroke:#666666,stroke-width:1px\n    classDef decision fill:#fff2cc,stroke:#d6b656\n\n    Start([Start Subscription Process]) --> NewSubscription[Create Subscription]\n    class NewSubscription process\n\n    NewSubscription --> InitialPayment[Process Initial Payment]\n    class InitialPayment process\n    \n    InitialPayment --> PaymentSystem[Payment Gateway]\n    class PaymentSystem externalSystem\n    \n    PaymentSystem --> PaymentSuccessful{Payment Successful?}\n    class PaymentSuccessful decision\n    \n    PaymentSuccessful -->|Yes| ActivateSubscription[Activate Subscription]\n    class ActivateSubscription process\n    \n    PaymentSuccessful -->|No| FailedSubscription[Failed Subscription]\n    class FailedSubscription process\n    \n    FailedSubscription --> NotifyFailure[Notify Customer of Failure]\n    class NotifyFailure process\n    \n    ActivateSubscription --> UserSubscriptionStarted[Publish UserSubscriptionStarted Event]\n    class UserSubscriptionStarted event\n    \n    UserSubscriptionStarted --> ScheduleRenewal[Schedule Next Renewal]\n    class ScheduleRenewal process\n    \n    ScheduleRenewal --> TimeForRenewal{Time for Renewal?}\n    class TimeForRenewal decision\n    \n    TimeForRenewal -->|Yes| ProcessRenewalPayment[Process Renewal Payment]\n    class ProcessRenewalPayment process\n    \n    TimeForRenewal -->|No| Wait[Wait for Renewal Date]\n    class Wait process\n    \n    Wait --> TimeForRenewal\n    \n    ProcessRenewalPayment --> PaymentSystem\n    \n    PaymentSystem --> RenewalSuccessful{Renewal Successful?}\n    class RenewalSuccessful decision\n    \n    RenewalSuccessful -->|Yes| ExtendSubscription[Extend Subscription]\n    class ExtendSubscription process\n    \n    RenewalSuccessful -->|No, after retries| CancelSubscription[Cancel Subscription]\n    class CancelSubscription process\n    \n    ExtendSubscription --> ScheduleRenewal\n    \n    CancelSubscription --> UserSubscriptionCancelled[Publish UserSubscriptionCancelled Event]\n    class UserSubscriptionCancelled event\n    \n    UserSubscriptionCancelled --> End([End Subscription Process])\n```\n\n## Detailed Data Flow Examples\n\n### Customer Order Flow\n\nThe following diagram shows the data flow when a customer places an order:\n\n```mermaid\nsequenceDiagram\n    participant Customer\n    participant Web/Mobile App\n    participant API Gateway\n    participant OrdersService\n    participant InventoryService\n    participant PaymentService\n    participant ShippingService\n    participant NotificationService\n    participant EventBus\n\n    Customer->>Web/Mobile App: Add items to cart\n    Customer->>Web/Mobile App: Proceed to checkout\n    Web/Mobile App->>API Gateway: POST /orders\n    API Gateway->>OrdersService: Create order request\n    \n    OrdersService->>InventoryService: Check inventory availability\n    InventoryService-->>OrdersService: Inventory status\n    \n    OrdersService->>PaymentService: Process payment\n    PaymentService-->>OrdersService: Payment result\n    \n    alt Order Successful\n        OrdersService->>EventBus: Publish OrderConfirmed\n        EventBus->>InventoryService: OrderConfirmed\n        InventoryService->>EventBus: Publish InventoryAdjusted\n        EventBus->>ShippingService: OrderConfirmed\n        ShippingService->>EventBus: Publish ShipmentCreated\n        EventBus->>NotificationService: OrderConfirmed\n        NotificationService->>Customer: Send order confirmation\n    else Order Failed\n        OrdersService->>EventBus: Publish OrderFailed\n        EventBus->>NotificationService: OrderFailed\n        NotificationService->>Customer: Send failure notification\n    end\n```\n\n### Product Return Flow\n\nThe following diagram shows the data flow when a customer returns a product:\n\n```mermaid\nsequenceDiagram\n    participant Customer\n    participant Web/Mobile App\n    participant API Gateway\n    participant OrdersService\n    participant ShippingService\n    participant InventoryService\n    participant PaymentService\n    participant NotificationService\n    participant EventBus\n\n    Customer->>Web/Mobile App: Request return\n    Web/Mobile App->>API Gateway: POST /returns\n    API Gateway->>OrdersService: Create return request\n    OrdersService->>ShippingService: Generate return label\n    \n    ShippingService->>EventBus: Publish ReturnInitiated\n    EventBus->>NotificationService: ReturnInitiated\n    NotificationService->>Customer: Send return instructions\n    \n    Note over Customer,ShippingService: Customer ships the item back\n    \n    ShippingService->>EventBus: Publish ReturnReceived\n    EventBus->>InventoryService: ReturnReceived\n    InventoryService->>EventBus: Publish InventoryAdjusted\n    \n    EventBus->>PaymentService: ReturnReceived\n    PaymentService->>EventBus: Publish RefundIssued\n    \n    EventBus->>NotificationService: RefundIssued\n    NotificationService->>Customer: Send refund confirmation\n```\n\n## Data Storage Overview\n\nThe following diagram provides a high-level overview of the data storage architecture:\n\n```mermaid\nerDiagram\n    CUSTOMER ||--o{ ORDER : places\n    CUSTOMER ||--o{ SUBSCRIPTION : subscribes\n    ORDER ||--|{ ORDER_ITEM : contains\n    ORDER ||--|| SHIPMENT : fulfilled-by\n    ORDER ||--|| PAYMENT : paid-by\n    PRODUCT ||--o{ ORDER_ITEM : included-in\n    PRODUCT ||--o{ INVENTORY : stocked-as\n    WAREHOUSE ||--o{ INVENTORY : holds\n    SUBSCRIPTION ||--o{ SUBSCRIPTION_PAYMENT : generates\n    \n    CUSTOMER {\n        string id PK\n        string firstName\n        string lastName\n        string email\n        string phone\n        datetime createdAt\n    }\n    \n    ORDER {\n        string id PK\n        string customerId FK\n        string status\n        decimal totalAmount\n        datetime orderDate\n    }\n    \n    ORDER_ITEM {\n        string id PK\n        string orderId FK\n        string productId FK\n        int quantity\n        decimal unitPrice\n    }\n    \n    PRODUCT {\n        string id PK\n        string name\n        string description\n        decimal price\n        string category\n    }\n    \n    INVENTORY {\n        string id PK\n        string productId FK\n        string warehouseId FK\n        int quantity\n        int reserved\n    }\n    \n    WAREHOUSE {\n        string id PK\n        string name\n        string location\n    }\n    \n    SHIPMENT {\n        string id PK\n        string orderId FK\n        string status\n        string trackingNumber\n        datetime shippedDate\n    }\n    \n    PAYMENT {\n        string id PK\n        string orderId FK\n        decimal amount\n        string paymentMethod\n        string status\n        datetime paymentDate\n    }\n    \n    SUBSCRIPTION {\n        string id PK\n        string customerId FK\n        string status\n        string plan\n        date startDate\n        date endDate\n        string billingCycle\n    }\n    \n    SUBSCRIPTION_PAYMENT {\n        string id PK\n        string subscriptionId FK\n        decimal amount\n        datetime paymentDate\n        string status\n    }\n```\n\n## Event Flow and Message Schema\n\nThe following diagram shows a sample of our event structure and flow:\n\n```mermaid\ngraph TD\n    classDef publisher fill:#d1e0f0,stroke:#6c8ebf\n    classDef event fill:#f8cecc,stroke:#b85450\n    classDef consumer fill:#d5e8d4,stroke:#82b366\n\n    OrdersService[Orders Service]:::publisher --> OrderConfirmed[OrderConfirmed Event]:::event\n    \n    subgraph OrderConfirmedSchema[ ]\n        direction TB\n        OrderConfirmedMessage[\"OrderConfirmed Schema\n        {\n          orderId: string,\n          userId: string,\n          orderItems: [\n            {\n              productId: string,\n              quantity: number,\n              unitPrice: number\n            }\n          ],\n          totalAmount: number,\n          timestamp: datetime\n        }\"]\n    end\n    \n    OrderConfirmed --- OrderConfirmedSchema\n    \n    OrderConfirmed --> InventoryService[Inventory Service]:::consumer\n    OrderConfirmed --> ShippingService[Shipping Service]:::consumer\n    \n    InventoryService --> InventoryAdjusted[InventoryAdjusted Event]:::event\n    \n    subgraph InventoryAdjustedSchema[ ]\n        direction TB\n        InventoryAdjustedMessage[\"InventoryAdjusted Schema\n        {\n          productId: string,\n          warehouseId: string,\n          quantityChanged: number,\n          newQuantity: number,\n          timestamp: datetime\n        }\"]\n    end\n    \n    InventoryAdjusted --- InventoryAdjustedSchema\n    \n    InventoryAdjusted --> OrdersService:::consumer\n    InventoryAdjusted --> NotificationService[Notification Service]:::consumer\n```\n\n## Conclusion\n\nThis document has provided an in-depth view of the data flows within the FlowMart e-commerce platform. By understanding these flows, developers and stakeholders can better comprehend how data moves through the system and how different components interact with each other.\n\nFor more details on specific architecture components, refer to:\n- [High-Level System Overview](./01-high-level-system-overview.mdx)\n- [Domain-Level Architecture](./02-domain-level-architecture.mdx)\n- [Service-Level Architecture](./03-service-level-architecture.mdx)",
  "../examples/default/docs/technical-architecture-design/system-architecture-diagrams/04-data-flow-architecture.mdx",
  "3fb2081e597ccd4d",
  "docs/technical-architecture-design/architecture-decision-records/drafts/01-cloud-infrastructure-strategy",
  {
    "id": 708,
    "data": 710,
    "body": 714,
    "filePath": 715,
    "digest": 716,
    "deferredRender": 20
  },
  { "title": 711, "summary": 712, "sidebar": 713 },
  "Cloud Infrastructure Strategy",
  "Architectural decision record for cloud infrastructure approach for the FlowMart e-commerce platform",
  { "label": 711, "order": 470 },
  "# DRAFT - NOT YET APPROVED\n\n:::warning\nThis is a draft ADR. It is not yet approved and should not be used as a reference.\n:::\n\n## ADR-007: Cloud Infrastructure Strategy for FlowMart E-commerce Platform\n\n### Status\n\nDraft (Last Updated: 2024-09-25)\n\n\u003CFlow id=\"PaymentFlow\" version=\"latest\" includeKey={false} />\n\u003CFlow id=\"PaymentFlow\" version=\"latest\" includeKey={false} />\n\u003CFlow id=\"PaymentFlow\" version=\"latest\" includeKey={false} />\n\n### Context\n\nAs part of our transition to a microservices architecture, we need to define our cloud infrastructure strategy to support the new FlowMart e-commerce platform. Our current infrastructure consists primarily of on-premises data centers with some workloads in AWS, creating operational and scaling challenges:\n\n1. **Infrastructure Provisioning**: Manual processes for provisioning infrastructure lead to long lead times for new environments and services.\n\n2. **Scaling Limitations**: Physical hardware constraints prevent rapid scaling during peak shopping periods.\n\n3. **High Operational Overhead**: Significant effort required for hardware maintenance, patching, and capacity management.\n\n4. **Disaster Recovery Challenges**: Limited geographic redundancy and complex DR procedures.\n\n5. **Cloud Fragmentation**: Ad-hoc adoption of cloud services has created inconsistent practices and tooling.\n\n6. **Developer Experience**: Complex local development setup and environment inconsistencies slow down development cycles.\n\n7. **Cost Management**: Difficult to attribute infrastructure costs to specific business capabilities or teams.\n\n8. **Security Compliance**: Maintaining compliance across hybrid infrastructure requires duplicate controls and auditing.\n\nWe need a cohesive infrastructure strategy that enables rapid delivery, scalability, and operational efficiency for our new e-commerce platform.\n\n### Decision\n\nWe will adopt a **cloud-native infrastructure strategy** with a **multi-cloud capability** but **AWS-primary approach**. Key aspects of this strategy include:\n\n1. **Primary Cloud Platform**:\n   - AWS will be our primary cloud provider for all new workloads\n   - Azure will be maintained as a secondary provider for specific use cases (e.g., Microsoft-ecosystem services)\n   - GCP may be selectively used for specialized AI/ML workloads\n\n2. **Infrastructure as Code (IaC)**:\n   - Terraform as primary IaC tool for provisioning cloud resources\n   - AWS CDK for complex, AWS-specific resource configurations\n   - GitOps-based deployment workflows with infrastructure CI/CD pipelines\n\n3. **Containerization Strategy**:\n   - Containerize all new microservices using Docker\n   - Amazon EKS (Kubernetes) as primary container orchestration platform\n   - Amazon ECR for container registry with cross-region replication\n\n4. **Platform Services Over Custom Infrastructure**:\n   - Prefer managed services over self-managed infrastructure where possible\n   - AWS RDS, DocumentDB, ElastiCache for database needs\n   - AWS MSK (Managed Kafka) for event streaming\n   - CloudFront for CDN and edge caching\n\n5. **Multi-Region Architecture**:\n   - Primary operations in AWS US-East-1 and US-West-2\n   - Active-active configuration for critical services\n   - Region-specific deployments for EU and APAC markets to address data residency\n\n6. **Landing Zone and Account Structure**:\n   - Hub-and-spoke model with centralized security and governance\n   - Separate AWS accounts for production, staging, development, and sandbox environments\n   - Service-oriented account structure within each environment category\n   - Centralized logging, monitoring, and security controls\n\n7. **Network Architecture**:\n   - Transit Gateway for interconnecting VPCs and on-premises\n   - VPC design aligned with microservice domains\n   - AWS PrivateLink for service-to-service connectivity\n   - AWS Shield and WAF for DDoS protection and application security\n\n8. **Cost Optimization**:\n   - Tagging strategy for cost allocation and tracking\n   - Automated cost monitoring and anomaly detection\n   - Leverage Savings Plans and Reserved Instances strategically\n   - Implemented auto-scaling for elastic workloads\n\n### Infrastructure Architecture by Domain\n\n| Domain | Primary Services | Scaling Strategy | Special Requirements |\n|--------|------------------|------------------|----------------------|\n| Product Catalog | EKS, ElastiCache, DocumentDB | Horizontal pod scaling, Read replicas | High read throughput, Global availability |\n| Order Processing | EKS, RDS (PostgreSQL), MSK | Horizontal pod scaling, DB connection pooling | Strong consistency, Transaction support |\n| Payment | EKS, RDS (PostgreSQL), KMS | Fixed scaling with headroom | PCI-DSS compliance, Encryption requirements |\n| Inventory | EKS, DocumentDB, Lambda | Horizontal auto-scaling | Event-sourcing patterns, Eventual consistency |\n| User Authentication | Cognito, Lambda, DynamoDB | Regional deployments | Multi-factor auth, Token management |\n| Content Delivery | S3, CloudFront, Lambda@Edge | Edge caching, Regional replication | Image optimization, Low latency delivery |\n| Search | OpenSearch, Lambda, EKS | Search domain scaling, Query throttling | High cardinality, Complex query support |\n| Analytics | Redshift, Kinesis, EMR | Workload-based scaling | Batch processing, Data lake integration |\n\n### Consequences\n\n#### Positive\n\n1. **Improved Scalability**: Elastic infrastructure that scales with demand without manual intervention.\n\n2. **Faster Time-to-Market**: Automated provisioning and deployment enable rapid delivery of new features.\n\n3. **Enhanced Resilience**: Multi-region architecture improves availability and disaster recovery capabilities.\n\n4. **Cost Efficiency**: Pay-for-use model and automated scaling optimize infrastructure costs.\n\n5. **Developer Productivity**: Consistent environments and self-service capabilities improve developer experience.\n\n6. **Security Improvements**: Standardized security controls and automated compliance checks.\n\n7. **Operational Efficiency**: Reduced operational overhead through managed services and automation.\n\n8. **Global Reach**: Ability to deploy services closer to customers in different geographic regions.\n\n#### Negative\n\n1. **Cloud Vendor Dependency**: While designed for multi-cloud, primary workloads will have AWS dependencies.\n\n2. **Cost Management Complexity**: Cloud costs can escalate without proper governance and monitoring.\n\n3. **Skill Set Transition**: Team needs to develop new skills in cloud-native technologies and practices.\n\n4. **Increased Architectural Complexity**: Distributed cloud architecture is more complex to design and troubleshoot.\n\n5. **Security Model Changes**: Cloud security requires different approaches and tools than on-premises.\n\n6. **Data Transfer Costs**: Cross-region and internet data transfer can become a significant cost factor.\n\n7. **Service Maturity Variations**: Some AWS services are more mature and reliable than others.\n\n### Mitigation Strategies\n\n1. **Cloud Platform Team**:\n   - Create a dedicated platform engineering team for cloud infrastructure\n   - Develop reusable infrastructure modules and patterns\n   - Provide internal consultation and support to service teams\n\n2. **Cloud Center of Excellence (CCoE)**:\n   - Establish cloud best practices and governance\n   - Regular architecture reviews and guidance\n   - Develop certification and training program for engineering teams\n\n3. **Cloud Financial Management**:\n   - Implement FinOps practices and tooling\n   - Regular cost reviews and optimization cycles\n   - Showback/chargeback mechanisms to drive accountability\n\n4. **Abstraction Layers**:\n   - Create service abstractions to minimize direct cloud provider coupling\n   - Develop infrastructure interfaces that could support multiple providers\n   - Use cloud-agnostic tools and practices where practical\n\n5. **Hybrid Transition Strategy**:\n   - Phased migration from on-premises to cloud\n   - Maintain hybrid capabilities during transition period\n   - Clear exit criteria for legacy infrastructure decommissioning\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q4 2024)\n\n1. Establish AWS Landing Zone and account structure\n2. Implement core networking and security controls\n3. Set up CI/CD pipelines for infrastructure\n4. Deploy EKS clusters for development and staging\n5. Migrate first non-critical workloads\n\n#### Phase 2: Production Migration (Q1-Q2 2025)\n\n1. Deploy production EKS clusters and platform services\n2. Migrate core services to cloud infrastructure\n3. Implement multi-region capabilities for critical services\n4. Establish cloud cost management and optimization\n5. Complete developer tooling and self-service capabilities\n\n#### Phase 3: Advanced Capabilities (Q3-Q4 2025)\n\n1. Implement advanced security and compliance controls\n2. Deploy cross-region data synchronization\n3. Optimize for global performance and availability\n4. Enhance disaster recovery capabilities\n5. Begin decommissioning legacy infrastructure\n\n### Considered Alternatives\n\n#### 1. Maintain and Expand On-Premises Infrastructure\n\n**Pros**: Full control, potentially lower ongoing costs for stable workloads, no data sovereignty concerns  \n**Cons**: Limited agility, high capital expenditure, scaling limitations, operational overhead\n\nThis approach would not provide the agility and scalability needed for our strategic growth and would perpetuate our current limitations.\n\n#### 2. Single Cloud Provider (AWS Only)\n\n**Pros**: Simplified operations, deeper integration between services, volume discounts, focused expertise  \n**Cons**: Vendor lock-in, limited negotiating leverage, regional provider limitations\n\nWhile simpler operationally, this approach would increase our dependency on a single provider and limit flexibility.\n\n#### 3. Multi-Cloud with Equal Workload Distribution\n\n**Pros**: Maximize negotiating leverage, no single provider dependency, best-of-breed services  \n**Cons**: Significantly increased complexity, higher operations costs, fragmented expertise, integration challenges\n\nThe operational complexity and cost of maintaining equal capabilities across multiple cloud providers outweighs the benefits for our current needs.\n\n#### 4. Cloud Service Provider (CSP) Abstraction Layer\n\n**Pros**: Provider independence, standardized interfaces, easier migration between providers  \n**Cons**: Significant development overhead, lowest common denominator functionality, performance impacts\n\nBuilding comprehensive abstractions across cloud providers would create substantial engineering overhead and limit access to valuable provider-specific capabilities.\n\n### References\n\n1. AWS Well-Architected Framework ([AWS Documentation](https://aws.amazon.com/architecture/well-architected/))\n2. \"Cloud Strategy Leadership\" (Gartner)\n3. \"Architecting for the Cloud: AWS Best Practices\" ([AWS Whitepaper](https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf))\n4. \"Multi-cloud: The good, the bad and the ugly\" (ThoughtWorks Technology Radar)\n5. Terraform Documentation ([Terraform.io](https://www.terraform.io/docs/))\n6. \"Cloud Native Infrastructure\" by Justin Garrison and Kris Nova\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-09-10 | 0.1 | Initial draft | Marcus Johnson |\n| 2024-09-18 | 0.2 | Added implementation phases and domain details | Patricia Lopez |\n| 2024-09-25 | 0.3 | Incorporated feedback from architecture review | David Boyne |\n| TBD | 1.0 | Pending approval | Architecture Board |\n\n## Appendix A: Cloud Infrastructure Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"AWS Global\"\n        subgraph \"AWS Organizations\"\n            org[Management Account]\n            security[Security Account]\n            shared[Shared Services Account]\n            log[Logging Account]\n            net[Network Account]\n            \n            subgraph \"Production\"\n                prod1[Product Domain]\n                prod2[Order Domain]\n                prod3[Customer Domain]\n                prod4[Payment Domain]\n            end\n            \n            subgraph \"Non-Production\"\n                dev[Development]\n                stage[Staging]\n                test[Testing]\n                sandbox[Sandbox]\n            end\n        end\n    end\n    \n    subgraph \"On-Premises\"\n        dc[Data Center]\n        legacy[Legacy Systems]\n    end\n    \n    subgraph \"Network Fabric\"\n        dx[Direct Connect]\n        tgw[Transit Gateway]\n        vpn[VPN]\n        cf[CloudFront]\n    end\n    \n    org --> security\n    org --> shared\n    org --> log\n    org --> net\n    org --> prod1\n    org --> prod2\n    org --> prod3\n    org --> prod4\n    org --> dev\n    org --> stage\n    org --> test\n    org --> sandbox\n    \n    dc --> dx\n    dc --> vpn\n    legacy --> dx\n    \n    dx --> tgw\n    vpn --> tgw\n    \n    tgw --> net\n    net --> prod1\n    net --> prod2\n    net --> prod3\n    net --> prod4\n    net --> dev\n    net --> stage\n    \n    cf --> prod1\n    cf --> prod2\n    cf --> prod3\n    cf --> prod4\n```\n\n## Appendix B: Deployment Architecture\n\n```mermaid\nflowchart LR\n    subgraph \"Developer Experience\"\n        ide[IDE Plugins]\n        cli[CLI Tools]\n        local[Local Dev Environment]\n    end\n    \n    subgraph \"CI/CD Pipeline\"\n        git[Git Repository]\n        build[Build System]\n        test[Automated Tests]\n        scan[Security Scans]\n        artifact[Artifact Repository]\n    end\n    \n    subgraph \"GitOps Deployment\"\n        config[Config Repository]\n        argocd[ArgoCD]\n        flux[Flux]\n    end\n    \n    subgraph \"AWS EKS Clusters\"\n        subgraph \"Development\"\n            devNS1[Team 1 Namespace]\n            devNS2[Team 2 Namespace]\n        end\n        \n        subgraph \"Staging\"\n            stageNS1[Team 1 Namespace]\n            stageNS2[Team 2 Namespace]\n        end\n        \n        subgraph \"Production\"\n            prodNS1[Team 1 Namespace]\n            prodNS2[Team 2 Namespace]\n        end\n    end\n    \n    ide --> git\n    cli --> git\n    local --> git\n    \n    git --> build\n    build --> test\n    test --> scan\n    scan --> artifact\n    \n    artifact --> config\n    config --> argocd\n    config --> flux\n    \n    argocd --> devNS1\n    argocd --> devNS2\n    argocd --> stageNS1\n    argocd --> stageNS2\n    argocd --> prodNS1\n    argocd --> prodNS2\n    \n    flux --> devNS1\n    flux --> devNS2\n    flux --> stageNS1\n    flux --> stageNS2\n    flux --> prodNS1\n    flux --> prodNS2\n```\n\n## Appendix C: AWS Account Structure\n\n```mermaid\nflowchart TB\n    subgraph \"Management & Governance\"\n        mgmt[Management Account]\n        style mgmt fill:#f9f,stroke:#333,stroke-width:2px\n        \n        security[Security Account]\n        audit[Audit Account]\n        shared[Shared Services]\n        network[Network Account]\n        \n        mgmt --> security\n        mgmt --> audit\n        mgmt --> shared\n        mgmt --> network\n    end\n    \n    subgraph \"Production OU\"\n        prodOU[Production]\n        \n        payment[Payment Services]\n        order[Order Services]\n        product[Product Catalog]\n        identity[Identity Services]\n        content[Content Services]\n        analytics[Analytics]\n        \n        prodOU --> payment\n        prodOU --> order\n        prodOU --> product\n        prodOU --> identity\n        prodOU --> content\n        prodOU --> analytics\n    end\n    \n    subgraph \"Non-Production OU\"\n        nonProdOU[Non-Production]\n        \n        dev[Development]\n        stage[Staging]\n        test[Testing]\n        sandbox[Sandbox]\n        \n        nonProdOU --> dev\n        nonProdOU --> stage\n        nonProdOU --> test\n        nonProdOU --> sandbox\n    end\n    \n    mgmt --> prodOU\n    mgmt --> nonProdOU\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/drafts/01-cloud-infrastructure-strategy.mdx",
  "95941061539062c7",
  "docs/technical-architecture-design/architecture-decision-records/drafts/02-cicd-deployment-strategy",
  {
    "id": 717,
    "data": 719,
    "body": 724,
    "filePath": 725,
    "digest": 726,
    "deferredRender": 20
  },
  { "title": 720, "summary": 721, "sidebar": 722 },
  "CI/CD and Deployment Strategy",
  "Architectural decision record for continuous integration, delivery and deployment approach for the FlowMart e-commerce platform",
  { "label": 723, "order": 487 },
  "CI/CD Strategy",
  "# DRAFT - NOT YET APPROVED\n\n:::warning\nThis is a draft ADR. It is not yet approved and should not be used as a reference.\n:::\n\n## ADR-008: CI/CD and Deployment Strategy for FlowMart E-commerce Platform\n\n### Status\n\nDraft (Last Updated: 2024-10-05)\n\n### Context\n\nAs we transition to a microservices architecture with dozens of independently deployable services, our current deployment approach presents several challenges:\n\n1. **Manual Deployment Processes**: Deployments are largely manual, requiring significant coordination and causing deployment anxiety.\n\n2. **Environment Inconsistency**: Configuration differences between environments lead to environment-specific bugs and \"works on my machine\" issues.\n\n3. **Long Lead Times**: The process from code commit to production deployment takes days or weeks due to manual testing and approval gates.\n\n4. **Deployment Coupling**: Services must be deployed together in coordinated releases, slowing down the delivery of all features.\n\n5. **Limited Testing Automation**: Insufficient automated testing leads to quality issues discovered late in the delivery process.\n\n6. **Configuration Management**: Configuration is managed inconsistently across environments and services.\n\n7. **Deployment Visibility**: Limited visibility into deployment status, history, and metrics.\n\n8. **Rollback Challenges**: Rolling back problematic deployments is difficult and error-prone.\n\nOur current approach does not support the rapid, independent delivery of microservices that is essential for our new architecture. We need a comprehensive CI/CD and deployment strategy that enables teams to deliver high-quality services with velocity and confidence.\n\n### Decision\n\nWe will implement a **GitOps-based CI/CD and deployment strategy** with **continuous deployment to production** for our microservices architecture. Key aspects of this strategy include:\n\n1. **Trunk-Based Development Model**:\n   - Short-lived feature branches merged frequently to main/trunk\n   - Feature toggles for in-progress work\n   - Automated code quality checks and linting on pull requests\n   - Main branch always deployable\n\n2. **Continuous Integration Pipeline**:\n   - Automated builds triggered on every commit\n   - Comprehensive automated testing suite\n   - Security scanning (SAST, SCA, secrets scanning)\n   - Container image building and signing\n   - Test environments provisioned on demand for PR validation\n\n3. **GitOps Deployment Approach**:\n   - Declarative infrastructure and application configuration in Git\n   - ArgoCD as primary GitOps operator\n   - Environment-specific configuration via Kustomize overlays\n   - Git as single source of truth for deployed state\n   - Automatic drift detection and remediation\n\n4. **Deployment Progression Strategy**:\n   - Automated deployments through dev and test environments\n   - Production deployments with optional approval (human in the loop)\n   - Environment promotion rather than rebuilding artifacts\n   - Canary deployments for high-risk services\n   - Blue/green deployments for critical components\n\n5. **Configuration Management**:\n   - Externalized configuration in Git repositories\n   - Kubernetes ConfigMaps and Secrets for application configuration\n   - Sealed Secrets for sensitive information\n   - HashiCorp Vault for secrets rotation and dynamic credentials\n   - Environment-specific configuration via layered overlays\n\n6. **Deployment Safety Mechanisms**:\n   - Progressive delivery with canary deployments\n   - Automated pre-deployment validation\n   - Automated post-deployment testing\n   - Automated rollback on failure\n   - Circuit breakers for dependent services\n\n7. **Release Coordination**:\n   - API versioning and backwards compatibility requirements\n   - Service-level dependency management\n   - Deployment sequencing for interdependent services\n   - Deployment windows for critical services\n\n8. **Deployment Metrics and Observability**:\n   - Deployment frequency tracking\n   - Change lead time measurement\n   - Mean time to recovery monitoring\n   - Change failure rate tracking\n   - Deployment health dashboards\n\n### Technology Stack\n\n| Component | Primary Technology | Alternative/Backup | Purpose |\n|-----------|-------------------|-------------------|---------|\n| Source Control | GitHub | GitLab | Version control and collaboration |\n| CI Pipeline | GitHub Actions | Jenkins | Build, test, and validation |\n| Artifact Registry | AWS ECR | GitHub Packages | Container image storage |\n| GitOps Operator | ArgoCD | Flux | Kubernetes-based deployment automation |\n| Secrets Management | Sealed Secrets + Vault | AWS Secrets Manager | Secure configuration management |\n| Deployment Orchestration | ArgoCD + Argo Rollouts | Spinnaker | Controlled deployment progression |\n| Feature Flags | LaunchDarkly | Flagsmith | Runtime feature enablement/disablement |\n| Testing Framework | Jest, Cypress, k6 | Various | Automated testing across layers |\n| Deployment Monitoring | Prometheus + Grafana | Datadog | Deployment metrics and alerting |\n\n### Consequences\n\n#### Positive\n\n1. **Accelerated Delivery**: Reduced lead time from commit to production deployment.\n\n2. **Improved Quality**: Comprehensive automated testing and validation.\n\n3. **Increased Deployment Frequency**: Teams can deploy independently at their own pace.\n\n4. **Enhanced Reliability**: Consistent, repeatable deployment processes with automated rollbacks.\n\n5. **Better Visibility**: Clear audit trail and status of all deployments.\n\n6. **Reduced Coordination Overhead**: Less need for cross-team deployment coordination.\n\n7. **Improved Developer Experience**: Self-service deployments with rapid feedback.\n\n8. **Environment Consistency**: Reproducible environments with minimal drift.\n\n#### Negative\n\n1. **Learning Curve**: Teams need to adapt to new tools and processes.\n\n2. **Initial Setup Complexity**: Significant effort to establish the complete CI/CD pipeline.\n\n3. **Infrastructure Requirements**: Additional infrastructure to support the CI/CD toolchain.\n\n4. **Potential Deployment Sprawl**: Multiple services deploying independently can create coordination challenges.\n\n5. **Testing Complexity**: Comprehensive testing across distributed services is challenging.\n\n6. **Feature Flag Management**: Complexity of managing feature flags across services.\n\n7. **Observability Requirements**: Need for sophisticated monitoring to detect deployment issues.\n\n### Mitigation Strategies\n\n1. **Platform Team Support**:\n   - Create a dedicated platform engineering team focused on CI/CD\n   - Provide standardized pipeline templates and documentation\n   - Enable self-service capabilities with guardrails\n\n2. **Phased Implementation**:\n   - Start with less critical services\n   - Gradually increase automation and reduce manual gates\n   - Measure and demonstrate improved outcomes\n\n3. **Developer Enablement**:\n   - Comprehensive documentation and examples\n   - Regular training sessions and office hours\n   - Inner-source model for pipeline improvements\n\n4. **Testing Strategy**:\n   - Standard test libraries and frameworks\n   - Service virtualization for dependencies\n   - Comprehensive end-to-end testing strategy\n\n5. **Change Management**:\n   - Clear communication about process changes\n   - Regular retrospectives and continuous improvement\n   - Celebrate success stories and share lessons learned\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q4 2024)\n\n1. Establish CI pipeline standardization\n2. Implement container build and security scanning\n3. Set up artifact repositories and image signing\n4. Deploy ArgoCD and initial GitOps workflows\n5. Implement trunk-based development practices\n\n#### Phase 2: Advanced Delivery (Q1 2025)\n\n1. Enable canary and blue/green deployments\n2. Implement comprehensive automated testing\n3. Set up feature flag management\n4. Deploy secrets management solution\n5. Create deployment metrics dashboards\n\n#### Phase 3: Continuous Deployment (Q2 2025)\n\n1. Implement continuous deployment to production\n2. Enable automated rollbacks and circuit breakers\n3. Set up deployment SLOs and monitoring\n4. Implement sophisticated deployment strategies\n5. Optimize deployment performance and efficiency\n\n### Deployment Process Flow\n\nThe following outlines our target deployment process flow from code commit to production:\n\n1. **Code Commit & PR**:\n   - Developer creates branch and commits changes\n   - Pull request created with automated linting and checks\n   - CI pipeline validates build, tests, and security\n\n2. **CI Verification**:\n   - Automated unit and integration tests\n   - Security scanning (SAST, SCA, container scanning)\n   - Code quality metrics and coverage checks\n   - On-demand test environment provisioning\n\n3. **Artifact Creation**:\n   - Container images built and tagged\n   - Images signed and pushed to registry\n   - Deployment manifests generated\n   - Configuration updates prepared\n\n4. **Development Deployment**:\n   - Automatic deployment to development environment\n   - Post-deployment testing and validation\n   - Integration testing with other services\n   - Performance and security validation\n\n5. **Staging Deployment**:\n   - Promotion of verified artifacts to staging\n   - Environment-specific configuration applied\n   - System-level testing and validation\n   - Performance testing against production-like load\n\n6. **Production Deployment**:\n   - Optional approval gate for high-risk services\n   - Canary or blue/green deployment strategy\n   - Incremental traffic shifting\n   - Health check verification at each step\n\n7. **Post-Deployment Validation**:\n   - Automated smoke tests\n   - Synthetic transaction monitoring\n   - Key metric monitoring and alerting\n   - Automated rollback if metrics deviate\n\n### Considered Alternatives\n\n#### 1. Traditional Release-Based Deployment Model\n\n**Pros**: Familiar approach, coordinated releases, comprehensive testing cycles  \n**Cons**: Slow delivery, limited independence, large batch sizes increasing risk\n\nThis approach would not provide the delivery velocity required for our business needs and would limit the benefits of our microservices architecture.\n\n#### 2. Pure Environment Promotion Model\n\n**Pros**: Artifact consistency, simplified promotion process, reduced build time  \n**Cons**: Limited environment-specific customization, potential configuration complexity\n\nWhile we adopt aspects of this approach, we need the flexibility of environment-specific configuration that a pure promotion model limits.\n\n#### 3. Central Deployment Team\n\n**Pros**: Standardized processes, specialized expertise, controlled deployments  \n**Cons**: Potential bottleneck, reduced team autonomy, slower feedback loops\n\nThis approach would create a deployment bottleneck and reduce the ownership and autonomy of our product teams.\n\n#### 4. Fully Automated No-Approval Deployments\n\n**Pros**: Maximum velocity, reduced human intervention, forced quality automation  \n**Cons**: Increased risk for critical systems, cultural resistance, advanced testing requirements\n\nWhile this is our long-term goal, we need to balance velocity with appropriate controls, especially for critical payment and order processing systems.\n\n### References\n\n1. Forsgren, N., Humble, J., & Kim, G. \"Accelerate: The Science of Lean Software and DevOps\" (IT Revolution Press)\n2. Humble, J. & Farley, D. \"Continuous Delivery\" (Addison-Wesley)\n3. [GitOps Working Group](https://github.com/gitops-working-group/gitops-working-group)\n4. [Argo CD Documentation](https://argo-cd.readthedocs.io/)\n5. [Kubernetes Deployment Strategies](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy)\n6. [Trunk Based Development](https://trunkbaseddevelopment.com/)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-09-28 | 0.1 | Initial draft | Jason Miller |\n| 2024-10-03 | 0.2 | Added implementation phases and deployment flow | Thomas Wong |\n| 2024-10-05 | 0.3 | Incorporated feedback from DevOps and development teams | David Boyne |\n| TBD | 1.0 | Pending approval | Architecture Board |\n\n## Appendix A: CI/CD Pipeline Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Developer Workflow\"\n        dev[Developer]\n        ide[IDE]\n        local[Local Testing]\n        \n        dev --> ide\n        ide --> local\n    end\n    \n    subgraph \"Source Control\"\n        branch[Feature Branch]\n        pr[Pull Request]\n        trunk[Main Branch]\n        \n        local --> branch\n        branch --> pr\n        pr --> trunk\n    end\n    \n    subgraph \"Continuous Integration\"\n        build[Build & Test]\n        security[Security Scan]\n        quality[Quality Gates]\n        artifact[Create Artifacts]\n        \n        trunk --> build\n        build --> security\n        security --> quality\n        quality --> artifact\n    end\n    \n    subgraph \"Artifact Management\"\n        registry[Container Registry]\n        config[Config Repository]\n        \n        artifact --> registry\n        artifact --> config\n    end\n    \n    subgraph \"GitOps Deployment\"\n        argo[ArgoCD]\n        dev_env[Development]\n        stage_env[Staging]\n        prod_env[Production]\n        \n        config --> argo\n        argo --> dev_env\n        argo --> stage_env\n        argo --> prod_env\n    end\n    \n    subgraph \"Deployment Strategies\"\n        canary[Canary]\n        blue_green[Blue/Green]\n        \n        prod_env --> canary\n        prod_env --> blue_green\n    end\n    \n    subgraph \"Observability\"\n        metrics[Deployment Metrics]\n        alerts[Deployment Alerts]\n        dash[Deployment Dashboard]\n        \n        canary --> metrics\n        blue_green --> metrics\n        metrics --> alerts\n        metrics --> dash\n    end\n```\n\n## Appendix B: Deployment Pipeline Flow\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant Git as GitHub\n    participant CI as CI Pipeline\n    participant Reg as Container Registry\n    participant Config as Config Repository\n    participant ArgoCD as ArgoCD\n    participant Env as Environments\n    participant Monitor as Monitoring\n    \n    Dev->>Git: Commit Code\n    Git->>CI: Trigger Pipeline\n    \n    CI->>CI: Build & Test\n    CI->>CI: Security Scan\n    CI->>CI: Quality Gates\n    \n    CI->>Reg: Push Image\n    CI->>Config: Update Manifests\n    \n    Config->>ArgoCD: Detected Changes\n    ArgoCD->>Env: Deploy to Dev\n    \n    ArgoCD->>Env: Post-Deployment Tests\n    Env->>Monitor: Collect Metrics\n    \n    alt Tests Pass\n        ArgoCD->>Env: Deploy to Staging\n        Env->>Monitor: Collect Metrics\n        \n        alt Staging Healthy\n            ArgoCD->>Env: Deploy Canary to Production\n            Env->>Monitor: Collect Metrics\n            \n            alt Canary Healthy\n                ArgoCD->>Env: Complete Production Rollout\n            else Canary Unhealthy\n                ArgoCD->>Env: Rollback Canary\n            end\n        else Staging Unhealthy\n            ArgoCD->>Env: Rollback Staging\n        end\n    else Tests Fail\n        ArgoCD->>Env: Rollback Dev\n    end\n    \n    Monitor->>Dev: Deployment Status & Metrics\n```\n\n## Appendix C: Environment Configuration Strategy\n\n```mermaid\nflowchart LR\n    subgraph \"Git Repositories\"\n        app[Application Code]\n        infra[Infrastructure Code]\n        config[Configuration]\n    end\n    \n    subgraph \"Configuration Layers\"\n        base[Base Configuration]\n        env[Environment Overlays]\n        service[Service-Specific Config]\n    end\n    \n    subgraph \"Secret Management\"\n        sealed[Sealed Secrets]\n        vault[HashiCorp Vault]\n        esm[External Secrets Manager]\n    end\n    \n    subgraph \"Kubernetes Resources\"\n        cm[ConfigMaps]\n        secrets[Secrets]\n        pods[Deployments]\n    end\n    \n    app --> build{Build Process}\n    infra --> terraform{Terraform}\n    \n    config --> base\n    base --> kustomize{Kustomize}\n    env --> kustomize\n    service --> kustomize\n    \n    sealed --> kustomize\n    vault --> esm\n    esm --> secrets\n    \n    kustomize --> cm\n    kustomize --> secrets\n    \n    build --> pods\n    terraform --> platform[Platform Resources]\n    cm --> pods\n    secrets --> pods\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/drafts/02-cicd-deployment-strategy.mdx",
  "dc09a27a433fe42b",
  "docs/technical-architecture-design/architecture-decision-records/drafts/03-api-management-governance",
  {
    "id": 727,
    "data": 729,
    "body": 734,
    "filePath": 735,
    "digest": 736,
    "deferredRender": 20
  },
  { "title": 730, "summary": 731, "sidebar": 732 },
  "API Management and Governance",
  "Architectural decision record for our API management and governance approach for the FlowMart e-commerce platform",
  { "label": 733, "order": 515 },
  "API Management",
  "# DRAFT - NOT YET APPROVED\n\n## ADR-009: API Management and Governance Strategy\n\n### Status\n\nDraft (Last Updated: 2024-10-07)\n\n### Context\n\nAs we expand our microservices architecture for the FlowMart e-commerce platform, we need a comprehensive approach to API management and governance. We are facing several challenges with our current approach to APIs:\n\n1. **Proliferation of APIs**: With our transition to microservices, we anticipate having 50+ internal APIs and multiple external-facing APIs within the next year.\n\n2. **Inconsistent Design Patterns**: Teams are implementing APIs with inconsistent patterns, naming conventions, error handling, and response formats.\n\n3. **Documentation Gaps**: API documentation is inconsistent, often outdated, and frequently exists only in code comments or team wikis.\n\n4. **Discoverability Issues**: Developers struggle to find existing APIs, leading to duplication of functionality.\n\n5. **Security Concerns**: APIs lack consistent security controls, authentication mechanisms, and authorization models.\n\n6. **Version Management**: No clear strategy for versioning APIs, handling breaking changes, or maintaining backward compatibility.\n\n7. **Performance Visibility**: Limited visibility into API performance, usage patterns, and availability.\n\n8. **Cross-Cutting Concerns**: Features like rate limiting, caching, and observability are implemented inconsistently across services.\n\n9. **Developer Experience**: Onboarding to consume APIs is complex and time-consuming, both for internal teams and external partners.\n\nFlowMart is transitioning from a monolithic architecture to microservices, with a growing ecosystem of APIs. We need a cohesive approach to ensure these APIs are secure, discoverable, consistent, and manageable at scale.\n\n### Decision\n\nWe will implement a comprehensive **API Management and Governance Strategy** with the following key components:\n\n1. **API Gateway Architecture**:\n   - Implement Kong as our primary API gateway for both internal and external APIs\n   - Deploy separate gateway instances for external, internal, and partner traffic\n   - Utilize a mesh pattern for internal service-to-service communication\n   - Implement a Backend for Frontend (BFF) pattern for frontend-specific aggregation\n\n2. **API Design Standards**:\n   - Adopt OpenAPI (formerly Swagger) as our API specification standard\n   - Establish comprehensive REST API design guidelines\n   - Implement a design-first approach for all new APIs\n   - Define standard patterns for resource naming, query parameters, pagination, filtering, and error responses\n   - Standardize on JSON:API specification for response formatting\n\n3. **API Lifecycle Management**:\n   - Define clear stages: Design → Review → Development → Testing → Deployment → Deprecation → Retirement\n   - Implement automated API contract testing in CI/CD pipelines\n   - Require specification update for any API changes\n   - Establish deprecation and sunsetting policies with clear timelines\n\n4. **API Governance Model**:\n   - Create an API Guild with representatives from each team\n   - Establish an API review process for all new APIs and significant changes\n   - Implement automated linting and compliance checking against standards\n   - Define metrics for API quality and compliance\n   - Regular review of API portfolio for duplication and consolidation opportunities\n\n5. **API Documentation and Discovery**:\n   - Deploy a central API developer portal using Backstage\n   - Ensure all APIs have OpenAPI specifications\n   - Implement automated documentation generation from OpenAPI specs\n   - Create a searchable API catalog with metadata, ownership, and usage examples\n   - Develop interactive API playground environments\n\n6. **API Security Framework**:\n   - Standardize on OAuth 2.0 and OpenID Connect for authentication\n   - Implement role-based and attribute-based access control\n   - Deploy centralized API key management\n   - Establish security scanning and penetration testing for APIs\n   - Implement API request validation based on schemas\n\n7. **API Monitoring and Analytics**:\n   - Deploy comprehensive API metrics collection\n   - Create dashboards for performance, availability, and usage\n   - Implement distributed tracing for end-to-end request flows\n   - Set up alerting on API SLOs and error rates\n   - Establish usage analytics for product and developer experience improvement\n\n8. **API Versioning Strategy**:\n   - Adopt semantic versioning (major.minor.patch) for all APIs\n   - Support at least one previous major version for backward compatibility\n   - Use URI versioning for major versions (/v1/, /v2/)\n   - Implement feature flags for progressive rollout of API changes\n   - Establish clear communication channels for API changes\n\n### Technology Stack\n\n| Component | Primary Technology | Alternative/Backup | Purpose |\n|-----------|-------------------|-------------------|---------|\n| API Gateway | Kong | Apigee, Amazon API Gateway | Traffic management, routing, policies |\n| API Specification | OpenAPI 3.0 | AsyncAPI (for event-driven) | API contract and documentation |\n| Developer Portal | Backstage | SwaggerHub, Readme.io | Discovery, documentation, onboarding |\n| Identity Provider | Auth0 | Keycloak, AWS Cognito | Authentication and authorization |\n| API Testing | Postman + Newman | SoapUI, Karate | Automated API testing |\n| API Monitoring | Datadog | New Relic, Prometheus | Observability and analytics |\n| Contract Testing | Pact | Spring Cloud Contract | Consumer-driven contract testing |\n| GraphQL Federation | Apollo Federation | Netflix DGS | GraphQL implementation |\n| API Design Tooling | Stoplight Studio | SwaggerHub, Insomnia | Design-first workflow |\n\n### Consequences\n\n#### Positive\n\n1. **Improved Developer Experience**: Consistent, well-documented APIs accelerate development.\n\n2. **Enhanced Security**: Standardized authentication and authorization patterns.\n\n3. **Better Visibility**: Comprehensive monitoring and analytics of API usage and performance.\n\n4. **Reduced Duplication**: Central catalog prevents redundant API implementations.\n\n5. **Faster Integration**: Partners and internal teams can onboard more quickly.\n\n6. **Higher Quality**: Standardized design patterns and automated testing improve quality.\n\n7. **Future-Proofing**: Versioning strategy ensures backward compatibility.\n\n8. **Operational Efficiency**: Centralized management of cross-cutting concerns.\n\n#### Negative\n\n1. **Initial Overhead**: Additional processes and governance may slow initial development.\n\n2. **Learning Curve**: Teams need to adapt to new standards and practices.\n\n3. **Migration Effort**: Existing APIs need to be refactored to meet new standards.\n\n4. **Tooling Investment**: Significant investment in API management infrastructure.\n\n5. **Governance Challenges**: Maintaining compliance across teams requires persistent effort.\n\n6. **Potential Bottlenecks**: API review processes could become a bottleneck if not well-designed.\n\n7. **Operational Complexity**: Additional infrastructure components to maintain.\n\n### Mitigation Strategies\n\n1. **Phased Implementation**:\n   - Start with high-priority, externally facing APIs\n   - Gradually implement standards for internal APIs\n   - Provide flexible transition periods for existing APIs\n\n2. **Developer Enablement**:\n   - Create comprehensive training materials and workshops\n   - Develop starter templates and code generators\n   - Provide API design consultation services\n   - Create self-service tools for standards compliance\n\n3. **Governance Optimization**:\n   - Implement automated compliance checking\n   - Create lightweight review processes for low-risk changes\n   - Empower teams with self-service validation tools\n   - Focus governance on enablement rather than control\n\n4. **Migration Support**:\n   - Develop clear migration guidelines and timelines\n   - Provide tooling to assist with API migrations\n   - Allow grandfathering of legacy APIs with clear deprecation plans\n   - Prioritize high-value, high-impact APIs for migration\n\n5. **Platform Team Support**:\n   - Create a dedicated API platform team\n   - Offer consulting services to teams implementing or consuming APIs\n   - Develop reusable components for common API patterns\n   - Continuously evolve standards based on feedback\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q4 2024)\n\n1. Establish API design standards and guidelines\n2. Deploy API gateway for external-facing services\n3. Implement initial developer portal\n4. Create API governance process and guild\n5. Define API security standards\n\n#### Phase 2: Expansion (Q1 2025)\n\n1. Extend API gateway to internal services\n2. Implement comprehensive monitoring and analytics\n3. Deploy automated compliance checking\n4. Develop API versioning framework\n5. Create self-service API documentation tooling\n\n#### Phase 3: Optimization (Q2 2025)\n\n1. Implement advanced BFF patterns\n2. Deploy GraphQL federation for complex client needs\n3. Establish API performance optimization framework\n4. Create advanced analytics and business insights\n5. Develop partner API developer experiences\n\n### API Design Principles\n\nOur API design will adhere to the following principles:\n\n1. **Resource-Oriented Design**: Model APIs around business resources rather than operations.\n\n2. **Consistency**: APIs should behave consistently across the platform.\n\n3. **Least Privilege**: APIs should request only the permissions they need.\n\n4. **Robustness**: APIs should handle error cases gracefully and provide clear error messages.\n\n5. **Forward Compatibility**: Design APIs to be extended without breaking existing clients.\n\n6. **Discoverability**: APIs should be self-documenting and easily discoverable.\n\n7. **Performance**: APIs should be designed with performance considerations in mind.\n\n8. **Security by Design**: Security should be considered at every stage of the API lifecycle.\n\n### API Governance Model\n\nOur API governance model follows these principles:\n\n1. **Federated Ownership**: Teams own their APIs but follow central standards.\n\n2. **Standards over Rules**: Prefer guidance and patterns over strict enforcement.\n\n3. **Automated Compliance**: Automate standards checking wherever possible.\n\n4. **Continuous Improvement**: Regularly review and refine standards based on feedback.\n\n5. **Value-Driven**: Focus governance efforts on high-value, high-risk APIs.\n\n6. **Developer Experience**: Prioritize developer experience in governance decisions.\n\n7. **Transparency**: Make governance processes and decisions transparent to all teams.\n\n### Considered Alternatives\n\n#### 1. Decentralized API Management\n\n**Pros**: Maximum team autonomy, reduced coordination overhead  \n**Cons**: Inconsistent developer experience, potential security gaps, duplicated effort\n\nThis approach would give teams complete freedom but would result in an inconsistent and potentially insecure API landscape that would be difficult to navigate and maintain.\n\n#### 2. Centralized API Development Team\n\n**Pros**: Maximum consistency, specialized expertise, controlled quality  \n**Cons**: Development bottleneck, reduced team ownership, slower delivery\n\nWhile this would ensure consistency, it would create bottlenecks and reduce the autonomy and ownership of our product teams, contradicting our microservices philosophy.\n\n#### 3. Multiple API Gateways by Domain\n\n**Pros**: Domain isolation, reduced blast radius, team autonomy  \n**Cons**: Management complexity, potential inconsistencies, higher operational overhead\n\nThis approach offers benefits for large organizations but introduces unnecessary complexity for our current scale. We may revisit this approach as we grow.\n\n#### 4. GraphQL-First Approach\n\n**Pros**: Flexible client queries, reduced over-fetching, schema registry  \n**Cons**: Learning curve, performance concerns for some use cases, security complexity\n\nGraphQL offers benefits for certain use cases, but we believe a REST-first approach with GraphQL for specific complex client needs provides a better balance for our organization.\n\n### References\n\n1. \"Web API Design: The Missing Link\" by API Academy\n2. \"REST API Design Rulebook\" by Mark Masse\n3. [Google API Design Guide](https://cloud.google.com/apis/design)\n4. [Microsoft REST API Guidelines](https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md)\n5. [JSON:API Specification](https://jsonapi.org/)\n6. [API Governance: What, Why, and How](https://swagger.io/resources/articles/api-governance-what-why-how/)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-09-30 | 0.1 | Initial draft | Sarah Johnson |\n| 2024-10-04 | 0.2 | Added governance model and phasing | Michael Chen |\n| 2024-10-07 | 0.3 | Incorporated feedback from API Guild | David Boyne |\n| TBD | 1.0 | Pending approval | Architecture Board |\n\n## Appendix A: API Governance Process\n\n```mermaid\nflowchart TB\n    subgraph \"API Lifecycle\"\n        design[Design Phase]\n        review[Review Phase]\n        develop[Development Phase]\n        test[Testing Phase]\n        deploy[Deployment Phase]\n        monitor[Monitoring Phase]\n        deprecate[Deprecation Phase]\n        \n        design --> review\n        review --> develop\n        develop --> test\n        test --> deploy\n        deploy --> monitor\n        monitor --> deprecate\n    end\n    \n    subgraph \"Governance Touchpoints\"\n        standards[Standards & Guidelines]\n        templates[API Templates]\n        linting[Automated Linting]\n        reviews[Peer Reviews]\n        testing[Contract Testing]\n        security[Security Scanning]\n        analytics[Usage Analytics]\n        \n        standards --> design\n        templates --> design\n        linting --> review\n        reviews --> review\n        testing --> test\n        security --> test\n        analytics --> monitor\n    end\n    \n    subgraph \"Key Roles\"\n        guild[API Guild]\n        owners[API Owners]\n        platform[Platform Team]\n        consumers[API Consumers]\n        \n        guild --> standards\n        owners --> design\n        platform --> templates\n        consumers --> analytics\n    end\n    \n    subgraph \"Artifacts\"\n        spec[OpenAPI Specification]\n        docs[Documentation]\n        portal[Developer Portal]\n        metrics[Metrics Dashboard]\n        \n        design --> spec\n        spec --> docs\n        docs --> portal\n        monitor --> metrics\n    end\n```\n\n## Appendix B: API Management Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Clients\"\n        web[Web Clients]\n        mobile[Mobile Clients]\n        partners[Partner Systems]\n        internal[Internal Services]\n    end\n    \n    subgraph \"API Gateways\"\n        public[Public API Gateway]\n        partner[Partner API Gateway]\n        internal_gw[Internal API Gateway]\n    end\n    \n    subgraph \"BFF Layer\"\n        web_bff[Web BFF]\n        mobile_bff[Mobile BFF]\n    end\n    \n    subgraph \"API Services\"\n        products[Product APIs]\n        orders[Order APIs]\n        customers[Customer APIs]\n        payments[Payment APIs]\n        inventory[Inventory APIs]\n    end\n    \n    subgraph \"Cross-Cutting Concerns\"\n        auth[Authentication]\n        rate[Rate Limiting]\n        cache[Caching]\n        logging[Logging]\n        analytics[Analytics]\n    end\n    \n    web --> web_bff\n    mobile --> mobile_bff\n    partners --> partner\n    internal --> internal_gw\n    \n    web_bff --> public\n    mobile_bff --> public\n    \n    public --> products\n    public --> orders\n    public --> customers\n    partner --> products\n    partner --> orders\n    internal_gw --> products\n    internal_gw --> orders\n    internal_gw --> customers\n    internal_gw --> payments\n    internal_gw --> inventory\n    \n    auth --> public\n    auth --> partner\n    auth --> internal_gw\n    rate --> public\n    rate --> partner\n    cache --> public\n    cache --> partner\n    cache --> internal_gw\n    logging --> public\n    logging --> partner\n    logging --> internal_gw\n    analytics --> public\n    analytics --> partner\n    analytics --> internal_gw\n```\n\n## Appendix C: API Developer Portal Architecture\n\n```mermaid\nflowchart LR\n    subgraph \"API Developer Portal\"\n        catalog[API Catalog]\n        docs[API Documentation]\n        playground[API Playground]\n        analytics[Usage Analytics]\n        sandbox[Sandbox Environment]\n        support[Developer Support]\n    end\n    \n    subgraph \"API Sources\"\n        openapi[OpenAPI Specifications]\n        asyncapi[AsyncAPI Specifications]\n        graphql[GraphQL Schemas]\n        code[Source Code]\n    end\n    \n    subgraph \"Developer Tools\"\n        cli[CLI Tools]\n        sdks[Client SDKs]\n        ci[CI/CD Integration]\n        codegen[Code Generators]\n    end\n    \n    subgraph \"Governance Tools\"\n        standards[Standards Library]\n        linting[Linting Tools]\n        testing[Testing Framework]\n        workflow[Approval Workflows]\n    end\n    \n    openapi --> catalog\n    asyncapi --> catalog\n    graphql --> catalog\n    code --> docs\n    \n    catalog --> playground\n    catalog --> analytics\n    catalog --> sdks\n    catalog --> codegen\n    \n    playground --> sandbox\n    sandbox --> support\n    \n    standards --> linting\n    linting --> ci\n    testing --> ci\n    workflow --> ci\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/drafts/03-api-management-governance.mdx",
  "a950931c10d6917d",
  "docs/technical-architecture-design/architecture-decision-records/examples/01-microservice-mesh-adoption",
  {
    "id": 737,
    "data": 739,
    "body": 743,
    "filePath": 744,
    "digest": 745,
    "deferredRender": 20
  },
  { "title": 740, "summary": 741, "sidebar": 742 },
  "Service Mesh Adoption",
  "Architectural decision record for adopting a service mesh for TechNova's cloud platform",
  { "label": 740, "order": 470 },
  "## ADR-2023-05: Service Mesh Adoption for TechNova Cloud Platform\n\n### Status\n\nApproved (2023-08-15)\n\n### Context\n\nTechNova's cloud platform has grown to include over 75 microservices developed and maintained by 12 different teams. As our platform has scaled, we've encountered several challenges:\n\n1. **Service-to-Service Communication Complexity**: Managing service-to-service communication has become increasingly complex, with inconsistent implementation of patterns like circuit breaking, retries, and timeout handling.\n\n2. **Security Concerns**: Service-to-service authentication and authorization is implemented inconsistently, creating potential security vulnerabilities.\n\n3. **Observability Gaps**: Tracking request flows across services is difficult, making troubleshooting and performance optimization challenging.\n\n4. **Traffic Management Challenges**: We lack sophisticated traffic control capabilities such as traffic splitting, canary deployments, and blue/green releases.\n\n5. **Certificate Management**: Managing and rotating TLS certificates for service-to-service communication has become an operational burden.\n\n6. **Network Policy Enforcement**: Network access controls are difficult to implement and maintain consistently across services.\n\nOur engineering teams currently spend approximately 25% of their development effort on these cross-cutting concerns, diverting resources from building business functionality.\n\n### Decision\n\nWe will adopt **Istio** as our service mesh solution for the TechNova Cloud Platform. Specifically:\n\n1. **Sidecar Deployment Model**: We will deploy Istio using the sidecar proxy architecture to minimize disruption to existing services.\n\n2. **Gradual Implementation**: We will introduce the service mesh incrementally, starting with non-critical services and gradually expanding to include all services.\n\n3. **Standardized Patterns**: We will establish company-wide standards for:\n   - Circuit breaking configurations\n   - Retry policies\n   - Timeout configurations\n   - Mutual TLS implementation\n   - Traffic management patterns\n   - Observability integrations\n\n4. **Platform Team Ownership**: A dedicated platform team will own the service mesh infrastructure, including:\n   - Istio version management and upgrades\n   - Performance monitoring and optimization\n   - Security policy definition and enforcement\n   - Training and documentation\n\n5. **Integration with Existing Tooling**:\n   - Prometheus and Grafana for metrics and dashboarding\n   - Jaeger for distributed tracing\n   - Kiali for service mesh visualization\n   - Cert-Manager for certificate management\n\n6. **Control Plane Configuration**: We will deploy Istio with:\n   - High availability control plane (3 replicas)\n   - Split along environment boundaries (separate control planes for production and non-production)\n   - Regular automated backups of control plane configuration\n\n### Consequences\n\n#### Positive\n\n1. **Simplified Service Code**: Application developers can focus on business logic rather than communication concerns.\n\n2. **Enhanced Security**: Consistent mTLS implementation will improve our security posture.\n\n3. **Improved Observability**: End-to-end request tracing and detailed service metrics will enhance troubleshooting.\n\n4. **Advanced Traffic Management**: Capabilities for canary deployments and traffic steering will reduce deployment risk.\n\n5. **Certificate Automation**: Automated certificate management will reduce operational overhead.\n\n6. **Standardized Networking Policies**: Centralized network policy enforcement will improve security and compliance.\n\n#### Negative\n\n1. **Increased Complexity**: Service mesh adds architectural complexity and new failure modes.\n\n2. **Resource Overhead**: Sidecar proxies consume additional CPU and memory resources (~10-15% based on POC testing).\n\n3. **Learning Curve**: Teams will need to learn new concepts, tooling, and debugging approaches.\n\n4. **Deployment Changes**: CI/CD pipelines will need updates to accommodate service mesh configuration.\n\n5. **Potential Performance Impact**: Additional network hops may increase latency (observed ~5-10ms per hop in testing).\n\n6. **Operational Changes**: Incident response and troubleshooting procedures will need updates.\n\n### Mitigation Strategies\n\n1. **Comprehensive Education Program**:\n   - Create internal training curriculum for all engineering teams\n   - Schedule hands-on workshops focusing on practical use cases\n   - Develop troubleshooting playbooks and runbooks\n\n2. **Gradual Rollout Plan**:\n   - Phase 1 (Q3 2023): Deploy to non-critical services and gather metrics\n   - Phase 2 (Q4 2023): Expand to backend services with low customer impact\n   - Phase 3 (Q1 2024): Roll out to critical customer-facing services\n\n3. **Resource Optimization**:\n   - Tune proxy resource requests based on actual usage patterns\n   - Implement horizontal pod autoscaling for proxies\n   - Monitor and optimize control plane resource utilization\n\n4. **Performance Monitoring**:\n   - Establish baseline performance metrics before service mesh adoption\n   - Implement detailed performance monitoring dashboards\n   - Set alerting on latency increases beyond acceptable thresholds\n\n5. **Escape Hatch Mechanisms**:\n   - Document procedures for temporarily bypassing the service mesh if issues arise\n   - Create fast rollback capabilities for service mesh configurations\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q3 2023)\n\n1. Deploy Istio control plane in non-production environments\n2. Implement mTLS for a subset of non-critical services\n3. Set up integration with observability stack\n4. Develop initial training materials and documentation\n5. Create standard Istio configuration templates\n\n#### Phase 2: Expansion (Q4 2023)\n\n1. Deploy production control plane\n2. Expand to backend services\n3. Implement traffic management capabilities\n4. Develop automated testing for service mesh configurations\n5. Integrate service mesh configurations into CI/CD pipelines\n\n#### Phase 3: Completion (Q1 2024)\n\n1. Roll out to remaining services\n2. Implement advanced security policies\n3. Complete comprehensive monitoring and alerting\n4. Finalize operational procedures and runbooks\n5. Document lessons learned and best practices\n\n### Considered Alternatives\n\n#### 1. Linkerd\n\n**Pros**: Lighter weight, simpler operations, lower resource overhead  \n**Cons**: Fewer features, smaller community, less integration with enterprise tools\n\nWhile Linkerd offers a more lightweight approach, we found that its limited feature set would not address all our requirements, particularly around sophisticated traffic management and extensibility.\n\n#### 2. AWS App Mesh\n\n**Pros**: Native AWS integration, managed control plane, simplified operations  \n**Cons**: AWS-specific, limited feature set compared to Istio, less community support\n\nAWS App Mesh would align well with our AWS infrastructure but lacks some advanced features we require and would limit our multi-cloud flexibility.\n\n#### 3. Custom Solution with Envoy\n\n**Pros**: Tailored to our exact needs, potentially lower overhead  \n**Cons**: High development and maintenance cost, lack of community support, reinventing the wheel\n\nBuilding our own solution would require significant engineering resources and ongoing maintenance, which would outweigh the benefits of customization.\n\n#### 4. No Service Mesh (Status Quo)\n\n**Pros**: No additional complexity or overhead, familiar patterns  \n**Cons**: Continued inconsistency across services, growing technical debt, security risks\n\nMaintaining the status quo would fail to address our current challenges and would further increase technical debt as our platform continues to grow.\n\n### References\n\n1. [Istio Documentation](https://istio.io/latest/docs/)\n2. \"The Service Mesh Era\" - Cloud Native Computing Foundation Whitepaper\n3. Internal POC Report: \"Service Mesh Performance and Compatibility Testing\" (TechNova Engineering, June 2023)\n4. [CNCF Service Mesh Comparison](https://servicemesh.es/)\n5. [Envoy Proxy Documentation](https://www.envoyproxy.io/docs/envoy/latest/)\n6. \"Production Istio\" - O'Reilly Media, 2022\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2023-05-10 | 0.1 | Initial draft | Alexander Chen, Principal Architect |\n| 2023-06-22 | 0.2 | Updated based on POC results | Sophia Rodriguez, Platform Engineering |\n| 2023-07-15 | 0.3 | Added phased implementation plan | Marcus Johnson, Engineering Director |\n| 2023-08-15 | 1.0 | Approved by Architecture Review Board | TechNova ARB |\n\n## Appendix A: Service Mesh Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Control Plane\"\n        istiod[Istiod]\n        config[Configuration API]\n        certs[Certificate Authority]\n    end\n    \n    subgraph \"Ingress/Egress\"\n        ingress[Ingress Gateway]\n        egress[Egress Gateway]\n    end\n    \n    subgraph \"Observability\"\n        prom[Prometheus]\n        grafana[Grafana]\n        jaeger[Jaeger]\n        kiali[Kiali]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Service A\"\n            svcA[Service A]\n            proxyA[Envoy Proxy]\n        end\n        \n        subgraph \"Service B\"\n            svcB[Service B]\n            proxyB[Envoy Proxy]\n        end\n        \n        subgraph \"Service C\"\n            svcC[Service C]\n            proxyC[Envoy Proxy]\n        end\n    end\n    \n    client[Client] --> ingress\n    ingress --> proxyA\n    proxyA --> svcA\n    proxyA --> proxyB\n    proxyB --> svcB\n    proxyB --> proxyC\n    proxyC --> svcC\n    proxyC --> egress\n    egress --> external[External Services]\n    \n    istiod --> proxyA\n    istiod --> proxyB\n    istiod --> proxyC\n    istiod --> ingress\n    istiod --> egress\n    \n    config --> istiod\n    certs --> istiod\n    \n    proxyA --> prom\n    proxyB --> prom\n    proxyC --> prom\n    prom --> grafana\n    proxyA --> jaeger\n    proxyB --> jaeger\n    proxyC --> jaeger\n    prom --> kiali\n    jaeger --> kiali\n    config --> kiali\n```\n\n## Appendix B: Service Mesh Implementation Timeline\n\n```mermaid\ngantt\n    title TechNova Service Mesh Implementation\n    dateFormat  YYYY-MM-DD\n    \n    section Planning\n    Initial Research           :done, 2023-03-01, 2023-04-15\n    Proof of Concept           :done, 2023-04-16, 2023-06-15\n    Architecture Decision      :done, 2023-06-16, 2023-08-15\n    \n    section Phase 1: Foundation\n    Control Plane Setup        :active, 2023-08-16, 2023-09-15\n    Observability Integration  :active, 2023-09-01, 2023-09-30\n    Initial Service Onboarding :2023-09-15, 2023-10-15\n    Training & Documentation   :2023-09-01, 2023-10-31\n    \n    section Phase 2: Expansion\n    Production Control Plane   :2023-11-01, 2023-11-30\n    Backend Service Onboarding :2023-11-15, 2024-01-15\n    CI/CD Integration          :2023-12-01, 2024-01-31\n    Advanced Traffic Management:2024-01-01, 2024-02-15\n    \n    section Phase 3: Completion\n    Remaining Service Onboarding :2024-02-01, 2024-03-31\n    Security Policy Implementation:2024-02-15, 2024-03-31\n    Finalize Operations Procedures:2024-03-01, 2024-04-15\n    Performance Optimization     :2024-03-15, 2024-04-30\n    Project Completion          :milestone, 2024-04-30\n```\n\n## Appendix C: Estimated Resource Requirements\n\n| Component | CPU Request | Memory Request | Replicas | Environment | Notes |\n|-----------|-------------|---------------|----------|-------------|-------|\n| Istiod | 500m | 2Gi | 3 | Production | HA configuration |\n| Istiod | 500m | 2Gi | 1 | Non-Production | Single replica |\n| Ingress Gateway | 500m | 1Gi | 3 | Production | External traffic entry |\n| Ingress Gateway | 200m | 512Mi | 1 | Non-Production | Development testing |\n| Egress Gateway | 500m | 1Gi | 2 | Production | External service access |\n| Envoy Proxy | 100m | 256Mi | 1 per pod | All | Sidecar containers |\n| Prometheus | 1000m | 8Gi | 2 | Production | Metric collection |\n| Grafana | 200m | 1Gi | 2 | Production | Dashboarding |\n| Jaeger | 500m | 4Gi | 2 | Production | Distributed tracing |\n| Kiali | 200m | 1Gi | 2 | Production | Mesh visualization |\n\n*Note: These values are initial estimates based on our proof of concept and may be adjusted based on actual usage patterns.*",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/examples/01-microservice-mesh-adoption.mdx",
  "9a9f317fbd3cee0c",
  "docs/technical-architecture-design/architecture-decision-records/examples/02-data-platform-strategy",
  {
    "id": 746,
    "data": 748,
    "body": 752,
    "filePath": 753,
    "digest": 754,
    "deferredRender": 20
  },
  { "title": 749, "summary": 750, "sidebar": 751 },
  "Data Platform Strategy",
  "Architectural decision record for adopting a modern data platform at FinSecure",
  { "label": 749, "order": 487 },
  "## ADR-2023-12: Modern Data Platform Strategy for FinSecure\n\n### Status\n\nApproved (2023-12-18)\n\n### Context\n\nFinSecure is experiencing significant challenges with our current data architecture:\n\n1. **Data Silos**: Customer, transaction, and risk data are siloed across multiple legacy systems with inconsistent data models.\n\n2. **Limited Analytics Capabilities**: Rigid data warehousing solutions limit our ability to perform advanced analytics and machine learning.\n\n3. **Scalability Constraints**: Current data processing infrastructure is struggling to handle increasing data volumes (now exceeding 5TB daily).\n\n4. **Compliance Complexity**: Meeting GDPR, CCPA, and financial regulatory requirements across fragmented data systems is increasingly difficult.\n\n5. **Slow Time-to-Insight**: Business teams wait 2-3 weeks for new analytics dashboards or data models to be developed.\n\n6. **Technical Debt**: Legacy ETL processes are complex, brittle, and expensive to maintain.\n\n7. **Limited Real-time Capabilities**: Current architecture is primarily batch-oriented with limited ability to process streaming data for fraud detection and real-time decisioning.\n\n8. **Data Quality Issues**: Inconsistent data quality across systems impacts business decisions and customer experience.\n\nThese challenges are limiting our ability to leverage data as a strategic asset and inhibiting our digital transformation initiatives aimed at enhancing customer experiences and operational efficiency.\n\n### Decision\n\nWe will implement a **modern, cloud-based data platform** with a **lakehouse architecture**. Key components include:\n\n1. **Data Lake Foundation**:\n   - Azure Data Lake Storage Gen2 as the foundation for our data lake\n   - Databricks Delta Lake for ACID transactions and data reliability\n   - Structured organization with bronze (raw), silver (refined), and gold (business) layers\n\n2. **Data Ingestion and Processing**:\n   - Azure Data Factory for orchestration and batch data movement\n   - Kafka and Azure Event Hubs for real-time data ingestion\n   - Databricks for large-scale data processing\n   - Stream processing with Spark Structured Streaming\n\n3. **Semantic Layer and Data Serving**:\n   - Databricks SQL Warehouses for analytics workloads\n   - Azure Synapse Analytics for enterprise data warehousing needs\n   - Power BI as primary business intelligence tool\n   - REST APIs for serving data to applications\n\n4. **Data Governance and Security**:\n   - Azure Purview for data catalog and lineage\n   - Column-level encryption for sensitive data\n   - Role-based access control aligned with data classification\n   - Automated data retention and purging based on policies\n\n5. **Machine Learning Platform**:\n   - MLflow for experiment tracking and model registry\n   - Databricks ML for model development and deployment\n   - Model monitoring and retraining pipelines\n   - Feature store for reusable feature engineering\n\n6. **DataOps and Automation**:\n   - Infrastructure as Code using Terraform\n   - CI/CD pipelines for data pipelines and transformations\n   - Automated testing for data quality and pipeline integrity\n   - Comprehensive monitoring and alerting\n\n### Platform Architecture by Domain\n\n| Domain | Data Types | Primary Tools | Access Patterns | Special Requirements |\n|--------|------------|--------------|-----------------|----------------------|\n| Customer 360 | Customer profiles, interactions, preferences | Delta Lake, Databricks SQL | Batch analytics, Real-time lookups | GDPR compliance, Entity resolution |\n| Transaction Processing | Payment transactions, transfers, statements | Kafka, Delta Lake, Azure Synapse | Real-time streaming, Batch reporting | PCI-DSS compliance, 7-year retention |\n| Risk Management | Credit scores, market data, exposure calculations | Databricks, Delta Lake, ML models | Batch processing, Model inference | Auditability, Model governance |\n| Fraud Detection | Transaction patterns, behavioral signals | Kafka, Spark Streaming, ML models | Real-time streaming, Low-latency scoring | Sub-second latency, High availability |\n| Regulatory Reporting | Aggregated financial data, compliance metrics | Azure Synapse, Power BI | Scheduled batch, Ad-hoc analysis | Immutability, Approval workflows |\n| Marketing Analytics | Campaign data, customer segments, attribution | Databricks, Delta Lake, Power BI | Interactive queries, ML-based segmentation | Identity resolution, Attribution models |\n\n### Consequences\n\n#### Positive\n\n1. **Unified Data Access**: Single platform for accessing enterprise data with consistent governance.\n\n2. **Enhanced Analytical Capabilities**: Support for advanced analytics, machine learning, and AI initiatives.\n\n3. **Improved Scalability**: Cloud-native architecture can scale to handle growing data volumes.\n\n4. **Reduced Time-to-Insight**: Self-service capabilities and streamlined data pipelines reduce time to deliver insights.\n\n5. **Better Data Governance**: Centralized data catalog, lineage tracking, and security controls.\n\n6. **Real-time Capabilities**: Support for both batch and real-time processing using the same platform.\n\n7. **Cost Optimization**: Pay-for-use cloud model with ability to scale resources as needed.\n\n8. **Regulatory Compliance**: Improved ability to implement and demonstrate regulatory compliance.\n\n#### Negative\n\n1. **Implementation Complexity**: Significant effort required to migrate from legacy systems.\n\n2. **Skills Gap**: New technologies require reskilling of existing teams.\n\n3. **Initial Cost Increase**: Short-term investment in new technology and parallel running of systems.\n\n4. **Data Migration Challenges**: Data quality and mapping issues during migration.\n\n5. **Operational Changes**: New operational procedures and support models needed.\n\n6. **Integration Complexity**: Connecting legacy systems to new platform requires careful planning.\n\n7. **Organization Change Management**: New workflows and responsibilities across business and technical teams.\n\n### Mitigation Strategies\n\n1. **Phased Implementation Approach**:\n   - Start with highest-value, least-critical data domains\n   - Implement foundational capabilities before complex use cases\n   - Run legacy and new systems in parallel during transition\n   - Create clear success criteria for each phase\n\n2. **Talent and Skill Development**:\n   - Develop comprehensive training program for existing staff\n   - Strategic hiring for key specialized roles\n   - Partner with platform vendors for enablement\n   - Create centers of excellence for key technologies\n\n3. **Modern Data Governance**:\n   - Establish data governance council with cross-functional representation\n   - Define clear data ownership and stewardship model\n   - Implement automated data quality monitoring\n   - Create comprehensive data classification framework\n\n4. **Financial Management**:\n   - Detailed cloud cost monitoring and optimization\n   - Business-aligned chargeback model\n   - Clear ROI tracking for data initiatives\n   - Regular cost optimization reviews\n\n5. **Change Management Program**:\n   - Executive sponsorship and visible leadership\n   - Regular communication and success stories\n   - Early involvement of business stakeholders\n   - Incentives aligned with adoption goals\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q1-Q2 2024)\n\n1. Establish cloud environment and core infrastructure\n2. Implement data lake foundation with initial data domains\n3. Deploy data catalog and basic governance tools\n4. Migrate first non-critical data workloads\n5. Establish DataOps practices and pipelines\n\n#### Phase 2: Expansion (Q3-Q4 2024)\n\n1. Migrate core analytical workloads to the platform\n2. Implement real-time data processing capabilities\n3. Deploy self-service analytics for business users\n4. Enhance data quality frameworks and monitoring\n5. Develop initial ML use cases on the platform\n\n#### Phase 3: Advanced Capabilities (Q1-Q2 2025)\n\n1. Full enterprise adoption across all data domains\n2. Advanced ML capabilities and feature store\n3. Comprehensive data governance implementation\n4. Legacy system decommissioning\n5. Advanced real-time analytics and decisioning\n\n### Considered Alternatives\n\n#### 1. Modernize Existing Data Warehouse\n\n**Pros**: Lower initial disruption, familiar technology, focused scope  \n**Cons**: Limited flexibility, higher long-term costs, limited real-time capabilities\n\nThis approach would not address our fundamental needs for real-time processing, advanced analytics, and managing unstructured data.\n\n#### 2. Traditional Data Lake Architecture\n\n**Pros**: Lower cost storage, support for varied data types, scalability  \n**Cons**: Complexity in ensuring data quality, limited transactional support, governance challenges\n\nA traditional data lake without the lakehouse capabilities would create significant challenges for data reliability, performance, and governance.\n\n#### 3. Multiple Purpose-Built Systems\n\n**Pros**: Optimized solutions for specific use cases, potentially best-in-class capabilities  \n**Cons**: Increased integration complexity, data duplication, inconsistent governance\n\nThis approach would perpetuate our data silo issues and create ongoing integration and consistency challenges.\n\n#### 4. Maintain and Incrementally Improve Current Systems\n\n**Pros**: Minimal disruption, lower initial investment, familiar technology  \n**Cons**: Perpetuates technical debt, limited capability improvement, increasing maintenance costs\n\nThis would fail to address our fundamental challenges and put us at a competitive disadvantage as data volumes and complexity increase.\n\n### References\n\n1. \"Designing Data-Intensive Applications\" by Martin Kleppmann\n2. [Databricks Lakehouse Platform Documentation](https://docs.databricks.com/lakehouse/index.html)\n3. [Azure Data Factory Documentation](https://docs.microsoft.com/en-us/azure/data-factory/)\n4. \"Data Mesh: Delivering Data-Driven Value at Scale\" by Zhamak Dehghani\n5. FinSecure Internal Report: \"Data Platform Requirements Analysis\" (October 2023)\n6. [DAMA Data Management Body of Knowledge](https://www.dama.org/cpages/body-of-knowledge)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2023-10-15 | 0.1 | Initial draft | Jennifer Wu, Chief Data Officer |\n| 2023-11-08 | 0.2 | Updated based on technical review | Raj Patel, Data Engineering Lead |\n| 2023-12-02 | 0.3 | Added implementation phases and cost estimates | Michael Torres, Enterprise Architect |\n| 2023-12-18 | 1.0 | Approved by Executive Technology Committee | FinSecure ETC |\n\n## Appendix A: Data Platform Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Data Sources\"\n        core[Core Banking Systems]\n        crm[CRM Systems]\n        channels[Digital Channels]\n        market[Market Data Feeds]\n        apps[Applications]\n        ext[External Partners]\n    end\n    \n    subgraph \"Data Ingestion\"\n        batch[Batch Processing]\n        stream[Stream Processing]\n        cdc[Change Data Capture]\n        api[API Integration]\n    end\n    \n    subgraph \"Data Storage\"\n        subgraph \"Data Lake\"\n            bronze[Bronze Layer\u003Cbr>Raw Data]\n            silver[Silver Layer\u003Cbr>Cleansed & Conformed]\n            gold[Gold Layer\u003Cbr>Business Aggregates]\n        end\n        \n        dw[Enterprise Data Warehouse]\n        fs[Feature Store]\n    end\n    \n    subgraph \"Processing & Analytics\"\n        etl[ETL/ELT Processes]\n        bi[Business Intelligence]\n        ml[Machine Learning]\n        ad[Advanced Analytics]\n        realtime[Real-time Analytics]\n    end\n    \n    subgraph \"Data Consumption\"\n        reports[Reports & Dashboards]\n        apps2[Applications]\n        apis[APIs & Services]\n        exports[Exports & Feeds]\n        ml_svc[ML Services]\n    end\n    \n    subgraph \"Governance & Operations\"\n        catalog[Data Catalog]\n        quality[Data Quality]\n        lineage[Data Lineage]\n        security[Security & Privacy]\n        lifecycle[Data Lifecycle]\n    end\n    \n    core --> batch\n    crm --> batch\n    channels --> stream\n    market --> stream\n    apps --> cdc\n    ext --> api\n    \n    batch --> bronze\n    stream --> bronze\n    cdc --> bronze\n    api --> bronze\n    \n    bronze --> etl\n    etl --> silver\n    silver --> etl\n    etl --> gold\n    etl --> dw\n    gold --> dw\n    silver --> fs\n    \n    dw --> bi\n    gold --> bi\n    silver --> ml\n    gold --> ml\n    fs --> ml\n    silver --> ad\n    bronze --> realtime\n    silver --> realtime\n    \n    bi --> reports\n    ml --> ml_svc\n    ad --> apis\n    realtime --> apps2\n    dw --> exports\n    \n    catalog --> bronze\n    catalog --> silver\n    catalog --> gold\n    catalog --> dw\n    \n    quality --> silver\n    lineage --> etl\n    security --> dw\n    lifecycle --> bronze\n```\n\n## Appendix B: Data Platform Implementation Timeline\n\n```mermaid\ngantt\n    title FinSecure Data Platform Implementation\n    dateFormat  YYYY-MM-DD\n    \n    section Strategy & Planning\n    Requirements Analysis       :done, 2023-08-01, 2023-10-15\n    Vendor Evaluation           :done, 2023-09-15, 2023-11-15\n    Architecture Design         :done, 2023-10-01, 2023-12-15\n    \n    section Phase 1: Foundation\n    Core Infrastructure Setup   :active, 2024-01-15, 2024-02-28\n    Data Lake Implementation    :2024-02-01, 2024-03-31\n    Governance Framework        :2024-02-15, 2024-04-15\n    Initial Data Migration      :2024-03-01, 2024-05-31\n    DataOps Implementation      :2024-04-01, 2024-06-30\n    \n    section Phase 2: Expansion\n    Advanced Analytics Platform :2024-07-01, 2024-09-30\n    Streaming Data Processing   :2024-08-01, 2024-10-31\n    Self-service Analytics      :2024-09-01, 2024-11-30\n    Data Quality Framework      :2024-10-01, 2024-12-15\n    ML Platform Deployment      :2024-10-15, 2025-01-31\n    \n    section Phase 3: Advanced\n    Enterprise-wide Adoption    :2025-02-01, 2025-04-30\n    Legacy System Decomm        :2025-03-01, 2025-06-30\n    Advanced Governance         :2025-04-01, 2025-06-30\n    Real-time Decisioning       :2025-05-01, 2025-07-31\n    Program Completion          :milestone, 2025-07-31\n```\n\n## Appendix C: Target State Data Flow - Customer 360 Example\n\n```mermaid\nsequenceDiagram\n    participant Source as Source Systems\n    participant Ingest as Data Ingestion\n    participant Bronze as Bronze Layer\n    participant Process as Data Processing\n    participant Silver as Silver Layer\n    participant Gold as Gold Layer\n    participant Consume as Data Consumption\n    \n    Source->>Ingest: Customer Data\u003Cbr>(Core Banking, CRM, Channels)\n    Ingest->>Bronze: Raw Data Ingestion\u003Cbr>(Batch & Streaming)\n    \n    Bronze->>Process: Validation & Cleansing\n    Process->>Process: Data Quality Checks\n    Process->>Process: Deduplication\n    Process->>Process: Standardization\n    Process->>Silver: Refined Customer Records\n    \n    Silver->>Process: Entity Resolution\n    Process->>Process: Customer Attribute Generation\n    Process->>Process: Relationship Mapping\n    Process->>Process: History Processing\n    Process->>Gold: Unified Customer Profiles\n    \n    Gold->>Consume: Customer 360 Views\n    Gold->>Consume: Segmentation Models\n    Gold->>Consume: Marketing Analytics\n    Gold->>Consume: Risk Assessment\n    Gold->>Consume: Real-time Personalization\n    \n    Note over Bronze,Gold: Data Governance\u003Cbr>- Lineage Tracking\u003Cbr>- Privacy Controls\u003Cbr>- Data Classification\u003Cbr>- Quality Monitoring\n```\n\n## Appendix D: Key Performance Indicators\n\n| KPI | Current State | Target (2025) | Measurement Method |\n|-----|---------------|---------------|-------------------|\n| Data Integration Cycle Time | 7-14 days | &lt;24 hours | Average time from source change to data availability |\n| Self-service BI Adoption | 15% of business users | &gt;60% of business users | Monthly active users in self-service tools |\n| Data Quality Score | ~75% | &gt;95% | Composite score from automated quality checks |\n| Cost per TB of Analytics Storage | $2,500/TB | &lt;$500/TB | Total cost of ownership / storage volume |\n| Time to New Analytics | 2-3 weeks | &lt;3 days | Time from request to dashboard availability |\n| Data Platform Availability | 99.5% | 99.95% | Measured service uptime |\n| Regulatory Report Production Time | 10-15 days | 1-2 days | Time to produce monthly regulatory reports |\n| Real-time Decision Latency | Not available | &lt;250ms | Response time for real-time decision APIs |\n| ML Model Deployment Time | 4-6 weeks | &lt;1 week | Time from model approval to production deployment |\n| Data Engineer Productivity | ~30% on new features | &gt;70% on new features | Time allocation analysis |\n\n*Note: Metrics will be tracked quarterly and reported to the Data Governance Council.*",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/examples/02-data-platform-strategy.mdx",
  "394e28b03e3bbe12",
  "docs/technical-architecture-design/architecture-decision-records/published/01-api-gateway-pattern",
  {
    "id": 755,
    "data": 757,
    "body": 762,
    "filePath": 763,
    "digest": 764,
    "deferredRender": 20
  },
  { "title": 758, "summary": 759, "sidebar": 760 },
  "API Gateway Pattern Adoption",
  "Architectural decision record for implementing an API Gateway for the FlowMart e-commerce platform",
  { "label": 761, "order": 470 },
  "API Gateway Pattern",
  "## ADR-001: Adoption of API Gateway Pattern for FlowMart E-commerce Platform\n\n### Status\n\nApproved (2024-06-10)\n\n### Context\n\nAs we migrate to a microservices-based architecture for our e-commerce platform, we face several challenges in managing API endpoints:\n\n1. **API Proliferation**: Each microservice exposes its own APIs, leading to a large number of endpoints that clients need to interact with.\n2. **Cross-cutting Concerns**: Common functionality like authentication, logging, and rate limiting needs to be implemented consistently across all services.\n3. **Client Complexity**: Mobile apps, web frontends, and third-party integrations would need to understand the topology of our microservices ecosystem.\n4. **Protocol Diversity**: Some internal services may use protocols (gRPC, AMQP) that aren't suitable for direct client consumption.\n5. **Network Performance**: Multiple direct client-to-service calls increase network overhead, especially on mobile networks.\n6. **Security Exposure**: Direct exposure of microservices increases the attack surface of our platform.\n\nWe need a solution that simplifies our API landscape while maintaining the benefits of our microservices architecture.\n\n### Decision\n\nWe will implement an **API Gateway Pattern** as the primary entry point for all client applications interacting with the FlowMart e-commerce platform. Specifically:\n\n1. **Gateway Implementation**: We will use **Kong API Gateway** as our primary implementation due to its robust plugin ecosystem, performance characteristics, and open-source foundation.\n\n2. **Gateway Responsibilities**:\n   - **Request Routing**: Directing requests to appropriate backend services\n   - **Authentication & Authorization**: Validating user identity and permissions\n   - **Rate Limiting**: Protecting services from excessive load\n   - **Request/Response Transformation**: Adapting data formats between clients and services\n   - **Caching**: Reducing backend load for frequently requested data\n   - **Logging & Monitoring**: Centralized request tracking\n   - **Circuit Breaking**: Preventing cascading failures\n   - **Protocol Translation**: Supporting REST, GraphQL, and gRPC as needed\n\n3. **Gateway Topology**:\n   - **Edge Gateway**: A public-facing gateway for all external clients\n   - **Internal Gateway**: For service-to-service communication within our private network\n\n4. **API Composition**: The gateway will aggregate responses from multiple services when needed to reduce client-side complexity.\n\n5. **Backend-for-Frontend (BFF) Pattern**: We will implement specific gateway configurations for different client platforms (web, mobile iOS, mobile Android) to optimize payloads and calls.\n\n6. **API Versioning**: The gateway will support multiple API versions to enable graceful evolution of our services.\n\n### Consequences\n\n#### Positive\n\n1. **Simplified Client Development**: Clients only need to know about a single entry point rather than multiple service endpoints.\n\n2. **Consistent Security Enforcement**: Authentication, authorization, and other security controls can be implemented uniformly.\n\n3. **Operational Visibility**: Centralized logging and monitoring of all API traffic.\n\n4. **Performance Optimization**: Opportunity for response caching, request collapsing, and other optimizations.\n\n5. **Flexible Evolution**: Backend services can be refactored, replaced, or decomposed without changing client implementations.\n\n6. **Traffic Control**: Ability to throttle, prioritize, or redirect traffic based on various conditions.\n\n#### Negative\n\n1. **Potential Single Point of Failure**: The gateway becomes critical infrastructure that must be highly available.\n\n2. **Increased Latency**: Adding another network hop introduces some additional latency.\n\n3. **Gateway Complexity**: The gateway configuration grows in complexity as the number of services increases.\n\n4. **Operational Overhead**: Additional infrastructure to monitor, maintain, and scale.\n\n5. **Deployment Coupling**: Gateway configuration changes may need coordination with service deployments.\n\n6. **Performance Bottleneck Risk**: Without proper scaling, the gateway could become a bottleneck under high load.\n\n### Mitigation Strategies\n\n1. **High Availability**: Deploy the API Gateway in a redundant, auto-scaling configuration across multiple availability zones.\n\n2. **Performance Optimization**: Implement efficient caching strategies and regular performance testing.\n\n3. **Circuit Breakers**: Implement circuit breakers to prevent cascading failures if backend services are unavailable.\n\n4. **Automated Configuration**: Use infrastructure as code to manage gateway configuration, with automated testing.\n\n5. **API Governance**: Establish clear ownership and review processes for gateway configuration changes.\n\n6. **Observability**: Implement comprehensive monitoring and alerting specifically for the gateway.\n\n### Implementation Details\n\n#### Phase 1: Core Infrastructure (Q2 2024)\n\n1. Deploy Kong API Gateway in development and staging environments\n2. Implement basic routing for 2-3 core services\n3. Set up authentication and authorization\n4. Establish monitoring and logging\n5. Create CI/CD pipeline for gateway configuration\n\n#### Phase 2: Feature Expansion (Q3 2024)\n\n1. Migrate all existing APIs to the gateway\n2. Implement rate limiting and circuit breaking\n3. Add response caching for appropriate endpoints\n4. Develop BFF configurations for web and mobile\n5. Implement comprehensive end-to-end testing\n\n#### Phase 3: Advanced Capabilities (Q4 2024)\n\n1. Implement GraphQL support for specific use cases\n2. Add advanced analytics and API usage dashboards\n3. Integrate with service mesh for internal service communication\n4. Implement advanced security features (WAF, etc.)\n5. Optimize for global performance\n\n### Considered Alternatives\n\n#### 1. Direct Client-to-Microservice Communication\n\n**Pros**: Simplicity, no additional network hop, no central bottleneck  \n**Cons**: Client complexity, inconsistent policy enforcement, security challenges\n\nThis approach would expose our internal service architecture directly to clients, creating significant complexity and security concerns.\n\n#### 2. Service Mesh Only (no API Gateway)\n\n**Pros**: Great for service-to-service communication, built-in resilience  \n**Cons**: Not designed for edge traffic, less focus on client-specific concerns\n\nWhile we plan to use a service mesh for internal communication, it doesn't address the client-facing concerns that an API Gateway solves.\n\n#### 3. Bespoke API Gateway\n\n**Pros**: Custom-built for our exact needs, no license costs  \n**Cons**: Development and maintenance overhead, feature gaps, opportunity cost\n\nBuilding our own gateway would divert significant resources from our core business and likely result in a less robust solution than established products.\n\n#### 4. Alternative Gateway Products (Apigee, AWS API Gateway)\n\n**Pros**: Managed services (reduced operational burden), rich feature sets  \n**Cons**: Higher costs, potential vendor lock-in, less customization\n\nThese were viable alternatives, but Kong offered the best balance of features, flexibility, and cost for our requirements.\n\n### References\n\n1. Richardson, Chris. \"Pattern: API Gateway / Backends for Frontends.\" [Microservices.io](https://microservices.io/patterns/apigateway.html)\n2. Fowler, Martin. \"BFF: Backend for Frontend.\" [martinfowler.com](https://martinfowler.com/articles/gateway-pattern.html)\n3. [Kong API Gateway Documentation](https://docs.konghq.com/)\n4. Newman, Sam. Building Microservices (O'Reilly)\n5. API Gateway Pattern, Microsoft Azure Architecture Center. [Microsoft Docs](https://docs.microsoft.com/en-us/azure/architecture/microservices/design/gateway)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-05-15 | 0.1 | Initial draft | James Wilson |\n| 2024-05-25 | 0.2 | Added implementation phases and alternatives | Sarah Chen |\n| 2024-06-05 | 0.3 | Incorporated feedback from architecture review | David Boyne |\n| 2024-06-10 | 1.0 | Approved by Architecture Board | Architecture Board |\n\n## Appendix A: API Gateway Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"External Clients\"\n        web[Web Application]\n        mobile[Mobile Apps]\n        partners[Partner Systems]\n    end\n\n    subgraph \"API Layer\"\n        gateway[Kong API Gateway]\n        style gateway fill:#b9e0a5,stroke:#333,stroke-width:2px\n        auth[Auth Service]\n        gateway \u003C--> auth\n    end\n\n    subgraph \"Backend Services\"\n        products[Product Service]\n        orders[Order Service]\n        inventory[Inventory Service]\n        pricing[Pricing Service]\n        users[User Service]\n        recommendations[Recommendation Service]\n    end\n\n    web --> gateway\n    mobile --> gateway\n    partners --> gateway\n    \n    gateway --> products\n    gateway --> orders\n    gateway --> inventory\n    gateway --> pricing\n    gateway --> users\n    gateway --> recommendations\n```\n\n## Appendix B: Gateway Policy Application Flow\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Gateway as API Gateway\n    participant Auth as Authentication Service\n    participant Service as Backend Service\n    \n    Client->>Gateway: API Request\n    \n    Gateway->>Gateway: Apply Rate Limiting\n    Note over Gateway: Check quota and rate limits\n    \n    Gateway->>Auth: Validate Token\n    Auth-->>Gateway: Token Valid/Invalid\n    \n    alt Token Valid\n        Gateway->>Gateway: Apply Request Transformation\n        Gateway->>Service: Forward Request\n        Service-->>Gateway: Service Response\n        Gateway->>Gateway: Apply Response Transformation\n        Gateway->>Gateway: Apply Caching Headers\n        Gateway-->>Client: Transformed Response\n    else Token Invalid\n        Gateway-->>Client: 401 Unauthorized\n    end\n```\n\n## Appendix C: Gateway Feature Rollout\n\n| Feature | Target Date | Priority | Dependencies | Owner |\n|---------|-------------|----------|--------------|-------|\n| Basic Routing | Q2 2024 | Critical | Gateway Infrastructure | DevOps Team |\n| Authentication | Q2 2024 | Critical | Auth Service | Security Team |\n| Monitoring & Logging | Q2 2024 | High | Observability Platform | DevOps Team |\n| Rate Limiting | Q3 2024 | High | None | Platform Team |\n| Request/Response Transformation | Q3 2024 | Medium | API Standards | API Team |\n| Circuit Breaking | Q3 2024 | Medium | Service Health Metrics | Platform Team |\n| Caching | Q3 2024 | Medium | Cache Invalidation Strategy | Performance Team |\n| GraphQL Support | Q4 2024 | Low | GraphQL Schema | API Team |\n| Advanced Analytics | Q4 2024 | Low | Data Platform | Data Team |",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/published/01-api-gateway-pattern.mdx",
  "32ee617232934ccc",
  "docs/technical-architecture-design/architecture-decision-records/published/02-eda-adoption",
  {
    "id": 765,
    "data": 767,
    "body": 771,
    "filePath": 772,
    "digest": 773,
    "deferredRender": 20
  },
  { "title": 768, "summary": 632, "sidebar": 769 },
  "Event-driven architecture adoption",
  { "label": 770, "order": 487 },
  "EDA Adoption",
  "## ADR-001: Adoption of Event-Driven Architecture for FlowMart E-commerce Platform\n\n### Status\n\nApproved (2024-07-15)\n\n### Context\n\nFlowMart is building a new e-commerce platform to replace our legacy monolithic application. The current system faces several challenges:\n\n1. **Scalability Issues**: During peak shopping periods (e.g., Black Friday, holiday season), the system struggles to handle increased traffic, resulting in degraded performance and occasional outages.\n2. **Maintenance Complexity**: Adding new features or modifying existing ones requires extensive regression testing and often leads to unexpected side effects.\n3. **Technology Constraints**: The monolithic architecture limits our ability to adopt new technologies or update components independently.\n4. **Data Consistency**: Ensuring data consistency across different parts of the application has become increasingly difficult.\n5. **Team Independence**: Multiple development teams working on different aspects of the application frequently block each other.\n\nWe need an architecture that addresses these challenges while enabling rapid innovation and scaling to meet our projected growth over the next 3-5 years.\n\n### Decision\n\nWe will adopt an **Event-Driven Architecture (EDA)** using a microservices approach for the new FlowMart e-commerce platform. Specifically:\n\n1. **Domain-Driven Design (DDD)**: We will organize our services around business domains (Orders, Inventory, Payment, Shipping, etc.) with clearly defined bounded contexts.\n\n2. **Event Sourcing**: Critical business transactions will be stored as a sequence of immutable events that can be used to reconstruct the system state at any point in time.\n\n3. **Command Query Responsibility Segregation (CQRS)**: We will separate read and write operations where appropriate to optimize for different performance and scaling requirements.\n\n4. **Apache Kafka** will serve as our primary event streaming platform for asynchronous communication between services.\n\n5. **Eventual Consistency Model**: We acknowledge that the system will prioritize availability and partition tolerance over immediate consistency (following the CAP theorem), with mechanisms to ensure eventual consistency.\n\n6. **Service Autonomy**: Each service will:\n   - Have its own database\n   - Be independently deployable\n   - Have well-defined APIs and event contracts\n   - Be responsible for publishing domain events when state changes occur\n\n7. **Choreography Over Orchestration**: Services will primarily react to events rather than being orchestrated by a central coordinator, though we will use orchestration for complex workflows when necessary.\n\n### Consequences\n\n#### Positive\n\n1. **Improved Scalability**: Individual services can scale independently based on demand, allowing us to allocate resources more efficiently.\n\n2. **Better Fault Isolation**: Failures in one service are less likely to cascade across the entire system, improving overall reliability.\n\n3. **Technology Flexibility**: Teams can choose the most appropriate technologies for their specific domains, allowing for incremental adoption of new technologies.\n\n4. **Team Autonomy**: Domain-aligned teams can develop, test, and deploy their services independently, reducing cross-team dependencies.\n\n5. **Enhanced Auditability**: Event sourcing provides a complete audit trail of all system changes, which is valuable for debugging, compliance, and business analytics.\n\n6. **Improved Extensibility**: New capabilities can be added by creating new consumers of existing events without modifying the original producers.\n\n#### Negative\n\n1. **Increased Complexity**: Distributed systems are inherently more complex to develop, test, debug, and operate compared to monolithic applications.\n\n2. **Learning Curve**: The team will need to learn new patterns, technologies, and operational practices, which may slow initial development.\n\n3. **Eventual Consistency Challenges**: Business operations and UI design must account for data that might not be immediately consistent across services.\n\n4. **Operational Overhead**: Managing multiple services, event streams, and databases requires more sophisticated monitoring, deployment, and operational tools.\n\n5. **Transaction Management**: Ensuring transactional integrity across service boundaries requires careful design and implementation of compensation patterns.\n\n6. **Testing Complexity**: End-to-end testing becomes more challenging, requiring new testing strategies and tools.\n\n### Compliance Requirements\n\nOur implementation must adhere to the following requirements:\n\n1. **Data Privacy**: Personal customer data must be handled in compliance with GDPR, CCPA, and other applicable regulations.\n\n2. **PCI DSS**: Payment processing components must comply with Payment Card Industry Data Security Standards.\n\n3. **Audit Trail**: All critical business transactions must be traceable and auditable for a minimum of 7 years.\n\n4. **Security**: Authentication, authorization, and data encryption standards must be consistently applied across all services.\n\n### Implementation Details\n\n#### Phase 1: Core Domain Decomposition (Q3 2024)\n\n1. Identify and define core domain boundaries\n2. Establish event schemas and contracts\n3. Implement Kafka infrastructure and operational tooling\n4. Migrate the first domain (Orders) to the new architecture\n5. Set up CI/CD pipelines and monitoring\n\n#### Phase 2: Domain Expansion (Q4 2024)\n\n1. Migrate Inventory and Payment domains\n2. Implement event sourcing for critical domains\n3. Develop read models for reporting and analytics\n4. Establish cross-domain consistency patterns\n\n#### Phase 3: Legacy Decommissioning (Q1-Q2 2025)\n\n1. Migrate remaining domains\n2. Implement advanced monitoring and alerting\n3. Gradually decommission legacy system components\n4. Complete performance tuning and optimization\n\n### Considered Alternatives\n\n#### 1. Modular Monolith\n\n**Pros**: Simpler development model, transactional integrity, easier testing  \n**Cons**: Limited independent scaling, technology constraints, deployment coupling\n\nThis approach would address some concerns (maintainability, modularity) but would not solve our scalability and team independence challenges.\n\n#### 2. Microservices with REST-only Communication\n\n**Pros**: Well-understood patterns, synchronous communication simplicity  \n**Cons**: Tighter coupling, limited resilience, cascading failures\n\nThis approach would improve modularity but would not adequately address resilience and scalability concerns.\n\n#### 3. Serverless Architecture\n\n**Pros**: Minimal infrastructure management, high elasticity, pay-per-use model  \n**Cons**: Vendor lock-in, cold start latency, limited control over infrastructure\n\nWhile appealing for certain scenarios, this approach would not provide the control and predictability needed for our core business functions.\n\n### References\n\n1. Building Event-Driven Microservices (Adam Bellemare, O'Reilly)\n2. Domain-Driven Design (Eric Evans, Addison-Wesley)\n3. Enterprise Integration Patterns (Gregor Hohpe, Bobby Woolf, Addison-Wesley)\n4. [Kafka Documentation](https://kafka.apache.org/documentation/)\n5. [CQRS Pattern](https://martinfowler.com/bliki/CQRS.html) (Martin Fowler)\n6. [Event Sourcing Pattern](https://martinfowler.com/eaaDev/EventSourcing.html) (Martin Fowler)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-06-22 | 0.1 | Initial draft | David Boyne |\n| 2024-06-30 | 0.2 | Incorporate feedback from architecture review | Amy Smith |\n| 2024-07-10 | 0.3 | Added implementation phasing and compliance requirements | Kiran Patel |\n| 2024-07-15 | 1.0 | Approved by Architecture Board | Architecture Board |\n\n## Appendix A: High-Level Architecture Diagram\n\n```mermaid\nflowchart TB\n    subgraph \"API Gateway\"\n        gw[API Gateway]\n    end\n\n    subgraph \"Customer Domain\"\n        cust[Customer Service]\n        auth[Authentication Service]\n    end\n\n    subgraph \"Order Domain\"\n        order[Order Service]\n        orderhist[Order History Service]\n    end\n\n    subgraph \"Inventory Domain\"\n        inv[Inventory Service]\n        stock[Stock Management]\n    end\n\n    subgraph \"Payment Domain\"\n        payment[Payment Service]\n        refund[Refund Service]\n    end\n\n    subgraph \"Shipping Domain\"\n        ship[Shipping Service]\n        track[Tracking Service]\n    end\n\n    subgraph \"Event Backbone\"\n        kafka[Apache Kafka]\n    end\n\n    gw --> order\n    gw --> cust\n    gw --> auth\n    gw --> inv\n    gw --> payment\n    gw --> ship\n    gw --> track\n\n    order --> kafka\n    cust --> kafka\n    inv --> kafka\n    payment --> kafka\n    ship --> kafka\n    track --> kafka\n\n    kafka --> orderhist\n    kafka --> stock\n    kafka --> refund\n```\n\n## Appendix B: Key Event Flows\n\n### Order Placement Flow\n\n```mermaid\nsequenceDiagram\n    participant C as Customer\n    participant OS as Order Service\n    participant IS as Inventory Service\n    participant PS as Payment Service\n    participant SS as Shipping Service\n    participant NS as Notification Service\n\n    C->>OS: Place Order\n    OS->>IS: Check Inventory\n    IS-->>OS: Inventory Available\n    OS->>PS: Process Payment\n    PS-->>OS: Payment Approved\n    OS->>OS: Create Order\n    OS->>+kafka: Publish OrderCreated\n    kafka->>IS: OrderCreated\n    IS->>IS: Reserve Inventory\n    IS->>kafka: Publish InventoryReserved\n    kafka->>SS: InventoryReserved\n    SS->>SS: Create Shipment\n    SS->>kafka: Publish ShipmentCreated\n    kafka->>NS: ShipmentCreated\n    NS->>C: Send Order Confirmation\n```\n\n### Out-of-Stock Handling Flow\n\n```mermaid\nsequenceDiagram\n    participant IS as Inventory Service\n    participant K as Kafka\n    participant NS as Notification Service\n    participant RS as Replenishment Service\n    participant OS as Order Service\n    participant CS as Customer\n\n    IS->>IS: Stock level check\n    IS->>+K: Publish OutOfStockEvent\n    K->>NS: OutOfStockEvent\n    K->>RS: OutOfStockEvent\n    K->>OS: OutOfStockEvent\n    \n    NS->>CS: Send out-of-stock notification\n    RS->>RS: Create replenishment order\n    RS->>+K: Publish ReplenishmentOrderedEvent\n    OS->>OS: Update product availability\n    OS->>CS: Show \"Notify when available\" option\n\n    note over RS,IS: After stock is replenished\n    RS->>IS: Restock inventory\n    IS->>+K: Publish InventoryRestockedEvent\n    K->>NS: InventoryRestockedEvent\n    K->>OS: InventoryRestockedEvent\n    \n    NS->>CS: Send back-in-stock notification\n    OS->>OS: Update product availability\n```\n\n## Appendix C: Service Ownership\n\n| Domain | Service | Team |\n|--------|---------|------|\n| Customer | Customer Service | Full Stack Team |\n| Customer | Authentication Service | Security Team |\n| Order | Order Service | Order Management Team |\n| Order | Order History Service | Order Management Team |\n| Inventory | Inventory Service | Full Stack Team |\n| Inventory | Stock Management | Full Stack Team |\n| Payment | Payment Service | Payment Team |\n| Payment | Refund Service | Payment Team |\n| Shipping | Shipping Service | Logistics Team |\n| Shipping | Tracking Service | Logistics Team |",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/published/02-eda-adoption.mdx",
  "28383edde8585263",
  "docs/technical-architecture-design/architecture-decision-records/published/04-database-strategy",
  {
    "id": 774,
    "data": 776,
    "body": 781,
    "filePath": 782,
    "digest": 783,
    "deferredRender": 20
  },
  { "title": 777, "summary": 778, "sidebar": 779 },
  "Database Strategy for Microservices",
  "Architectural decision record for database selection and data management in the FlowMart e-commerce platform",
  { "label": 780, "order": 584 },
  "Database Strategy",
  "## ADR-004: Database Strategy for Microservices in FlowMart E-commerce Platform\n\n### Status\n\nApproved (2024-08-20)\n\n### Context\n\nOur transition from a monolithic architecture to microservices necessitates a reassessment of our database strategy. The existing monolithic application uses a centralized Oracle database with hundreds of tables spanning multiple business domains. This approach has created several challenges:\n\n1. **Schema Coupling**: Changes to database schemas require careful coordination across teams to avoid breaking dependent services.\n2. **Scalability Limitations**: The relational database becomes a bottleneck during high-traffic periods, affecting all application features.\n3. **One-Size-Fits-All Approach**: Different data access patterns are all forced into the same relational model, leading to suboptimal performance.\n4. **Operational Overhead**: Database administration is complex due to competing requirements from different application components.\n5. **Development Bottlenecks**: Teams must coordinate database changes, slowing down development velocity.\n6. **Cost Efficiency**: The enterprise database licensing model is expensive and doesn't scale cost-effectively with our varying workloads.\n\nAs we decompose our application into microservices, we need a database strategy that promotes service autonomy while ensuring data consistency, performance, and operational efficiency.\n\n### Decision\n\nWe will adopt a **polyglot persistence strategy** for our microservices architecture, following the principle of \"right database for the right job.\" Key aspects of this strategy include:\n\n1. **Database-per-Service Pattern**:\n   - Each microservice will own and exclusively manage its database\n   - No direct database access from other services\n   - Services interact via well-defined APIs, not shared databases\n\n2. **Primary Database Technologies**:\n   - **PostgreSQL**: Primary relational database for services with complex transactional requirements\n   - **MongoDB**: Document database for services with flexible schema requirements and read-heavy workloads\n   - **Redis**: In-memory data store for caching, session management, and real-time features\n   - **Elasticsearch**: Search engine for product catalog and content search\n   - **Apache Cassandra**: Wide-column store for high-volume time-series data and analytics\n   - **Amazon DynamoDB**: Managed NoSQL for highly scalable microservices with predictable access patterns\n\n3. **Data Consistency Patterns**:\n   - **Saga Pattern**: For coordinating transactions across multiple services\n   - **Event Sourcing**: For critical business domains requiring complete audit trails\n   - **CQRS**: For services with asymmetric read/write loads\n   - **Outbox Pattern**: For reliable event publishing during state changes\n\n4. **Data Migration and Evolution**:\n   - Versioned database schemas\n   - Backward-compatible schema changes\n   - Blue/green deployment for database changes\n   - Incremental data migration approach\n\n5. **Data Governance**:\n   - Centralized data catalog and lineage tracking\n   - Common data models for shared business entities\n   - Standard approach to master data management\n   - Automated data quality monitoring\n\n6. **Operational Excellence**:\n   - Infrastructure as Code (IaC) for all database resources\n   - Automated backup and recovery procedures\n   - Standardized monitoring and alerting\n   - Database reliability engineering practices\n\n### Database Selection Criteria by Domain\n\n| Domain | Database Type | Primary Factors | Secondary Considerations |\n|--------|---------------|-----------------|--------------------------|\n| Product Catalog | MongoDB + Elasticsearch | Schema flexibility, Search capabilities | High read-to-write ratio, Caching |\n| Order Management | PostgreSQL | ACID transactions, Complex queries | Event sourcing for audit trail |\n| Customer Profile | PostgreSQL | Data consistency, Relational integrity | Privacy compliance, Scalable reads |\n| Inventory | MongoDB | Frequent schema evolution, High write throughput | Eventual consistency model |\n| Cart & Checkout | Redis + PostgreSQL | Low latency, High availability | Transaction support for checkout |\n| Payment Processing | PostgreSQL | Strong consistency, Transaction support | Compliance requirements, Security |\n| Analytics | Cassandra | High write throughput, Time-series data | Analytical query patterns |\n| Recommendations | Redis + MongoDB | Low latency reads, Flexible data model | Machine learning feature support |\n| User Sessions | Redis | Ultra-low latency, Expiration support | High availability, Cross-region replication |\n| Content Management | MongoDB | Flexible schema, Document structure | Full-text search capability |\n\n### Consequences\n\n#### Positive\n\n1. **Service Autonomy**: Teams can independently select, optimize, and evolve their databases without cross-team coordination.\n\n2. **Optimized Performance**: Each service can use a database technology optimized for its specific data access patterns.\n\n3. **Independent Scaling**: Database resources can be scaled according to the specific needs of each service.\n\n4. **Improved Resilience**: Database failures are isolated to specific services rather than affecting the entire system.\n\n5. **Fit-for-Purpose Solutions**: Different data models (relational, document, key-value, etc.) can be applied where most appropriate.\n\n6. **Cost Optimization**: Resources can be allocated based on the specific needs of each service.\n\n7. **Incremental Adoption**: New database technologies can be introduced for new services without migrating legacy data.\n\n#### Negative\n\n1. **Increased Operational Complexity**: Managing multiple database technologies requires broader expertise and more sophisticated tooling.\n\n2. **Data Consistency Challenges**: Maintaining consistency across service boundaries requires careful design and implementation.\n\n3. **Learning Curve**: Development teams need to learn multiple database technologies and data access patterns.\n\n4. **Distributed Transactions**: Business processes spanning multiple services require more complex transaction management.\n\n5. **Increased Infrastructure Costs**: Running and maintaining multiple database systems may increase overall infrastructure costs.\n\n6. **Data Duplication**: Some data may need to be duplicated across services, creating synchronization challenges.\n\n7. **Monitoring and Troubleshooting Complexity**: Different databases require different monitoring approaches and expertise.\n\n### Mitigation Strategies\n\n1. **Platform Database Service**:\n   - Create an internal platform team that provides database-as-a-service capabilities\n   - Standardize on a limited set of supported database technologies\n   - Provide automated provisioning, backup, and monitoring\n\n2. **Data Access Layer Pattern**:\n   - Create standardized libraries for common database access patterns\n   - Abstract database-specific details behind consistent interfaces\n   - Implement retry logic, circuit breakers, and observability\n\n3. **Data Synchronization Framework**:\n   - Implement a standardized approach to CDC (Change Data Capture)\n   - Create reusable components for the Outbox Pattern\n   - Develop standard data synchronization workflows\n\n4. **Database Reliability Engineering**:\n   - Establish dedicated database reliability engineers\n   - Create runbooks for common database operations\n   - Implement chaos engineering practices for database failure testing\n\n5. **Developer Training and Support**:\n   - Comprehensive training program for supported database technologies\n   - Internal knowledge base and best practice documentation\n   - Database design review process for new services\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q3-Q4 2024)\n\n1. Establish database platform team and core services\n2. Set up standard PostgreSQL and MongoDB offerings\n3. Implement database CI/CD pipelines\n4. Develop initial monitoring and alerting\n5. Create database provisioning automation\n\n#### Phase 2: Expansion (Q1-Q2 2025)\n\n1. Add Redis and Elasticsearch to supported offerings\n2. Implement data synchronization framework\n3. Develop CQRS and Event Sourcing patterns\n4. Create data governance tooling\n5. Expand monitoring capabilities\n\n#### Phase 3: Advanced Capabilities (Q3-Q4 2025)\n\n1. Add Cassandra and specialized databases\n2. Implement advanced high availability features\n3. Develop cross-region replication capabilities\n4. Create advanced analytics integration\n5. Implement predictive scaling and optimization\n\n### Considered Alternatives\n\n#### 1. Shared Database Approach\n\n**Pros**: Simplicity, familiar model, transactional integrity, easier reporting  \n**Cons**: Tight coupling, scalability limitations, schema coordination challenges\n\nThis approach would minimize short-term changes but would recreate many of the current issues in our new architecture.\n\n#### 2. Single Database Technology (PostgreSQL Only)\n\n**Pros**: Operational simplicity, consistent expertise, familiar tooling  \n**Cons**: Not optimal for all workloads, potential performance compromises\n\nWhile PostgreSQL is versatile, it isn't the best choice for all our diverse data requirements.\n\n#### 3. Database-as-a-Service Only (e.g., AWS RDS/DynamoDB)\n\n**Pros**: Reduced operational overhead, managed scaling, built-in high availability  \n**Cons**: Vendor lock-in, potential cost concerns, less flexibility\n\nWhile we'll use managed services where appropriate, we need the flexibility to run certain databases on our own infrastructure.\n\n#### 4. Fully Centralized Data Lake Approach\n\n**Pros**: Centralized analytics, simplified data governance, consistent data model  \n**Cons**: Complex real-time synchronization, potential performance issues, development overhead\n\nThis approach works well for analytics but isn't suitable for operational data needs.\n\n### References\n\n1. Kleppmann, Martin. \"Designing Data-Intensive Applications\" (O'Reilly Media)\n2. Vernon, Vaughn. \"Implementing Domain-Driven Design\" (Addison-Wesley)\n3. Fowler, Martin. \"PolyglotPersistence\" [martinfowler.com](https://martinfowler.com/bliki/PolyglotPersistence.html)\n4. Richardson, Chris. \"Microservices Patterns\" (Manning Publications)\n5. [Database per Service Pattern](https://microservices.io/patterns/data/database-per-service.html)\n6. [Saga Pattern](https://microservices.io/patterns/data/saga.html)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-07-20 | 0.1 | Initial draft | Robert Kim |\n| 2024-08-02 | 0.2 | Added domain-specific database recommendations | Maria Garcia |\n| 2024-08-15 | 0.3 | Incorporated feedback from architecture review | David Boyne |\n| 2024-08-20 | 1.0 | Approved by Architecture Board | Architecture Board |\n\n## Appendix A: Database Architecture Overview\n\n```mermaid\nflowchart TB\n    subgraph \"Service Domains\"\n        subgraph \"Product Domain\"\n            pc[Product Catalog]\n            ps[Product Search]\n            pc --- MongoDB\n            ps --- Elasticsearch\n        end\n        \n        subgraph \"Order Domain\"\n            om[Order Management]\n            oh[Order History]\n            om --- PostgreSQL\n            oh --- MongoDB\n        end\n        \n        subgraph \"Customer Domain\"\n            cp[Customer Profile]\n            auth[Authentication]\n            sess[Sessions]\n            cp --- PostgreSQL\n            auth --- PostgreSQL\n            sess --- Redis\n        end\n        \n        subgraph \"Inventory Domain\"\n            inv[Inventory Management]\n            stk[Stock Tracking]\n            inv --- MongoDB\n            stk --- MongoDB\n        end\n        \n        subgraph \"Payment Domain\"\n            pay[Payment Processing]\n            tx[Transaction Ledger]\n            pay --- PostgreSQL\n            tx --- PostgreSQL\n        end\n        \n        subgraph \"Analytics Domain\"\n            metrics[Metrics Collection]\n            reports[Reporting]\n            metrics --- Cassandra\n            reports --- Elasticsearch\n        end\n    end\n    \n    subgraph \"Data Integration\"\n        ksql[KSQL]\n        cdc[CDC Connectors]\n        etl[ETL Processes]\n        \n        Kafka --> ksql\n        cdc --> Kafka\n        ksql --> etl\n    end\n    \n    MongoDB --> cdc\n    PostgreSQL --> cdc\n    Cassandra --> cdc\n    \n    etl --> DataLake\n```\n\n## Appendix B: Data Consistency Patterns by Service Type\n\n```mermaid\nflowchart TB\n    subgraph \"Data Consistency Patterns\"\n        direction TB\n        \n        subgraph \"Transactional Services\"\n            saga[Saga Pattern]\n            tcc[Try-Confirm/Cancel Pattern]\n            2pc[Two-Phase Commit]\n        end\n        \n        subgraph \"Event-Driven Services\"\n            es[Event Sourcing]\n            cqrs[CQRS]\n            outbox[Outbox Pattern]\n        end\n        \n        subgraph \"Caching Patterns\"\n            cache[Cache-Aside]\n            writeBehind[Write-Behind]\n            readThrough[Read-Through]\n        end\n    end\n    \n    subgraph \"Service Examples\"\n        checkout[Checkout Service]\n        inventory[Inventory Service]\n        catalog[Product Catalog]\n        recommendations[Recommendations]\n    end\n    \n    checkout --> saga\n    checkout --> 2pc\n    inventory --> es\n    inventory --> outbox\n    catalog --> cqrs\n    catalog --> cache\n    recommendations --> readThrough\n```\n\n## Appendix C: Database Migration Strategy\n\n```mermaid\nsequenceDiagram\n    participant Legacy as Legacy Database\n    participant Middleware as Migration Middleware\n    participant New as New Service Databases\n    participant CDC as Change Data Capture\n    participant Events as Event Bus\n    \n    Note over Legacy, Events: Phase 1: Shadow Reading\n    \n    Legacy->>Middleware: Original Data Access\n    Middleware->>New: Shadow Writes\n    \n    Note over Legacy, Events: Phase 2: Dual Write\n    \n    Legacy->>Middleware: Original Data Access\n    Middleware->>Legacy: Write to Legacy\n    Middleware->>New: Write to New System\n    \n    Note over Legacy, Events: Phase 3: CDC Migration\n    \n    Legacy->>CDC: Capture Changes\n    CDC->>Events: Publish Change Events\n    Events->>New: Update New System\n    \n    Note over Legacy, Events: Phase 4: Reverse Flow\n    \n    New->>Middleware: Primary Data Access\n    Middleware->>Legacy: Synchronize for Legacy Systems\n    \n    Note over Legacy, Events: Phase 5: Legacy Retirement\n    \n    New->>New: Fully Migrated Operations\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/published/04-database-strategy.mdx",
  "0b4064a825f8b911",
  "docs/technical-architecture-design/architecture-decision-records/published/03-auth-strategy",
  {
    "id": 784,
    "data": 786,
    "body": 791,
    "filePath": 792,
    "digest": 793,
    "deferredRender": 20
  },
  { "title": 787, "summary": 788, "sidebar": 789 },
  "Authentication and Authorization Strategy",
  "Architectural decision record for implementing authentication and authorization for the FlowMart e-commerce platform",
  { "label": 790, "order": 515 },
  "Auth Strategy",
  "## ADR-003: Authentication and Authorization Strategy for FlowMart E-commerce Platform\n\n### Status\n\nApproved (2024-08-01)\n\n### Context\n\nAs we move to a microservices architecture for our e-commerce platform, we need a robust, scalable, and secure approach to authentication and authorization. The current monolithic application uses a custom auth solution with session-based authentication, which presents several challenges in a distributed environment:\n\n1. **Session Stickiness**: Requires load balancer configuration to route users to the same server.\n2. **Scalability Constraints**: Session state storage becomes challenging as we scale horizontally.\n3. **Cross-Service Authentication**: No standardized way for services to validate user identity and permissions.\n4. **Partner and Third-Party Integration**: Difficult to securely expose APIs to external systems.\n5. **Multiple Authentication Factors**: Need to support various authentication methods (passwords, social logins, biometrics).\n6. **Varying Authorization Requirements**: Different resources require different permission models.\n7. **Compliance Requirements**: GDPR, PCI-DSS, and industry regulations impose strict requirements on handling authentication data.\n\nWe need an authentication and authorization strategy that works effectively in a distributed microservices environment while maintaining high security standards.\n\n### Decision\n\nWe will implement a token-based authentication and authorization strategy using **OAuth 2.0** and **OpenID Connect (OIDC)** standards with **Auth0** as our identity provider. Specifically:\n\n1. **Authentication Pattern**:\n   - **JWT (JSON Web Tokens)** for representing claims between parties\n   - **Stateless authentication** where possible to improve scalability\n   - **OAuth 2.0 Authorization Code Flow with PKCE** for web applications\n   - **OAuth 2.0 Resource Owner Password Grant** (limited to legacy/internal applications)\n   - **OAuth 2.0 Client Credentials Grant** for service-to-service communication\n\n2. **Authorization Pattern**:\n   - **Role-Based Access Control (RBAC)** as the primary mechanism for coarse-grained permissions\n   - **Attribute-Based Access Control (ABAC)** for fine-grained access decisions\n   - **Policy Enforcement Points (PEP)** implemented at the API Gateway level for common policies\n   - **Service-level authorization** for domain-specific access control\n\n3. **User Management**:\n   - Centralized user directory in Auth0\n   - Self-service user registration and profile management\n   - Admin-managed role assignments and permissions\n   - Progressive profiling to collect user information gradually\n\n4. **Multi-Factor Authentication (MFA)**:\n   - Required for administrative accounts and sensitive operations\n   - Optional but encouraged for standard user accounts\n   - Risk-based authentication triggers (unusual location, device, behavior)\n\n5. **Service-to-Service Authentication**:\n   - Mutual TLS (mTLS) for service mesh communication\n   - Client credentials OAuth flow with short-lived tokens for cross-boundary requests\n\n6. **API Security**:\n   - Token validation at the API Gateway\n   - Scoped access tokens for limiting API permissions\n   - Token introspection for high-security operations\n\n### Consequences\n\n#### Positive\n\n1. **Improved Scalability**: Stateless authentication allows for better horizontal scaling without session replication.\n\n2. **Standardized Security**: Using established security protocols (OAuth 2.0/OIDC) provides well-tested security patterns.\n\n3. **Reduced Development Burden**: Auth0 handles many security concerns (token issuance, validation, revocation, etc.).\n\n4. **Flexible Integration**: Easier to integrate with third-party systems and identity providers.\n\n5. **Centralized Policy Management**: Authorization policies can be managed centrally and applied consistently.\n\n6. **Enhanced User Experience**: Support for modern authentication patterns (social login, passwordless, etc.).\n\n7. **Compliance Support**: Built-in features for consent management, audit logging, and other compliance requirements.\n\n#### Negative\n\n1. **Increased Complexity**: More moving parts in the authentication flow compared to simple session-based auth.\n\n2. **Token Management Overhead**: Need to handle token lifecycle, refresh, and invalidation carefully.\n\n3. **External Dependency**: Reliance on Auth0 as a critical service and potential single point of failure.\n\n4. **Performance Considerations**: Token validation and policy evaluation add some latency to requests.\n\n5. **Cost**: Subscription fees for Auth0 based on active users and features used.\n\n6. **Learning Curve**: Team needs to understand OAuth 2.0 flows and token-based authentication patterns.\n\n### Mitigation Strategies\n\n1. **Caching and Performance Optimization**:\n   - Implement efficient token validation with local caching\n   - Use token introspection only when necessary\n   - Optimize policy evaluation paths\n\n2. **Resilience Planning**:\n   - Implement graceful degradation if Auth0 is temporarily unavailable\n   - Consider a multi-region Auth0 deployment for critical workloads\n   - Regular disaster recovery testing\n\n3. **Security Hardening**:\n   - Set appropriate token lifetimes (shorter for sensitive operations)\n   - Implement proper token storage in clients (secure cookies, encrypted storage)\n   - Regular security reviews and penetration testing\n\n4. **Developer Experience**:\n   - Create SDKs and libraries to abstract authentication complexity\n   - Comprehensive documentation and training\n   - Authentication dev sandbox environment\n\n### Implementation Details\n\n#### Phase 1: Core Authentication Infrastructure (Q3 2024)\n\n1. Set up Auth0 tenant and configure core settings\n2. Implement login, registration, and password reset flows\n3. Migrate existing user accounts (passwords securely hashed)\n4. Integrate with API Gateway for token validation\n5. Set up basic roles and permissions\n\n#### Phase 2: Advanced Authorization (Q4 2024)\n\n1. Implement fine-grained ABAC policies\n2. Develop centralized policy management tools\n3. Set up delegated administration capabilities\n4. Implement audit logging and monitoring\n5. Configure MFA for administrative accounts\n\n#### Phase 3: Partner and Integration Capabilities (Q1 2025)\n\n1. Implement client credentials for service-to-service\n2. Set up partner authentication portal\n3. Configure rate limiting and throttling\n4. Implement token exchange capabilities\n5. Set up public developer documentation\n\n### Considered Alternatives\n\n#### 1. Custom Authentication Solution\n\n**Pros**: Complete control, no external dependencies, potentially lower direct costs  \n**Cons**: Development time, security risks, maintenance burden, limited features\n\nBuilding our own solution would require significant security expertise and ongoing maintenance, with higher risk of vulnerabilities.\n\n#### 2. Keycloak (Open Source Identity Provider)\n\n**Pros**: Open source, flexible, no subscription fees, self-hosted control  \n**Cons**: Operational overhead, scalability challenges, fewer integrations\n\nKeycloak would reduce subscription costs but increase operational complexity and require more internal expertise.\n\n#### 3. AWS Cognito\n\n**Pros**: Tight AWS integration, managed service, good scalability  \n**Cons**: Less flexible than Auth0, fewer enterprise features, AWS lock-in\n\nCognito would be suitable if we were fully committed to AWS, but we needed more flexibility for our multi-cloud strategy.\n\n#### 4. Session-Based Authentication with Distributed Cache\n\n**Pros**: Simpler architecture, familiar pattern, less paradigm shift  \n**Cons**: Scalability challenges, higher operational complexity, less standardized\n\nThis would maintain our current approach but with scalability improvements, though it wouldn't address many of our requirements.\n\n### Standards Compliance\n\nOur implementation will comply with the following standards and regulations:\n\n1. **OAuth 2.0 (RFC 6749)**: The authorization framework\n2. **OpenID Connect 1.0**: Identity layer on top of OAuth 2.0\n3. **JWT (RFC 7519)**: Compact token format\n4. **GDPR**: For EU user data protection\n5. **PCI-DSS**: For payment-related authentication\n6. **NIST 800-63B**: Digital Identity Guidelines\n\n### References\n\n1. [OAuth 2.0 RFC 6749](https://tools.ietf.org/html/rfc6749)\n2. [OpenID Connect Core 1.0](https://openid.net/specs/openid-connect-core-1_0.html)\n3. [Auth0 Architecture Scenarios](https://auth0.com/docs/architecture-scenarios)\n4. [JWT Introduction](https://jwt.io/introduction)\n5. Nate Barbettini, \"OAuth 2.0 and OpenID Connect in Plain English\" ([YouTube](https://www.youtube.com/watch?v=996OiexHze0))\n6. \"API Security in Action\" by Neil Madden (Manning Publications)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-07-01 | 0.1 | Initial draft | Elena Rodriguez |\n| 2024-07-15 | 0.2 | Added implementation phases and alternatives | Michael Chang |\n| 2024-07-25 | 0.3 | Incorporated security team feedback | Sarah Chen |\n| 2024-08-01 | 1.0 | Approved by Architecture and Security Boards | Architecture Board |\n\n## Appendix A: Authentication Flows\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Client as Web/Mobile App\n    participant API as API Gateway\n    participant Auth0 as Auth0\n    participant Service as Microservice\n\n    %% Login Flow\n    User->>Client: Initiates Login\n    Client->>Auth0: Redirect to Auth0 (/authorize)\n    Auth0->>User: Present Login UI\n    User->>Auth0: Enter Credentials\n    opt MFA Triggered\n        Auth0->>User: Request Second Factor\n        User->>Auth0: Provide Second Factor\n    end\n    Auth0->>Client: Authorization Code\n    Client->>Auth0: Exchange Code for Tokens\n    Auth0->>Client: Access Token, ID Token, Refresh Token\n    \n    %% API Access Flow\n    Client->>API: Request with Access Token\n    API->>API: Validate Token\n    API->>Service: Forward Request + Claims\n    Service->>Service: Check Authorization\n    Service->>API: Response\n    API->>Client: Response\n    \n    %% Token Refresh\n    Note over Client: When Access Token Expires\n    Client->>Auth0: Request with Refresh Token\n    Auth0->>Client: New Access Token\n```\n\n## Appendix B: Authorization Model\n\n```mermaid\nflowchart TB\n    subgraph \"Authorization Layers\"\n        direction TB\n        \n        subgraph \"Layer 1: Authentication\"\n            auth[User Authentication]\n            token[Token Issuance]\n        end\n        \n        subgraph \"Layer 2: Coarse Authorization\"\n            rbac[Role-Based Access Control]\n            scopes[OAuth Scopes]\n        end\n        \n        subgraph \"Layer 3: Fine Authorization\"\n            abac[Attribute-Based Policies]\n            biz[Business Rules]\n        end\n    end\n    \n    subgraph \"Enforcement Points\"\n        gateway[API Gateway]\n        services[Microservices]\n    end\n    \n    auth --> token\n    token --> rbac\n    token --> scopes\n    rbac --> abac\n    scopes --> abac\n    abac --> biz\n    \n    rbac --> gateway\n    scopes --> gateway\n    abac --> gateway\n    biz --> services\n    \n    gateway --> services\n```\n\n## Appendix C: Role and Permission Mapping\n\n| Role | Description | Permissions | MFA Required |\n|------|-------------|------------|--------------|\n| Anonymous | Unauthenticated user | Browse catalog, View public content | No |\n| Customer | Registered shopper | Place orders, Manage profile, Write reviews | Optional |\n| Premium Customer | Paid tier customer | Customer + Early access, Special discounts | Optional |\n| Customer Service | Support staff | View orders, Issue refunds, Update customer info | Yes |\n| Store Manager | Retail location manager | Customer Service + Inventory management, Staff management | Yes |\n| Admin | System administrator | All permissions | Yes |\n| API Partner | External system | Specific API access based on agreement | No (mTLS) |\n| Service Account | Internal service | Specific service-to-service communication | No (mTLS) |",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/published/03-auth-strategy.mdx",
  "61108e1ba54202b6",
  "docs/technical-architecture-design/architecture-decision-records/published/05-frontend-architecture",
  {
    "id": 794,
    "data": 796,
    "body": 800,
    "filePath": 801,
    "digest": 802,
    "deferredRender": 20
  },
  { "title": 797, "summary": 798, "sidebar": 799 },
  "Frontend Architecture",
  "Architectural decision record for frontend architecture of the FlowMart e-commerce platform",
  { "label": 797, "order": 540 },
  "## ADR-005: Frontend Architecture for FlowMart E-commerce Platform\n\n### Status\n\nApproved (2024-09-05)\n\n### Context\n\nThe frontend of our current monolithic e-commerce application faces numerous challenges:\n\n1. **Performance Issues**: The current server-rendered application has slow page loads and poor mobile performance.\n2. **Developer Productivity**: Shared frontend codebase creates development bottlenecks and team dependencies.\n3. **Inconsistent User Experience**: Different parts of the application have divergent design patterns and interaction models.\n4. **Limited Reusability**: Components are tightly coupled to specific pages, making code reuse difficult.\n5. **Testing Challenges**: The current codebase has limited test coverage and is difficult to test effectively.\n6. **Technology Constraints**: Outdated technology stack limits our ability to leverage modern frontend capabilities.\n7. **Scalability Concerns**: Our current approach doesn't scale well with increasing development team size.\n8. **Mobile Experience**: The responsive web approach doesn't deliver optimal mobile experiences.\n\nAs we transition to a microservices backend architecture, we need a complementary frontend strategy that addresses these challenges while supporting our business goals of improved customer experience, faster time-to-market, and technical agility.\n\n### Decision\n\nWe will adopt a **modern, component-based frontend architecture** with the following key characteristics:\n\n1. **Micro-Frontend Approach**:\n   - Decompose the frontend into domain-aligned micro-frontends\n   - Enable independent development and deployment of frontend components\n   - Provide clear ownership boundaries aligned with backend microservices\n\n2. **Core Technology Stack**:\n   - **React**: Primary UI library for component development\n   - **Next.js**: Framework for server-rendering and static generation\n   - **TypeScript**: For type safety and improved developer experience\n   - **Styled Components**: For component-scoped styling\n   - **React Query**: For data fetching and state management\n   - **Cypress & React Testing Library**: For testing\n\n3. **Design System**:\n   - Create a comprehensive design system with reusable UI components\n   - Implement a living style guide and component documentation\n   - Establish design tokens for consistent theming\n   - Support multiple brands and white-labeling capabilities\n\n4. **Architecture Patterns**:\n   - **Module Federation**: For sharing components between micro-frontends\n   - **Composition Layer**: Shell application for integrating micro-frontends\n   - **BFF Pattern**: Backend-for-Frontend APIs for optimized data access\n   - **State Management**: Local state when possible, shared state when necessary\n   - **Feature Flags**: For controlled feature rollout and A/B testing\n\n5. **Performance Optimization**:\n   - Server-side rendering for initial page load performance\n   - Client-side rendering for rich interactive experiences\n   - Code splitting and lazy loading for optimized bundle sizes\n   - Aggressive caching strategies for static assets\n   - Optimized media delivery with responsive images and lazy loading\n\n6. **Mobile Strategy**:\n   - Progressive Web App (PWA) capabilities for mobile web\n   - Responsive design with mobile-first approach\n   - Native app shell with React Native for iOS/Android applications\n   - Shared business logic between web and native through abstraction layers\n\n### Frontend Domain Decomposition\n\n| Domain | Micro-Frontend | Primary Responsibilities | Team |\n|--------|---------------|--------------------------|------|\n| Product Discovery | product-browser | Product listing, search, filtering, recommendations | Catalog Team |\n| Product Details | product-details | Product information, options, reviews, related items | Catalog Team |\n| Shopping Cart | cart-experience | Cart management, saved items, quick checkout | Checkout Team |\n| Checkout | checkout-flow | Multi-step checkout, address management, payment | Checkout Team |\n| User Account | account-portal | Profile management, preferences, order history | Customer Team |\n| Content | content-pages | CMS-managed content, landing pages, promotional content | Marketing Team |\n| Store Locator | store-finder | Store search, maps integration, store details | Location Team |\n| Order Management | order-tracker | Order status, tracking, returns management | Order Team |\n\n### Consequences\n\n#### Positive\n\n1. **Improved Development Velocity**: Teams can work independently on their domains without blocking each other.\n\n2. **Better Performance**: Optimized loading strategies and modern frontend practices will improve user experience.\n\n3. **Enhanced Reusability**: Shared component library and design system enable consistent, reusable UI elements.\n\n4. **Independent Deployments**: Micro-frontends can be deployed independently, reducing release coordination.\n\n5. **Technology Flexibility**: Different domains can adopt new technologies at their own pace.\n\n6. **Better Testing**: Smaller, more focused codebases are easier to test thoroughly.\n\n7. **Improved User Experience**: Consistent design language and optimized interactions improve customer satisfaction.\n\n8. **Team Autonomy**: Clear ownership boundaries enable teams to take full responsibility for their domains.\n\n#### Negative\n\n1. **Increased Complexity**: Micro-frontend architectures add operational and integration complexity.\n\n2. **Learning Curve**: Teams need to adapt to new patterns and technologies.\n\n3. **Potential Duplication**: Without careful governance, similar solutions may be implemented multiple times.\n\n4. **Integration Challenges**: Ensuring consistent behavior across micro-frontends requires careful coordination.\n\n5. **Performance Overhead**: Micro-frontend composition can introduce additional runtime overhead if not carefully managed.\n\n6. **Increased Infrastructure Needs**: More sophisticated build, deployment, and monitoring infrastructure required.\n\n7. **Governance Challenges**: Balancing team autonomy with architectural consistency requires active governance.\n\n### Mitigation Strategies\n\n1. **Frontend Platform Team**:\n   - Create a dedicated platform team to provide shared infrastructure and tools\n   - Develop reusable patterns and documentation for micro-frontend implementation\n   - Provide developer tooling and simplified local development experience\n\n2. **Comprehensive Design System**:\n   - Invest in a robust design system with clear guidelines and components\n   - Create automated tools for design compliance checking\n   - Regular design system sessions to ensure alignment across teams\n\n3. **Performance Budgeting**:\n   - Establish clear performance metrics and budgets for each micro-frontend\n   - Automated performance testing in CI/CD pipeline\n   - Regular performance reviews and optimization workshops\n\n4. **Developer Experience**:\n   - Create standardized templates and generators for new micro-frontends\n   - Provide comprehensive documentation and internal training\n   - Establish frontend community of practice for knowledge sharing\n\n5. **Governance Model**:\n   - Create a frontend architecture council with representatives from each team\n   - Regular architecture reviews and pattern sharing\n   - Clear guidelines for when to share vs. create new components\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q4 2024)\n\n1. Create core design system and component library\n2. Establish micro-frontend shell architecture\n3. Develop initial build and deployment pipeline\n4. Implement authentication and session management\n5. Create developer documentation and examples\n\n#### Phase 2: Domain Migration (Q1-Q2 2025)\n\n1. Migrate high-priority domains to micro-frontend architecture\n2. Implement analytics and monitoring strategy\n3. Develop advanced patterns for cross-domain interaction\n4. Enhance performance optimization capabilities\n5. Create specialized mobile experiences\n\n#### Phase 3: Advanced Capabilities (Q3-Q4 2025)\n\n1. Implement personalization framework\n2. Develop advanced A/B testing capabilities\n3. Enhance internationalization and localization\n4. Create specialized native experiences\n5. Implement advanced analytics and behavior tracking\n\n### Considered Alternatives\n\n#### 1. Monolithic Single-Page Application (SPA)\n\n**Pros**: Simpler architecture, unified codebase, shared state management  \n**Cons**: Development bottlenecks, scaling challenges, larger bundle sizes\n\nThis approach would be simpler initially but would recreate many of our current scaling challenges.\n\n#### 2. Server-Side Rendering Only\n\n**Pros**: Simpler technology stack, better SEO by default, reduced client-side JavaScript  \n**Cons**: Limited interactivity, slower subsequent navigation, poorer offline capabilities\n\nWhile this would improve initial load performance, it would limit our ability to create rich interactive experiences.\n\n#### 3. Native Mobile Apps Only for Mobile\n\n**Pros**: Optimal mobile experience, full native capabilities, offline functionality  \n**Cons**: Development cost, platform duplication, release friction\n\nThis would deliver better mobile experiences but at significantly higher development and maintenance costs.\n\n#### 4. Framework-Agnostic Approach\n\n**Pros**: Maximum team autonomy, best-tool-for-job flexibility  \n**Cons**: Duplication of efforts, inconsistent experiences, integration challenges\n\nWhile offering maximum flexibility, this would lead to significant inconsistency and integration challenges.\n\n### References\n\n1. [Micro Frontends](https://martinfowler.com/articles/micro-frontends.html) (Martin Fowler)\n2. [Module Federation](https://webpack.js.org/concepts/module-federation/) (Webpack Documentation)\n3. [\"Building Micro-Frontends\" by Luca Mezzalira](https://www.oreilly.com/library/view/building-micro-frontends/9781492082989/)\n4. [Frontend Design Systems](https://designsystemsrepo.com/design-systems)\n5. [Atomic Design](https://atomicdesign.bradfrost.com/) by Brad Frost\n6. [React Documentation](https://reactjs.org/docs/getting-started.html)\n7. [Next.js Documentation](https://nextjs.org/docs)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-08-10 | 0.1 | Initial draft | Jennifer Lee |\n| 2024-08-20 | 0.2 | Added implementation phases and domain decomposition | Alex Johnson |\n| 2024-08-30 | 0.3 | Incorporated feedback from UX and frontend teams | Sarah Chen |\n| 2024-09-05 | 1.0 | Approved by Architecture and UX Boards | Architecture Board |\n\n## Appendix A: Frontend Architecture Overview\n\n```mermaid\nflowchart TB\n    subgraph \"Client Applications\"\n        web[Web Browser]\n        ios[iOS App]\n        android[Android App]\n    end\n    \n    subgraph \"Micro-Frontend Shell\"\n        shell[Shell Application]\n        router[Routing Layer]\n        auth[Auth Module]\n    end\n    \n    subgraph \"Micro-Frontends\"\n        product[Product Experience]\n        cart[Cart & Checkout]\n        account[User Account]\n        content[Content Pages]\n        order[Order Management]\n    end\n    \n    subgraph \"Shared Foundation\"\n        design[Design System]\n        core[Core Components]\n        utils[Utility Functions]\n        hooks[React Hooks]\n    end\n    \n    subgraph \"Backend Integration\"\n        bff[BFF Layer]\n        api[API Gateway]\n    end\n    \n    web --> shell\n    ios --> shell\n    android --> shell\n    \n    shell --> router\n    shell --> auth\n    \n    router --> product\n    router --> cart\n    router --> account\n    router --> content\n    router --> order\n    \n    product --> design\n    cart --> design\n    account --> design\n    content --> design\n    order --> design\n    \n    product --> core\n    cart --> core\n    account --> core\n    content --> core\n    order --> core\n    \n    product --> bff\n    cart --> bff\n    account --> bff\n    content --> bff\n    order --> bff\n    \n    bff --> api\n```\n\n## Appendix B: Component Development Workflow\n\n```mermaid\nsequenceDiagram\n    participant Designer\n    participant Dev as Developer\n    participant PR as Pull Request\n    participant CI as CI/CD Pipeline\n    participant DS as Design System\n    participant App as Application\n    \n    Designer->>Designer: Create Component Design\n    Designer->>Dev: Handoff Design Specs\n    \n    Dev->>Dev: Develop Component\n    Dev->>Dev: Write Component Tests\n    Dev->>Dev: Document Component\n    \n    Dev->>PR: Submit Pull Request\n    PR->>CI: Trigger CI Pipeline\n    \n    CI->>CI: Lint Check\n    CI->>CI: Type Check\n    CI->>CI: Unit Tests\n    CI->>CI: Visual Regression Tests\n    CI->>CI: Bundle Size Analysis\n    CI->>CI: Accessibility Tests\n    \n    CI->>PR: Report Results\n    PR->>DS: Merge to Design System\n    \n    DS->>App: Component Available for Use\n    App->>App: Integrate Component\n```\n\n## Appendix C: Micro-Frontend Integration Patterns\n\n```mermaid\nflowchart TB\n    subgraph \"Integration Approaches\"\n        direction TB\n        \n        subgraph \"Build-Time Integration\"\n            npm[NPM Dependencies]\n            mono[Monorepo Packages]\n        end\n        \n        subgraph \"Run-Time Integration\"\n            fed[Module Federation]\n            iframes[IFrames]\n            web[Web Components]\n        end\n        \n        subgraph \"Edge-Side Integration\"\n            esi[Edge Side Includes]\n            ssi[Server Side Includes]\n        end\n    end\n    \n    subgraph \"Implementation Patterns\"\n        direction TB\n        \n        subgraph \"Horizontal Split\"\n            domains[By Domain/Business Function]\n        end\n        \n        subgraph \"Vertical Split\"\n            pages[Page-Based Composition]\n        end\n        \n        subgraph \"Hybrid Approach\"\n            shell[Shell + Domain Modules]\n        end\n    end\n    \n    fed --> shell\n    domains --> fed\n    pages --> iframes\n    shell --> web\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/published/05-frontend-architecture.mdx",
  "cd4e37cf6a0fc54b",
  "docs/technical-architecture-design/architecture-decision-records/published/06-observability-strategy",
  {
    "id": 803,
    "data": 805,
    "body": 809,
    "filePath": 810,
    "digest": 811,
    "deferredRender": 20
  },
  { "title": 806, "summary": 807, "sidebar": 808 },
  "Observability Strategy",
  "Architectural decision record for implementing observability in the FlowMart e-commerce platform",
  { "label": 806, "order": 550 },
  "## ADR-006: Observability Strategy for FlowMart E-commerce Platform\n\n### Status\n\nApproved (2024-09-15)\n\n### Context\n\nAs we transition from a monolithic architecture to a distributed microservices-based e-commerce platform, traditional monitoring approaches are no longer sufficient. The increased complexity of our architecture introduces several challenges:\n\n1. **Distributed Systems Complexity**: With dozens of microservices communicating asynchronously, understanding system behavior becomes significantly more difficult.\n\n2. **Increased Failure Points**: A distributed architecture introduces more potential failure points and complex failure modes.\n\n3. **Service Interdependencies**: Issues in one service can cascade to others, making root cause analysis challenging.\n\n4. **Multiple Technologies**: Different services use different languages, frameworks, and datastores, requiring diverse instrumentation approaches.\n\n5. **Deployment Frequency**: With continuous deployment across multiple services, correlating issues with specific changes becomes more complex.\n\n6. **Performance Bottlenecks**: Identifying performance bottlenecks in a distributed system requires end-to-end visibility.\n\n7. **Cross-Team Collaboration**: Multiple teams own different services, requiring a common observability approach and shared understanding.\n\n8. **Business Impact Correlation**: Need to connect technical metrics with business outcomes to prioritize improvements.\n\nOur current monitoring strategy is primarily focused on infrastructure metrics and basic application health checks, which is insufficient for effectively operating our new architecture.\n\n### Decision\n\nWe will implement a comprehensive **observability strategy** based on the \"three pillars\" approach (metrics, logs, and traces) with distributed tracing as a foundational element. Key components of this strategy include:\n\n1. **Observability Stack**:\n   - **Metrics**: Prometheus for metrics collection and alerting\n   - **Logs**: Elasticsearch, Logstash, and Kibana (ELK) for log aggregation and analysis\n   - **Traces**: Jaeger for distributed tracing\n   - **Dashboard**: Grafana for unified visualization and dashboarding\n   - **Alerting**: Prometheus Alertmanager with PagerDuty integration\n\n2. **Instrumentation Standards**:\n   - **Distributed Tracing**: OpenTelemetry as the standard instrumentation framework\n   - **Structured Logging**: JSON-formatted logs with standardized fields across all services\n   - **Metrics Naming**: Consistent metrics naming convention following Prometheus best practices\n   - **Service Level Objectives (SLOs)**: Defined for all critical user journeys\n   - **Error Budgets**: Established for each service and user journey\n\n3. **Core Observability Capabilities**:\n   - **Request Tracing**: End-to-end tracing for all user-initiated actions\n   - **Dependency Monitoring**: Monitoring of all external dependencies and services\n   - **Business Metrics**: Tracking of key business metrics alongside technical metrics\n   - **Synthetic Monitoring**: Regular testing of critical user journeys\n   - **Real User Monitoring (RUM)**: Frontend performance and error tracking\n   - **Anomaly Detection**: Automated detection of abnormal system behavior\n   - **Correlation Engine**: Tools to correlate metrics, logs, and traces during investigation\n\n4. **Data Retention and Sampling**:\n   - Critical business transaction traces retained for 30 days\n   - High-cardinality metrics sampled at appropriate rates\n   - Error logs retained for 90 days\n   - Regular logs retained for 30 days\n   - Aggregated metrics retained for 13 months for year-over-year analysis\n\n5. **Implementation Approach**:\n   - Platform team creates and maintains observability infrastructure\n   - Standardized libraries and SDKs for each supported language/framework\n   - Observability as code, with instrumentation verified in CI/CD pipelines\n   - Service templates with pre-configured observability components\n   - Progressive enhancement of observability capabilities\n\n### Observability Requirements by Domain\n\n| Domain | Key Metrics | Special Requirements | SLO Targets |\n|--------|------------|----------------------|-------------|\n| Product Catalog | Search latency, Cache hit rate | High cardinality data handling | 99.9% availability, p95 \u003C 300ms |\n| Order Processing | Order volume, Processing time, Error rate | Comprehensive transaction tracing | 99.95% availability, p95 \u003C 500ms |\n| Payment | Transaction volume, Success rate, Fraud detection rate | PCI compliance in logging | 99.99% availability, p95 \u003C 800ms |\n| Inventory | Stock level changes, Reservation rate, Stockout events | Event-sourcing visibility | 99.9% availability, p95 \u003C 400ms |\n| User Authentication | Login volume, Success rate, MFA usage | Security-focused monitoring | 99.99% availability, p95 \u003C 250ms |\n| Checkout | Cart conversion rate, Abandonment points, Session duration | User journey analysis | 99.95% availability, p95 \u003C 600ms |\n| Shipping | Fulfillment time, Carrier performance, Tracking accuracy | Third-party integration monitoring | 99.9% availability, p95 \u003C 350ms |\n| Content Delivery | Cache hit ratio, Origin fetch time, Asset size | CDN performance visibility | 99.9% availability, p95 \u003C 200ms |\n\n### Consequences\n\n#### Positive\n\n1. **Improved Troubleshooting**: Faster identification and resolution of issues through correlated observability data.\n\n2. **Proactive Detection**: Ability to detect potential issues before they impact users through anomaly detection and trend analysis.\n\n3. **Enhanced Understanding**: Better understanding of system behavior, dependencies, and performance characteristics.\n\n4. **Data-Driven Optimization**: Ability to make targeted performance improvements based on actual usage patterns.\n\n5. **Cross-Team Collaboration**: Common observability platform enables better collaboration during incident response.\n\n6. **Business Alignment**: Correlation between technical metrics and business outcomes helps prioritize technical work.\n\n7. **Resilience Verification**: Ability to verify that resilience mechanisms (circuit breakers, retries, etc.) function properly.\n\n8. **Capacity Planning**: Better data for capacity planning and scaling decisions.\n\n#### Negative\n\n1. **Implementation Overhead**: Adding comprehensive instrumentation requires additional development effort.\n\n2. **Data Volume Challenges**: Managing the volume of observability data requires careful planning and potential sampling.\n\n3. **Performance Impact**: Instrumentation adds some overhead to application performance, which must be managed.\n\n4. **Complexity**: A sophisticated observability stack adds operational complexity and maintenance requirements.\n\n5. **Learning Curve**: Teams need to learn new tools, concepts, and practices for effective use of observability data.\n\n6. **Cost Considerations**: Storage and processing of observability data has significant cost implications at scale.\n\n7. **Privacy and Security**: Observability data may contain sensitive information requiring appropriate controls.\n\n### Mitigation Strategies\n\n1. **Automated Instrumentation**:\n   - Use auto-instrumentation agents where possible\n   - Create starter templates with instrumentation pre-configured\n   - Build instrumentation verification into CI/CD pipelines\n\n2. **Data Management**:\n   - Implement appropriate sampling strategies for high-volume data\n   - Utilize data compression and aggregation techniques\n   - Define appropriate retention policies based on data criticality\n\n3. **Operating Model**:\n   - Create an observability platform team to manage the core infrastructure\n   - Establish observability champions within each service team\n   - Regular observability review and enhancement sessions\n\n4. **Knowledge Sharing**:\n   - Comprehensive documentation and training on observability tools\n   - Regular workshops on effective use of observability data\n   - Shared dashboards and runbooks for common scenarios\n\n5. **Security and Privacy**:\n   - Automated PII detection and redaction in logs and traces\n   - Role-based access control for observability data\n   - Regular audits of observability data for sensitive information\n\n### Implementation Details\n\n#### Phase 1: Foundation (Q4 2024)\n\n1. Deploy core observability infrastructure (Prometheus, ELK, Jaeger, Grafana)\n2. Implement standardized logging format and collection pipeline\n3. Create initial service dashboards and alerting\n4. Develop instrumentation libraries for primary service frameworks\n5. Establish basic SLOs for critical services\n\n#### Phase 2: Enhanced Capabilities (Q1 2025)\n\n1. Implement distributed tracing across all critical user journeys\n2. Create business metrics dashboards correlated with technical metrics\n3. Develop anomaly detection for key system behaviors\n4. Implement synthetic monitoring for critical paths\n5. Create runbooks integrated with observability tools\n\n#### Phase 3: Advanced Observability (Q2-Q3 2025)\n\n1. Implement ML-based anomaly detection and prediction\n2. Create self-service observability platform capabilities\n3. Develop advanced correlation between metrics, logs, and traces\n4. Implement automated performance testing with observability verification\n5. Develop capacity planning and forecasting based on observability data\n\n### Considered Alternatives\n\n#### 1. Commercial APM Solution Only (e.g., Dynatrace, New Relic)\n\n**Pros**: Comprehensive out-of-the-box capabilities, reduced implementation effort, integrated platform  \n**Cons**: High cost at scale, reduced flexibility, potential vendor lock-in\n\nWhile commercial APM tools provide excellent capabilities, we chose an open-source approach for cost flexibility and customization ability. We will reevaluate this decision as our needs evolve.\n\n#### 2. Minimal Custom Instrumentation\n\n**Pros**: Reduced development overhead, simplicity, lower initial investment  \n**Cons**: Limited visibility, reactive troubleshooting, challenges scaling observability with system growth\n\nThis approach would not provide the depth of insight needed for effective operation of our distributed system.\n\n#### 3. Service Mesh-Based Observability\n\n**Pros**: Reduced application instrumentation, consistent approach, network-level visibility  \n**Cons**: Limited application-level context, additional infrastructure complexity, potential performance impact\n\nWhile we will leverage service mesh observability capabilities, we need application-level instrumentation for complete visibility.\n\n#### 4. Multiple Independent Monitoring Systems\n\n**Pros**: Specialized tools for each domain, team autonomy in tooling decisions  \n**Cons**: Fragmented visibility, integration challenges, inconsistent practices\n\nThis approach would create silos and make cross-service troubleshooting significantly more difficult.\n\n### References\n\n1. Charity Majors, Liz Fong-Jones, George Miranda, \"Observability Engineering\" (O'Reilly)\n2. Cindy Sridharan, \"Distributed Systems Observability\" (O'Reilly)\n3. [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n4. [Google SRE Book - Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)\n5. [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n6. [Grafana Observability Strategy](https://grafana.com/blog/2019/10/21/whats-next-for-observability/)\n\n### Decision Record History\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2024-08-20 | 0.1 | Initial draft | Kevin Zhang |\n| 2024-09-01 | 0.2 | Added implementation phases and domain details | Rachel Williams |\n| 2024-09-10 | 0.3 | Incorporated feedback from SRE and platform teams | David Boyne |\n| 2024-09-15 | 1.0 | Approved by Architecture and Operations Boards | Architecture Board |\n\n## Appendix A: Observability Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Data Sources\"\n        apps[Applications]\n        infra[Infrastructure]\n        db[Databases]\n        net[Network]\n        fe[Frontend]\n    end\n    \n    subgraph \"Collection Layer\"\n        prom[Prometheus]\n        fluent[FluentBit]\n        otel[OpenTelemetry Collector]\n        beats[Beats]\n    end\n    \n    subgraph \"Storage Layer\"\n        tsdb[Prometheus TSDB]\n        es[Elasticsearch]\n        jaeger_db[Jaeger Storage]\n        loki[Loki]\n    end\n    \n    subgraph \"Processing Layer\"\n        alerts[Alertmanager]\n        anom[Anomaly Detection]\n        corr[Correlation Engine]\n        agg[Log Aggregation]\n    end\n    \n    subgraph \"Visualization Layer\"\n        grafana[Grafana]\n        kibana[Kibana]\n        jaeger_ui[Jaeger UI]\n        dashboards[Custom Dashboards]\n    end\n    \n    subgraph \"Notification Layer\"\n        pd[PagerDuty]\n        slack[Slack]\n        email[Email]\n        webhook[Webhooks]\n    end\n    \n    apps --> otel\n    apps --> fluent\n    apps --> prom\n    infra --> beats\n    infra --> prom\n    db --> prom\n    db --> beats\n    net --> prom\n    net --> beats\n    fe --> otel\n    \n    otel --> jaeger_db\n    otel --> loki\n    otel --> tsdb\n    fluent --> es\n    beats --> es\n    prom --> tsdb\n    \n    tsdb --> alerts\n    tsdb --> anom\n    es --> agg\n    es --> corr\n    jaeger_db --> corr\n    loki --> corr\n    \n    tsdb --> grafana\n    es --> kibana\n    es --> grafana\n    jaeger_db --> jaeger_ui\n    jaeger_db --> grafana\n    corr --> dashboards\n    agg --> dashboards\n    \n    alerts --> pd\n    alerts --> slack\n    alerts --> email\n    alerts --> webhook\n```\n\n## Appendix B: Observability Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Service1 as Service A\n    participant Service2 as Service B\n    participant Service3 as Service C\n    participant OTel as OpenTelemetry\n    participant Logs as Log System\n    participant Metrics as Metrics System\n    participant Traces as Tracing System\n    participant Dashboard as Dashboard\n    participant Alerts as Alert System\n    \n    User->>Service1: Request\n    \n    Service1->>OTel: Generate Span 1\n    Service1->>Logs: Log Request Details\n    Service1->>Metrics: Update Request Counter\n    \n    Service1->>Service2: Internal Request\n    Service2->>OTel: Generate Span 2 (child of Span 1)\n    Service2->>Logs: Log Processing Details\n    Service2->>Metrics: Update Processing Metrics\n    \n    Service2->>Service3: Database Query\n    Service3->>OTel: Generate Span 3 (child of Span 2)\n    Service3->>Logs: Log Query Details\n    Service3->>Metrics: Update Query Metrics\n    \n    Service3-->>Service2: Query Result\n    Service2-->>Service1: Internal Response\n    Service1-->>User: Response\n    \n    Service1->>OTel: Complete Span 1\n    Service1->>Metrics: Update Response Time\n    \n    OTel->>Traces: Store Complete Trace\n    \n    Logs->>Dashboard: Visualize Logs\n    Metrics->>Dashboard: Visualize Metrics\n    Traces->>Dashboard: Visualize Traces\n    \n    Metrics->>Alerts: Trigger Alerts (if threshold exceeded)\n    Alerts->>Dashboard: Display Alert Status\n```\n\n## Appendix C: Service Level Objectives (SLOs) Framework\n\n```mermaid\nflowchart TB\n    subgraph \"SLO Definition Process\"\n        direction TB\n        \n        subgraph \"1. Identify Critical User Journeys\"\n            journey1[Product Search]\n            journey2[Add to Cart]\n            journey3[Checkout]\n            journey4[Order Status]\n        end\n        \n        subgraph \"2. Define Service Level Indicators (SLIs)\"\n            availability[Availability %]\n            latency[Latency (p50, p95, p99)]\n            errors[Error Rate %]\n            saturation[Resource Saturation]\n        end\n        \n        subgraph \"3. Set Target Objectives\"\n            slo1[99.9% Availability]\n            slo2[p95 \u003C 300ms]\n            slo3[Error Rate \u003C 0.1%]\n        end\n        \n        subgraph \"4. Establish Error Budgets\"\n            monthly[Monthly Budget]\n            quarterly[Quarterly Budget]\n            policy[Error Budget Policy]\n        end\n    end\n    \n    subgraph \"SLO Monitoring & Reporting\"\n        direction TB\n        \n        subgraph \"Real-time Dashboards\"\n            current[Current Status]\n            trends[Burn Rate]\n            history[Historical Performance]\n        end\n        \n        subgraph \"Alerting Strategy\"\n            warning[Budget Warning (50%)]\n            critical[Budget Critical (75%)]\n            depleted[Budget Depleted (90%)]\n        end\n        \n        subgraph \"Continuous Improvement\"\n            retro[SLO Reviews]\n            adjust[Target Adjustments]\n            prioritize[Reliability Work]\n        end\n    end\n    \n    journey1 --> availability\n    journey1 --> latency\n    journey2 --> availability\n    journey2 --> latency\n    journey3 --> errors\n    journey3 --> availability\n    journey4 --> latency\n    \n    availability --> slo1\n    latency --> slo2\n    errors --> slo3\n    \n    slo1 --> monthly\n    slo2 --> monthly\n    slo3 --> monthly\n    \n    monthly --> current\n    monthly --> warning\n    \n    current --> retro\n    warning --> prioritize\n```",
  "../examples/default/docs/technical-architecture-design/architecture-decision-records/published/06-observability-strategy.mdx",
  "60618cb16e2921f8",
  "domains",
  ["Map", 814, 815, 840, 841, 883, 884, 901, 902, 933, 934, 949, 950],
  "Payment-0.0.1",
  {
    "id": 814,
    "data": 816,
    "body": 837,
    "filePath": 838,
    "digest": 839,
    "deferredRender": 20
  },
  {
    "services": 817,
    "entities": 820,
    "id": 824,
    "name": 824,
    "summary": 829,
    "version": 830,
    "badges": 831,
    "owners": 835
  },
  [818],
  { "id": 604, "version": 819 },
  "latest",
  [821, 823, 825, 827],
  { "id": 822, "version": 819 },
  "Invoice",
  { "id": 824, "version": 819 },
  "Payment",
  { "id": 826, "version": 819 },
  "PaymentMethod",
  { "id": 828, "version": 819 },
  "Transaction",
  "Domain that contains payment related services and messages.\n",
  "0.0.1",
  [832],
  { "content": 833, "backgroundColor": 566, "textColor": 566, "icon": 834 },
  "Subdomain",
  "BoltIcon",
  [836],
  { "id": 42 },
  "## Overview\n\nThe Payment Domain encompasses all services and components related to handling financial transactions within the system. It is responsible for managing payments, transactions, billing, and financial records. The domain ensures secure, reliable, and efficient processing of all payment-related activities\n\n## Bounded context\n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} />",
  "../examples/default/domains/E-Commerce/subdomains/Payment/index.mdx",
  "1d8013dca1ff4413",
  "E-Commerce-1.0.0",
  {
    "id": 840,
    "data": 842,
    "body": 880,
    "filePath": 881,
    "digest": 882,
    "deferredRender": 20
  },
  {
    "domains": 843,
    "id": 851,
    "name": 851,
    "version": 852,
    "badges": 853,
    "owners": 860,
    "resourceGroups": 863
  },
  [844, 846, 847, 849],
  { "id": 845, "version": 819 },
  "Orders",
  { "id": 824, "version": 819 },
  { "id": 848, "version": 819 },
  "Subscription",
  { "id": 850, "version": 819 },
  "MySubdomain",
  "E-Commerce",
  "1.0.0",
  [854, 857],
  { "content": 855, "backgroundColor": 566, "textColor": 566, "icon": 856 },
  "Core domain",
  "RectangleGroupIcon",
  { "content": 858, "backgroundColor": 573, "textColor": 573, "icon": 859 },
  "Business Critical",
  "ShieldCheckIcon",
  [861, 862],
  { "id": 42 },
  { "id": 254 },
  [864],
  { "id": 865, "title": 866, "items": 867, "limit": 879, "sidebar": 20 },
  "related-resources",
  "Core FlowMart Services",
  [868, 870, 871, 873, 874, 876, 877],
  { "id": 560, "version": 819, "type": 869 },
  "service",
  { "id": 594, "version": 819, "type": 869 },
  { "id": 872, "version": 819, "type": 869 },
  "NotificationService",
  { "id": 624, "version": 819, "type": 869 },
  { "id": 875, "version": 819, "type": 869 },
  "CustomerService",
  { "id": 604, "version": 819, "type": 869 },
  { "id": 878, "version": 819, "type": 869 },
  "AnalyticsService",
  10,
  "import Footer from '@catalog/components/footer.astro';\n\nThe E-Commerce domain is the core business domain of FlowMart, our modern digital marketplace. This domain orchestrates all critical business operations from product discovery to order fulfillment, handling millions of transactions monthly across our global customer base.\n\n\u003CTiles>\n    \u003CTile \n        icon=\"UserGroupIcon\" \n        href=\"/docs/teams/full-stack\" \n        title=\"Engineering Support\" \n        description=\"Questions? Contact our full-stack team for technical support\" \n    />\n    \u003CTile \n        icon=\"RectangleGroupIcon\" \n        href={`/visualiser/domains/${frontmatter.id}/${frontmatter.version}`} \n        title=\"Domain Architecture\" \n        description=\"Explore our domain structure and service interactions\" \n    />\n\u003C/Tiles>\n\n## Domain Overview\n\nThe E-Commerce domain encapsulates all the core business logic for the FlowMart e-commerce platform. It is built on event-driven microservices architecture.\n\n\u003CNodeGraph  />\n\nFlowMart's E-Commerce domain is built on event-driven microservices architecture, enabling:\n- Real-time inventory management across multiple warehouses\n- Seamless payment processing with multiple providers\n- Smart order routing and fulfillment\n- Personalized customer notifications\n- Subscription-based shopping experiences\n- Advanced fraud detection and prevention\n\n## Core Domains for E-Commerce\n\nThe \u003CResourceLink id=\"Orders\" type=\"domain\">Orders\u003C/ResourceLink> and \u003CResourceLink id=\"Subscription\" type=\"domain\">Subscription\u003C/ResourceLink> domains are core domains for the E-Commerce domain.\n\n\u003Cspan class=\"not-prose\">They are used to manage the orders and subscriptions for the E-Commerce domain.\u003C/span>\n\n\u003Cdiv class=\"grid grid-cols-2 gap-4 not-prose\">\n  \u003CNodeGraph id=\"Orders\" version=\"0.0.3\" type=\"domain\" />\n  \u003CNodeGraph id=\"Subscription\" version=\"0.0.1\" type=\"domain\" />\n\u003C/div>\n\nThe E-Commerce domain is built on the following sub domains:\n\n- \u003CResourceLink id=\"Orders\" type=\"domain\">Orders\u003C/ResourceLink> - Core domain for order management\n- \u003CResourceLink id=\"Payment\" type=\"domain\">Payment\u003C/ResourceLink> - A generic domain for payment processing using Stripe as a payment provider\n- \u003CResourceLink id=\"Subscription\" type=\"domain\">Subscription\u003C/ResourceLink> - Generic subscription domain handling users subscriptions\n\n\n## Target Architecture (Event Storming Results)\n\nOur target architecture was defined through collaborative event storming sessions with product, engineering, and business stakeholders. This represents our vision for FlowMart's commerce capabilities.\n\n\u003CMiro boardId=\"uXjVIHCImos=/\" moveToWidget=\"3074457347671667709\" edit={false} />\n\n\n### Order Processing Flow\n\n```mermaid\nsequenceDiagram\n    participant Customer\n    participant OrdersService\n    participant InventoryService\n    participant PaymentService\n    participant NotificationService\n    participant ShippingService\n\n    Customer->>OrdersService: Place Order\n    OrdersService->>InventoryService: Check Stock Availability\n    InventoryService-->>OrdersService: Stock Confirmed\n    OrdersService->>PaymentService: Process Payment\n    PaymentService-->>OrdersService: Payment Successful\n    OrdersService->>InventoryService: Reserve Inventory\n    OrdersService->>ShippingService: Create Shipping Label\n    ShippingService-->>OrdersService: Shipping Label Generated\n    OrdersService->>NotificationService: Send Order Confirmation\n    NotificationService-->>Customer: Order & Tracking Details\n```\n\n## Key Business Flows\n\n### Subscription Management\nOur subscription service powers FlowMart's popular \"Subscribe & Save\" feature:\n\n\u003CFlow id=\"CancelSubscription\" version=\"latest\" includeKey={false} />\n\n### Payment Processing\nSecure, multi-provider payment processing with fraud detection:\n\n\u003CFlow id=\"PaymentFlow\" version=\"latest\" includeKey={false} />\n\n## Core Services\n\nThese services form the backbone of FlowMart's e-commerce operations:\n\n\u003CResourceGroupTable \n    id=\"related-resources\" \n    limit={7} \n    showOwners={true} \n    description=\"Essential services powering our e-commerce platform\" \n/>\n\n## Performance SLAs\n\n- Order Processing: \u003C 2 seconds\n- Payment Processing: \u003C 3 seconds\n- Inventory Updates: Real-time\n- Notification Delivery: \u003C 30 seconds\n\n## Monitoring & Alerts\n\n- Real-time order volume monitoring\n- Payment gateway health checks\n- Inventory level alerts\n- Customer experience metrics\n- System performance dashboards\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/index.mdx",
  "eb51cb6c9ca73a4b",
  "Subscription-0.0.1",
  {
    "id": 883,
    "data": 885,
    "body": 837,
    "filePath": 899,
    "digest": 900,
    "deferredRender": 20
  },
  {
    "services": 886,
    "entities": 889,
    "id": 848,
    "name": 848,
    "summary": 894,
    "version": 830,
    "badges": 895,
    "owners": 897
  },
  [887],
  { "id": 888, "version": 819 },
  "SubscriptionService",
  [890, 892],
  { "id": 891, "version": 819 },
  "BillingProfile",
  { "id": 893, "version": 819 },
  "SubscriptionPeriod",
  "Domain that contains subscription related services and messages.\n",
  [896],
  { "content": 833, "backgroundColor": 566, "textColor": 566 },
  [898],
  { "id": 283 },
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/index.mdx",
  "b4b5bcc7e54bbaea",
  "Orders-0.0.3",
  {
    "id": 901,
    "data": 903,
    "body": 930,
    "filePath": 931,
    "digest": 932,
    "deferredRender": 20
  },
  {
    "services": 904,
    "entities": 909,
    "id": 845,
    "name": 845,
    "version": 916,
    "badges": 917,
    "owners": 919,
    "resourceGroups": 922
  },
  [905, 906, 907, 908],
  { "id": 560, "version": 819 },
  { "id": 594, "version": 819 },
  { "id": 872, "version": 819 },
  { "id": 624, "version": 819 },
  [910, 912, 914],
  { "id": 911, "version": 819 },
  "Order",
  { "id": 913, "version": 819 },
  "OrderItem",
  { "id": 915, "version": 819 },
  "Customer",
  "0.0.3",
  [918],
  { "content": 833, "backgroundColor": 566, "textColor": 566, "icon": 856 },
  [920, 921],
  { "id": 42 },
  { "id": 254 },
  [923],
  { "id": 865, "title": 924, "items": 925, "limit": 879, "sidebar": 20 },
  "Core resources",
  [926, 927, 928, 929],
  { "id": 560, "version": 819, "type": 869 },
  { "id": 594, "version": 819, "type": 869 },
  { "id": 872, "version": 819, "type": 869 },
  { "id": 624, "version": 819, "type": 869 },
  "import Footer from '@catalog/components/footer.astro';\n\n\n\n:::warning\n\nPlease ensure all services are **updated** to the latest version for compatibility and performance improvements.\n:::\n\nThe Orders domain handles all operations related to customer orders, from creation to fulfillment. This documentation provides an overview of the events and services involved in the Orders domain, helping developers and stakeholders understand the event-driven architecture\n\n\u003CTiles >\n    \u003CTile icon=\"UserGroupIcon\" href=\"/docs/teams/full-stack\" title=\"Contact the team\" description=\"Any questions? Feel free to contact the owners\" />\n    \u003CTile icon=\"RectangleGroupIcon\" href={`/visualiser/domains/${frontmatter.id}/${frontmatter.version}`} title={`${frontmatter.services.length} services`} description=\"This domain contains the following services.\" />\n\u003C/Tiles>\n\n### Architecture for the Orders domain\n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} showChannels={true} title=\"Messages in/out of the domain\" />\n\n### Order example (sequence diagram)\n\n```mermaid\nsequenceDiagram\n    participant Customer\n    participant OrdersService\n    participant InventoryService\n    participant NotificationService\n\n    Customer->>OrdersService: Place Order\n    OrdersService->>InventoryService: Check Inventory\n    InventoryService-->>OrdersService: Inventory Available\n    OrdersService->>InventoryService: Reserve Inventory\n    OrdersService->>NotificationService: Send Order Confirmation\n    NotificationService-->>Customer: Order Confirmation\n    OrdersService->>Customer: Order Placed Successfully\n    OrdersService->>InventoryService: Update Inventory\n```\n\n## Flows\n\n### Cancel Subscription flow\nDocumented flow when a user cancels their subscription.\n\n\u003CFlow id=\"CancelSubscription\" version=\"latest\" includeKey={false} />\n\n### Payment processing flow\nDocumented flow when a user makes a payment within the order domain\n\n\u003CFlow id=\"PaymentFlow\" version=\"latest\" includeKey={false} />\n\n\u003CResourceGroupTable id=\"related-resources\" limit={4} showOwners={true} title=\"Core resources for the Orders domain\" description=\"Resources that are related to the Orders domain, you may find them useful\" />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/index.mdx",
  "807e56f5425bd33f",
  "Orders-0.0.2",
  {
    "id": 933,
    "data": 935,
    "body": 946,
    "filePath": 947,
    "digest": 948,
    "deferredRender": 20
  },
  {
    "services": 936,
    "id": 845,
    "name": 845,
    "version": 938,
    "badges": 941,
    "owners": 944
  },
  [937, 939, 940],
  { "id": 560, "version": 938 },
  "0.0.2",
  { "id": 872, "version": 938 },
  { "id": 594, "version": 938 },
  [942],
  { "content": 943, "backgroundColor": 566, "textColor": 566 },
  "New domain",
  [945],
  { "id": 42 },
  "## Overview\n\nThe Orders domain handles all operations related to customer orders, from creation to fulfillment. This documentation provides an overview of the events and services involved in the Orders domain, helping developers and stakeholders understand the event-driven architecture.\n\n:::warning\nPlease ensure all services are updated to the latest version for compatibility and performance improvements.\n:::\n\n## Bounded context\n\n\u003CNodeGraph />\n\n### Order example (sequence diagram)\n\n```mermaid\nsequenceDiagram\n    participant Customer\n    participant OrdersService\n    participant InventoryService\n    participant NotificationService\n\n    Customer->>OrdersService: Place Order\n    OrdersService->>InventoryService: Check Inventory\n    InventoryService-->>OrdersService: Inventory Available\n    OrdersService->>InventoryService: Reserve Inventory\n    OrdersService->>NotificationService: Send Order Confirmation\n    NotificationService-->>Customer: Order Confirmation\n    OrdersService->>Customer: Order Placed Successfully\n    OrdersService->>InventoryService: Update Inventory\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/versioned/0.0.2/index.mdx",
  "b395d6dd1442b98c",
  "Orders-0.0.1",
  {
    "id": 949,
    "data": 951,
    "body": 960,
    "filePath": 961,
    "digest": 962,
    "deferredRender": 20
  },
  {
    "services": 952,
    "id": 845,
    "name": 845,
    "summary": 954,
    "version": 830,
    "badges": 955,
    "owners": 957
  },
  [953],
  { "id": 560, "version": 938 },
  "Domain for everything shopping\n",
  [956],
  { "content": 943, "backgroundColor": 566, "textColor": 566 },
  [958, 959],
  { "id": 42 },
  { "id": 254 },
  "## Overview\n\n\u003CNodeGraph />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/versioned/0.0.1/index.mdx",
  "92e1f9f0299b99f8",
  "services",
  [
    "Map",
    965,
    966,
    1007,
    1008,
    1034,
    1035,
    1063,
    1064,
    1097,
    1098,
    1119,
    1120,
    1142,
    1143,
    1157,
    1158
  ],
  "OrdersService-0.0.3",
  {
    "id": 965,
    "data": 967,
    "body": 1004,
    "filePath": 1005,
    "digest": 1006,
    "deferredRender": 20
  },
  {
    "sends": 968,
    "receives": 977,
    "id": 594,
    "name": 986,
    "summary": 987,
    "version": 916,
    "owners": 988,
    "schemaPath": 990,
    "repository": 991,
    "specifications": 994
  },
  [969, 971, 973, 975],
  { "id": 970, "version": 819 },
  "OrderAmended",
  { "id": 972, "version": 819 },
  "OrderCancelled",
  { "id": 974, "version": 819 },
  "OrderConfirmed",
  { "id": 976, "version": 916 },
  "AddInventory",
  [978, 980, 982, 984],
  { "id": 979, "version": 916 },
  "InventoryAdjusted",
  { "id": 981, "version": 819 },
  "GetOrder",
  { "id": 983, "version": 819 },
  "PlaceOrder",
  { "id": 985, "version": 819 },
  "UserSubscriptionCancelled",
  "Orders Service",
  "Service that handles orders\n",
  [989],
  { "id": 297 },
  "openapi-v1.yml",
  { "language": 992, "url": 993 },
  "JavaScript",
  "https://github.com/event-catalog/pretend-shipping-service",
  [995, 998, 1001],
  { "type": 996, "path": 997 },
  "asyncapi",
  "order-service-asyncapi.yaml",
  { "type": 999, "path": 990, "name": 1000 },
  "openapi",
  "v1 API",
  { "type": 999, "path": 1002, "name": 1003 },
  "openapi-v2.yml",
  "v2 API",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Orders Service is responsible for managing customer orders within the system. It handles order creation, updating, status tracking, and interactions with other services such as Inventory, Payment, and Notification services to ensure smooth order processing and fulfillment.\n\n\u003CTiles >\n    \u003CTile icon=\"DocumentIcon\" href={`/docs/services/${frontmatter.id}/${frontmatter.version}/changelog`}  title=\"View the changelog\" description=\"Want to know the history of this service? View the change logs\" />\n    \u003CTile icon=\"UserGroupIcon\" href=\"/docs/teams/full-stack\" title=\"Contact the team\" description=\"Any questions? Feel free to contact the owners\" />\n    \u003CTile icon=\"BoltIcon\" href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Sends ${frontmatter.sends.length} messages`} description=\"This service sends messages to downstream consumers\" />\n    \u003CTile icon=\"BoltIcon\"  href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Receives ${frontmatter.receives.length} messages`} description=\"This service receives messages from other services\" />\n\u003C/Tiles>\n\n### Core features\n\n| Feature | Description |\n|---------|-------------|\n| Order Management | Handles order creation, updates, and status tracking |\n| Inventory Integration | Validates and processes inventory for incoming orders |\n| Payment Processing | Integrates with payment gateways to handle payment transactions |\n| Notification Integration | Sends notifications to users and other services |\n\n## Architecture diagram \n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} />\n\n## Infrastructure\n\nThe Orders Service is hosted on AWS.\n\nThe diagram below shows the infrastructure of the Orders Service. The service is hosted on AWS and uses AWS Lambda to handle the order requests. The order is stored in an AWS Aurora database and the order metadata is stored in an AWS S3 bucket.\n\n```mermaid\narchitecture-beta\n    group api(logos:aws)\n\n    service db(logos:aws-aurora)[Order DB] in api\n    service disk1(logos:aws-s3)[Order Metadata] in api\n    service server(logos:aws-lambda)[Order Handler] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n```\n\nYou can find more information about the Orders Service infrastructure in the [Orders Service documentation](https://github.com/event-catalog/pretend-shipping-service/blob/main/README.md).\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/index.mdx",
  "45e05f02eaf8e01a",
  "NotificationService-0.0.2",
  {
    "id": 1007,
    "data": 1009,
    "body": 1031,
    "filePath": 1032,
    "digest": 1033,
    "deferredRender": 20
  },
  {
    "sends": 1010,
    "receives": 1016,
    "id": 872,
    "name": 1027,
    "summary": 987,
    "version": 938,
    "owners": 1028,
    "repository": 1030
  },
  [1011, 1013],
  { "id": 1012, "version": 819 },
  "OutOfStock",
  { "id": 1014, "version": 1015 },
  "GetInventoryList",
  "0.0.x",
  [1017, 1019, 1022, 1025],
  { "id": 979, "version": 1018 },
  ">1.0.0",
  { "id": 1020, "version": 1021 },
  "PaymentProcessed",
  "^1.0.0",
  { "id": 1023, "version": 1024 },
  "GetUserNotifications",
  "x",
  { "id": 1026, "version": 1024 },
  "GetNotificationDetails",
  "Notifications",
  [1029],
  { "id": 297 },
  { "language": 992, "url": 993 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Notification Service is responsible for managing and delivering notifications to users and other services. It supports various notification channels such as email, SMS, push notifications, and in-app notifications. The service ensures reliable and timely delivery of messages and integrates with other services to trigger notifications based on specific events.\n\n\u003CTiles >\n    \u003CTile icon=\"DocumentIcon\" href={`/docs/services/${frontmatter.id}/${frontmatter.version}/changelog`}  title=\"View the changelog\" description=\"Want to know the history of this service? View the change logs\" />\n    \u003CTile icon=\"UserGroupIcon\" href=\"/docs/teams/full-stack\" title=\"Contact the team\" description=\"Any questions? Feel free to contact the owners\" />\n    \u003CTile icon=\"BoltIcon\" href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Sends ${frontmatter.sends.length} messages`} description=\"This service sends messages to downstream consumers\" />\n    \u003CTile icon=\"BoltIcon\"  href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Receives ${frontmatter.receives.length} messages`} description=\"This service receives messages from other services\" />\n\u003C/Tiles>\n\n### Core features\n\n| Feature | Description |\n|---------|-------------|\n| Multi-Channel Delivery | Supports notifications via email, SMS, push notifications, and in-app messages |\n| Template Management | Customizable notification templates with dynamic content placeholders |\n| Delivery Status Tracking | Real-time tracking and monitoring of notification delivery status |\n| Rate Limiting | Prevents notification flooding through configurable rate limits |\n| Priority Queue | Handles urgent notifications with priority delivery mechanisms |\n| Batch Processing | Efficiently processes and sends bulk notifications |\n| Retry Mechanism | Automatic retry logic for failed notification deliveries |\n| Event-Driven Notifications | Triggers notifications based on system events and user actions |\n\n\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} />\n\n## Core Concepts\n\n\u003CAccordionGroup>\n  \u003CAccordion title=\"Notification\">\n    - Description: A message that is sent to a user or a service.\n    - Attributes: notificationId, type, recipient, content, channel, status, timestamp\n  \u003C/Accordion>\n  \u003CAccordion title=\"Channel\">\n    - Description: The medium through which the notification is delivered (e.g., email, SMS, push notification).\n    - Attributes: channelId, name, provider, configuration \n  \u003C/Accordion>\n\u003C/AccordionGroup>\n\n## Infrastructure\n\nThe Notification Service is hosted on AWS.\n\nThe diagram below shows the infrastructure of the Notification Service. The service is hosted on AWS and uses AWS Lambda to handle the notification requests. The notification is stored in an AWS Aurora database and the notification metadata is stored in an AWS S3 bucket.\n\n```mermaid\narchitecture-beta\n    group api(logos:aws)\n\n    service db(logos:aws-aurora)[Notification DB] in api\n    service disk1(logos:aws-s3)[Notification Metadata] in api\n    service server(logos:aws-lambda)[Notification Handler] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n```\n\nYou can find more information about the Notification Service infrastructure in the [Notification Service documentation](https://github.com/event-catalog/pretend-shipping-service/blob/main/README.md).\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/NotificationService/index.mdx",
  "2f3d49c49ec94a4a",
  "InventoryService-0.0.2",
  {
    "id": 1034,
    "data": 1036,
    "body": 1060,
    "filePath": 1061,
    "digest": 1062,
    "deferredRender": 20
  },
  {
    "sends": 1037,
    "receives": 1041,
    "id": 560,
    "name": 1052,
    "summary": 1053,
    "version": 938,
    "owners": 1054,
    "repository": 1056,
    "deprecated": 1057
  },
  [1038, 1039, 1040],
  { "id": 979, "version": 819 },
  { "id": 1012, "version": 819 },
  { "id": 981, "version": 819 },
  [1042, 1043, 1044, 1045, 1047, 1048, 1050],
  { "id": 974, "version": 819 },
  { "id": 1014, "version": 819 },
  { "id": 970, "version": 819 },
  { "id": 1046, "version": 819 },
  "UpdateInventory",
  { "id": 976, "version": 819 },
  { "id": 1049, "version": 819 },
  "GetInventoryStatus",
  { "id": 1051, "version": 819 },
  "DeleteInventory",
  "Inventory Service",
  "Service that handles the inventory\n",
  [1055],
  { "id": 297 },
  { "language": 992, "url": 993 },
  { "message": 1058, "date": 1059 },
  "This service is **being deprecated** and replaced by the new service **InventoryServiceV2**.\nPlease contact the [team for more information](mailto:inventory-team@example.com) or visit our [website](https://eventcatalog.dev).\n",
  ["Date", "2026-05-01T00:00:00.000Z"],
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Inventory Service is a critical component of the system responsible for managing product stock levels, tracking inventory movements, and ensuring product availability. It interacts with other services to maintain accurate inventory records and supports operations such as order fulfillment, restocking, and inventory audits.\n\n\u003CTiles >\n    \u003CTile icon=\"DocumentIcon\" href={`/docs/services/${frontmatter.id}/${frontmatter.version}/changelog`}  title=\"View the changelog\" description=\"Want to know the history of this service? View the change logs\" />\n    \u003CTile icon=\"UserGroupIcon\" href=\"/docs/teams/full-stack\" title=\"Contact the team\" description=\"Any questions? Feel free to contact the owners\" />\n    \u003CTile icon=\"BoltIcon\" href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Sends ${frontmatter.sends.length} messages`} description=\"This service sends messages to downstream consumers\" />\n    \u003CTile icon=\"BoltIcon\"  href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Receives ${frontmatter.receives.length} messages`} description=\"This service receives messages from other services\" />\n\u003C/Tiles>\n\n## Core features\n\n| Feature | Description |\n|---------|-------------|\n| Real-time Stock Tracking | Monitors inventory levels across all warehouses in real-time |\n| Automated Reordering | Triggers purchase orders when stock levels fall below defined thresholds |\n| Multi-warehouse Support | Manages inventory across multiple warehouse locations |\n| Batch Processing | Handles bulk inventory updates and adjustments efficiently |\n\n## Architecture diagram\n\n\u003CNodeGraph title=\"Hello world\" />\n\n\u003CMessageTable format=\"all\" limit={4} />\n\n# Infrastructure\n\nThe Inventory Service is hosted on AWS.\n\nThe diagram below shows the infrastructure of the Inventory Service. The service is hosted on AWS and uses AWS Lambda to handle the inventory requests. The inventory is stored in an AWS Aurora database and the inventory metadata is stored in an AWS S3 bucket.\n\n```mermaid\narchitecture-beta\n    group api(logos:aws)\n\n    service db(logos:aws-aurora)[Inventory DB] in api\n    service disk1(logos:aws-s3)[Inventory Metadata] in api\n    service server(logos:aws-lambda)[Inventory Handler] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n```\n\nYou can find more information about the Inventory Service infrastructure in the [Inventory Service documentation](https://github.com/event-catalog/pretend-shipping-service/blob/main/README.md).\n\n\n\n\u003CSteps title=\"How to connect to Inventory Service\">\n  \u003CStep title=\"Obtain API credentials\">\n    Request API credentials from the Inventory Service team.\n  \u003C/Step>\n  \u003CStep title=\"Install the SDK\">\n    Run the following command in your project directory:\n    ```bash\n    npm install inventory-service-sdk\n    ```\n  \u003C/Step>\n  \u003CStep title=\"Initialize the client\">\n  Use the following code to initialize the Inventory Service client:\n\n  ```js\n  const InventoryService = require('inventory-service-sdk');\n  const client = new InventoryService.Client({\n    clientId: 'YOUR_CLIENT_ID',\n    clientSecret: 'YOUR_CLIENT_SECRET',\n    apiUrl: 'https://api.inventoryservice.com/v1'\n  });\n```\n  \u003C/Step>\n  \u003CStep title=\"Make API calls\">\n  \n  You can now use the client to make API calls. For example, to get all products:\n\n  ```js\n  client.getProducts()\n    .then(products => console.log(products))\n    .catch(error => console.error(error));\n  ```\n  \u003C/Step>\n\u003C/Steps>\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/index.mdx",
  "6aaa18b0014d9edc",
  "ShippingService-0.0.1",
  {
    "id": 1063,
    "data": 1065,
    "body": 1094,
    "filePath": 1095,
    "digest": 1096,
    "deferredRender": 20
  },
  {
    "sends": 1066,
    "receives": 1079,
    "id": 624,
    "name": 1089,
    "summary": 1090,
    "version": 830,
    "owners": 1091,
    "repository": 1093
  },
  [1067, 1069, 1071, 1073, 1075, 1077],
  { "id": 1068, "version": 819 },
  "ShipmentCreated",
  { "id": 1070, "version": 819 },
  "ReturnInitiated",
  { "id": 1072, "version": 819 },
  "ShipmentDispatched",
  { "id": 1074, "version": 819 },
  "ShipmentInTransit",
  { "id": 1076, "version": 819 },
  "ShipmentDelivered",
  { "id": 1078, "version": 819 },
  "DeliveryFailed",
  [1080, 1082, 1084, 1086, 1088],
  { "id": 1081, "version": 819 },
  "CancelShipment",
  { "id": 1083, "version": 819 },
  "CreateReturnLabel",
  { "id": 1085, "version": 819 },
  "CreateShipment",
  { "id": 1087, "version": 819 },
  "UpdateShipmentStatus",
  { "id": 1020, "version": 819 },
  "Shipping Service",
  "Service that handles shipping\n",
  [1092],
  { "id": 42 },
  { "language": 992, "url": 993 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Shipping Service is responsible for managing shipping within the system. It handles order creation, updating, status tracking, and interactions with other services such as Inventory, Payment, and Notification services to ensure smooth order processing and fulfillment.\n\n\u003CTiles >\n    \u003CTile icon=\"BoltIcon\" href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Sends ${frontmatter.sends.length} messages`} description=\"This service sends messages to downstream consumers\" />\n    \u003CTile icon=\"BoltIcon\"  href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Receives ${frontmatter.receives.length} messages`} description=\"This service receives messages from other services\" />\n\u003C/Tiles>\n\n### Core features\n\nThe Shipping Service is responsible for managing shipping within the system. It handles order creation, updating, status tracking, and interactions with other services such as Inventory, Payment, and Notification services to ensure smooth order processing and fulfillment.\n\n\n## Architecture diagram \n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/index.mdx",
  "4e966f3f6b5fd108",
  "PaymentService-0.0.1",
  {
    "id": 1097,
    "data": 1099,
    "body": 1116,
    "filePath": 1117,
    "digest": 1118,
    "deferredRender": 20
  },
  {
    "sends": 1100,
    "receives": 1103,
    "id": 604,
    "name": 1111,
    "summary": 1112,
    "version": 830,
    "owners": 1113,
    "repository": 1115
  },
  [1101, 1102],
  { "id": 1020, "version": 830 },
  { "id": 981, "version": 819 },
  [1104, 1106, 1108, 1110],
  { "id": 1105, "version": 830 },
  "PaymentInitiated",
  { "id": 1107, "version": 819 },
  "GetPaymentStatus",
  { "id": 1109, "version": 819 },
  "UserSubscriptionStarted",
  { "id": 979, "version": 819 },
  "Payment Service",
  "Service that handles payments\n",
  [1114],
  { "id": 42 },
  { "language": 992, "url": 993 },
  "The Payment Service is a crucial component of our system that handles all payment-related operations. It processes payments, manages transactions, and communicates with other services through events. Using an event-driven architecture, it ensures that all actions are asynchronous, decoupled, and scalable.\n\n### Core features\n\n| Feature | Description |\n|---------|-------------|\n| Payment Processing | Processes payments and manages transactions |\n| Event-Driven Architecture | Ensures asynchronous, decoupled, and scalable operations |\n| Integration with Payment Gateways | Interfaces with external payment providers |\n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} />\n\n## Infrastructure\n\nThe Payment Service is hosted on AWS.\n\nThe diagram below shows the infrastructure of the Payment Service. The service is hosted on AWS and uses AWS Lambda to handle the payment requests. The payment is stored in an AWS Aurora database and the payment metadata is stored in an AWS S3 bucket.\n\n```mermaid\narchitecture-beta\n    group api(logos:aws)\n\n    service db(logos:aws-aurora)[Payment DB] in api\n    service disk1(logos:aws-s3)[Payment Metadata] in api\n    service server(logos:aws-lambda)[Payment Handler] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n```\n\nYou can find more information about the Payment Service infrastructure in the [Payment Service documentation](https://github.com/event-catalog/pretend-payment-service/blob/main/README.md).\n\n### Key Components\n- Payment API: Exposes endpoints for initiating payments and querying payment status.\n- Payment Processor: Handles the core payment processing logic.\n- Event Bus: Manages the communication between services using events.\n- Payment Gateway: Interfaces with external payment providers.\n- Transaction Service: Manages transaction records and states.\n- Notification Service: Sends notifications related to payment status changes.\n- Database: Stores transaction data and payment status.",
  "../examples/default/domains/E-Commerce/subdomains/Payment/services/PaymentService/index.mdx",
  "f34269b53f50aded",
  "SubscriptionService-0.0.1",
  {
    "id": 1119,
    "data": 1121,
    "body": 1139,
    "filePath": 1140,
    "digest": 1141,
    "deferredRender": 20
  },
  {
    "sends": 1122,
    "receives": 1125,
    "id": 888,
    "name": 1133,
    "summary": 1134,
    "version": 830,
    "owners": 1135,
    "repository": 1137
  },
  [1123, 1124],
  { "id": 1109, "version": 830 },
  { "id": 985, "version": 830 },
  [1126, 1128, 1130, 1132],
  { "id": 1127, "version": 830 },
  "SubscribeUser",
  { "id": 1129, "version": 830 },
  "CancelSubscription",
  { "id": 1131, "version": 819 },
  "GetSubscriptionStatus",
  { "id": 1020, "version": 830 },
  "Subscription Service",
  "Service that handles subscriptions\n",
  [1136],
  { "id": 283 },
  { "language": 992, "url": 1138 },
  "https://github.com/event-catalog/pretend-subscription-service",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe subscription Service is responsible for handling customer subscriptions in our system. It handles new subscriptions, cancelling subscriptions and updating them.\n\n\u003CTiles >\n    \u003CTile icon=\"DocumentIcon\" href={`/docs/services/${frontmatter.id}/${frontmatter.version}/changelog`}  title=\"View the changelog\" description=\"Want to know the history of this service? View the change logs\" />\n    \u003CTile icon=\"UserGroupIcon\" href=\"/docs/teams/full-stack\" title=\"Contact the team\" description=\"Any questions? Feel free to contact the owners\" />\n    \u003CTile icon=\"BoltIcon\" href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Sends ${frontmatter.sends.length} messages`} description=\"This service sends messages to downstream consumers\" />\n    \u003CTile icon=\"BoltIcon\"  href={`/visualiser/services/${frontmatter.id}/${frontmatter.version}`} title={`Receives ${frontmatter.receives.length} messages`} description=\"This service receives messages from other services\" />\n\u003C/Tiles>\n\n### Core features\n\n| Feature | Description |\n|---------|-------------|\n| Subscription Management | Handles subscription creation, updates, and status tracking |\n| Payment Processing | Integrates with payment gateways to handle payment transactions |\n| Notification Integration | Sends notifications to users and other services |\n| Multi-Channel Fulfillment | Supports multiple fulfillment channels (e.g., shipping, in-store pickup) |\n\n## Architecture diagram \n\n\u003CNodeGraph />\n\n\u003CMessageTable format=\"all\" limit={4} />\n\n## Infrastructure\n\nThe Subscription Service is hosted on AWS.\n\nThe diagram below shows the infrastructure of the Subscription Service. The service is hosted on AWS and uses AWS Lambda to handle the subscription requests. The subscription is stored in an AWS Aurora database and the subscription metadata is stored in an AWS S3 bucket.\n\n```mermaid\narchitecture-beta\n    group api(logos:aws)\n\n    service db(logos:aws-aurora)[Subscription DB] in api\n    service disk1(logos:aws-s3)[Subscription Metadata] in api\n    service server(logos:aws-lambda)[Subscription Handler] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n```\n\nYou can find more information about the Subscription Service infrastructure in the [Subscription Service documentation](https://github.com/event-catalog/pretend-subscription-service/blob/main/README.md).\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/index.mdx",
  "d5a21a4bb4809fb0",
  "OrdersService-0.0.2",
  {
    "id": 1142,
    "data": 1144,
    "body": 1154,
    "filePath": 1155,
    "digest": 1156,
    "deferredRender": 20
  },
  {
    "sends": 1145,
    "receives": 1147,
    "id": 594,
    "name": 986,
    "summary": 987,
    "version": 938,
    "owners": 1149,
    "schemaPath": 1151,
    "repository": 1152,
    "specifications": 1153
  },
  [1146],
  { "id": 976, "version": 916 },
  [1148],
  { "id": 979, "version": 916 },
  [1150],
  { "id": 42 },
  "openapi.yml",
  { "language": 992, "url": 993 },
  { "openapiPath": 1151, "asyncapiPath": 997 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Orders Service is responsible for managing customer orders within the system. It handles order creation, updating, status tracking, and interactions with other services such as Inventory, Payment, and Notification services to ensure smooth order processing and fulfillment.\n\n## Architecture diagram \n\n\u003CNodeGraph />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/versioned/0.0.2/index.mdx",
  "942495f4e1b12dcb",
  "InventoryService-0.0.1",
  {
    "id": 1157,
    "data": 1159,
    "body": 1174,
    "filePath": 1175,
    "digest": 1176,
    "deferredRender": 20
  },
  {
    "sends": 1160,
    "receives": 1164,
    "id": 560,
    "name": 1052,
    "summary": 1053,
    "version": 830,
    "owners": 1169,
    "repository": 1173
  },
  [1161, 1163],
  { "id": 979, "version": 1162 },
  "0.0.4",
  { "id": 1012, "version": 916 },
  [1165, 1166, 1167, 1168],
  { "id": 974, "version": 830 },
  { "id": 972, "version": 830 },
  { "id": 970, "version": 830 },
  { "id": 1046, "version": 916 },
  [1170, 1171, 1172],
  { "id": 42 },
  { "id": 254 },
  { "id": 270 },
  { "language": 992, "url": 993 },
  "## Overview\n\nThe Inventory Service is a critical component of the system responsible for managing product stock levels, tracking inventory movements, and ensuring product availability. It interacts with other services to maintain accurate inventory records and supports operations such as order fulfillment, restocking, and inventory audits.\n\n## Architecture diagram\n\n\u003CNodeGraph title=\"Hello world\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/versioned/0.0.1/index.mdx",
  "405a2c4b0e672d22",
  "commands",
  [
    "Map",
    1179,
    1180,
    1205,
    1206,
    1225,
    1226,
    1246,
    1247,
    1257,
    1258,
    1268,
    1269,
    1279,
    1280,
    1296,
    1297,
    1310,
    1311,
    1323,
    1324
  ],
  "DeleteInventory-0.0.3",
  {
    "id": 1179,
    "data": 1181,
    "body": 1202,
    "filePath": 1203,
    "digest": 1204,
    "deferredRender": 20
  },
  {
    "channels": 1182,
    "id": 1051,
    "name": 1187,
    "summary": 1188,
    "version": 916,
    "badges": 1189,
    "owners": 1193,
    "schemaPath": 1199,
    "sidebar": 1200
  },
  [1183],
  { "parameters": 1184, "id": 1186, "version": 819 },
  { "env": 1185 },
  "staging",
  "inventory.{env}.events",
  "Delete Inventory",
  "Command that will delete a given inventory item from the system\n",
  [1190],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  "Recently updated!",
  "green",
  [1194, 1195, 1196, 1197, 1198],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  "schema.json",
  { "badge": 1201 },
  "DELETE",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe DeleteInventory command is issued to remove a product from the inventory system. This command is used by the inventory management system when a product needs to be completely removed from the warehouse or store catalog, typically due to discontinuation or permanent removal of the item.\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/commands/DeleteInventory/index.mdx",
  "17b28d5ec21cc1f1",
  "AddInventory-0.0.3",
  {
    "id": 1205,
    "data": 1207,
    "body": 1222,
    "filePath": 1223,
    "digest": 1224,
    "deferredRender": 20
  },
  {
    "channels": 1208,
    "id": 976,
    "name": 1211,
    "summary": 1212,
    "version": 916,
    "badges": 1213,
    "owners": 1215,
    "schemaPath": 1199,
    "sidebar": 1220
  },
  [1209],
  { "parameters": 1210, "id": 1186, "version": 819 },
  { "env": 1185 },
  "Add inventory",
  "Command that will add item to a given inventory id\n",
  [1214],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1216, 1217, 1218, 1219],
  { "id": 42 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  { "badge": 1221 },
  "POST",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe AddInventory command is issued to add new stock to the inventory. This command is used by the inventory management system to update the quantity of products available in the warehouse or store.\n\n## Architecture diagram\n\n\u003CNodeGraph/>\n\n## Payload example\n\n```json title=\"Payload example\"\n{\n  \"productId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n  \"quantity\": 50,\n  \"warehouseId\": \"456e7891-c23d-45f6-b78a-123456789abc\",\n  \"timestamp\": \"2024-07-04T14:48:00Z\"\n}\n\n```\n\n## Schema\n\n\u003CSchema file=\"schema.json\"/>\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/commands/AddInventory/index.mdx",
  "106b7429daebcbb9",
  "UpdateInventory-0.0.3",
  {
    "id": 1225,
    "data": 1227,
    "body": 1243,
    "filePath": 1244,
    "digest": 1245,
    "deferredRender": 20
  },
  {
    "channels": 1228,
    "id": 1046,
    "name": 1231,
    "summary": 1232,
    "version": 916,
    "badges": 1233,
    "owners": 1235,
    "schemaPath": 1199,
    "sidebar": 1241
  },
  [1229],
  { "parameters": 1230, "id": 1186, "version": 819 },
  { "env": 1185 },
  "Update inventory",
  "Command that will update a given inventory item\n",
  [1234],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1236, 1237, 1238, 1239, 1240],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  { "badge": 1242 },
  "PUT",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe UpdateInventory command is issued to update the existing stock levels of a product in the inventory. This command is used by the inventory management system to adjust the quantity of products available in the warehouse or store, either by increasing or decreasing the current stock levels.\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />\n\n## Payload example\n\n```json title=\"Payload example\"\n{\n  \"productId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n  \"quantityChange\": -10,\n  \"warehouseId\": \"456e7891-c23d-45f6-b78a-123456789abc\",\n  \"timestamp\": \"2024-07-04T14:48:00Z\"\n}\n```\n\n## Schema (JSON schema)\n\n\u003CSchema file=\"schema.json\"/>\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/commands/UpdateInventory/index.mdx",
  "7a6aa4f22e442604",
  "CancelShipment-0.0.1",
  {
    "id": 1246,
    "data": 1248,
    "body": 1254,
    "filePath": 1255,
    "digest": 1256,
    "deferredRender": 20
  },
  {
    "id": 1081,
    "name": 1249,
    "summary": 1250,
    "version": 830,
    "owners": 1251,
    "schemaPath": 1199,
    "sidebar": 1253
  },
  "Cancel shipment",
  "POST request that will cancel a shipment, identified by its shipmentId.\n",
  [1252],
  { "id": 42 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `CancelShipment` message is a command used to cancel a shipment, identified by its `shipmentId`. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis command can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CSchemaViewer file=\"schema.json\" title=\"Schema\" maxHeight=\"500\" />\n\n\u003CNodeGraph />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/commands/CancelShipment/index.mdx",
  "48d96360e9322568",
  "UpdateShipmentStatus-0.0.1",
  {
    "id": 1257,
    "data": 1259,
    "body": 1265,
    "filePath": 1266,
    "digest": 1267,
    "deferredRender": 20
  },
  {
    "id": 1087,
    "name": 1260,
    "summary": 1261,
    "version": 830,
    "owners": 1262,
    "schemaPath": 1199,
    "sidebar": 1264
  },
  "Update shipment status",
  "POST request that will update the status of a shipment, identified by its shipmentId.\n",
  [1263],
  { "id": 42 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `UpdateShipmentStatus` message is a command used to update the status of a shipment, identified by its `shipmentId`. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis command can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/commands/UpdateShipmentStatus/index.mdx",
  "73cf35e5d36acfd9",
  "CreateShipment-0.0.1",
  {
    "id": 1268,
    "data": 1270,
    "body": 1276,
    "filePath": 1277,
    "digest": 1278,
    "deferredRender": 20
  },
  {
    "id": 1085,
    "name": 1271,
    "summary": 1272,
    "version": 830,
    "owners": 1273,
    "schemaPath": 1199,
    "sidebar": 1275
  },
  "Create shipment",
  "POST request that will create a shipment for a specific order, identified by its orderId.\n",
  [1274],
  { "id": 42 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `CreateShipment` message is a command used to create a shipment for a specific order, identified by its `orderId`. It provides information such as the order status (e.g., pending, completed, shipped), the items within the order, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis command can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time order data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/commands/CreateShipment/index.mdx",
  "98b8b56fe0d04090",
  "PlaceOrder-0.0.1",
  {
    "id": 1279,
    "data": 1281,
    "body": 1293,
    "filePath": 1294,
    "digest": 1295,
    "deferredRender": 20
  },
  {
    "id": 983,
    "name": 1282,
    "summary": 1283,
    "version": 830,
    "badges": 1284,
    "owners": 1286,
    "schemaPath": 1199,
    "sidebar": 1292
  },
  "Place Order",
  "Command that will place an order\n",
  [1285],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1287, 1288, 1289, 1290, 1291],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Order Placement Command is a versatile and robust system designed to streamline the process of placing an order. This command takes care of all the essential details needed to complete a purchase, ensuring a smooth and efficient transaction from start to finish.\n\n## Architecture diagram\n\n\u003CNodeGraph/>\n\n## Schema\n\n\u003CSchemaViewer file=\"schema.json\"/>\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/commands/PlaceOrder/index.mdx",
  "19d07e3824d444da",
  "CancelSubscription-0.0.1",
  {
    "id": 1296,
    "data": 1298,
    "body": 1307,
    "filePath": 1308,
    "digest": 1309,
    "deferredRender": 20
  },
  {
    "id": 1129,
    "name": 1299,
    "summary": 1300,
    "version": 830,
    "badges": 1301,
    "owners": 1304,
    "sidebar": 1306
  },
  "Cancel subscription",
  "Command that will try and cancel a users subscription\n",
  [1302],
  { "content": 1303, "backgroundColor": 1192, "textColor": 1192 },
  "New!",
  [1305],
  { "id": 283 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `CancelSubscription` command will try and cancel a subscription for the user.\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/commands/CancelSubscription/index.mdx",
  "ff2d317b58f56dbe",
  "SubscribeUser-0.0.1",
  {
    "id": 1310,
    "data": 1312,
    "body": 1320,
    "filePath": 1321,
    "digest": 1322,
    "deferredRender": 20
  },
  {
    "id": 1127,
    "name": 1313,
    "summary": 1314,
    "version": 830,
    "badges": 1315,
    "owners": 1317,
    "sidebar": 1319
  },
  "Subscribe user",
  "Command that will try and subscribe a given user\n",
  [1316],
  { "content": 1303, "backgroundColor": 1192, "textColor": 1192 },
  [1318],
  { "id": 283 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `SubscribeUser` command represents when a new user wants to subscribe to our service.\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/commands/SubscribeUser/index.mdx",
  "a96417bf8e330295",
  "CreateReturnLabel-0.0.1",
  {
    "id": 1323,
    "data": 1325,
    "body": 1331,
    "filePath": 1332,
    "digest": 1333,
    "deferredRender": 20
  },
  {
    "id": 1083,
    "name": 1326,
    "summary": 1327,
    "version": 830,
    "owners": 1328,
    "schemaPath": 1199,
    "sidebar": 1330
  },
  "Create return label",
  "POST request that will create a return label for a specific shipment, identified by its shipmentId.\n",
  [1329],
  { "id": 42 },
  { "badge": 1221 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `CreateReturnLabel` message is a command used to create a return label for a specific shipment, identified by its `shipmentId`. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis command can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/commands/CreateReturnLabel/index.mdx",
  "0a834e6a2e68b14a",
  "events",
  [
    "Map",
    1336,
    1337,
    1359,
    1360,
    1376,
    1377,
    1395,
    1396,
    1410,
    1411,
    1424,
    1425,
    1440,
    1441,
    1453,
    1454,
    1463,
    1464,
    1473,
    1474,
    1483,
    1484,
    1493,
    1494,
    1503,
    1504,
    1513,
    1514,
    1525,
    1526,
    1540,
    1541,
    1552,
    1553,
    1562,
    1563,
    1569,
    1570
  ],
  "InventoryAdjusted-1.0.1",
  {
    "id": 1336,
    "data": 1338,
    "body": 1356,
    "filePath": 1357,
    "digest": 1358,
    "deferredRender": 20
  },
  {
    "channels": 1339,
    "id": 979,
    "name": 1341,
    "summary": 1342,
    "version": 1343,
    "badges": 1344,
    "owners": 1349,
    "schemaPath": 1355
  },
  [1340],
  { "id": 1186, "version": 819 },
  "Inventory adjusted",
  "Indicates a change in inventory level\n",
  "1.0.1",
  [1345, 1346],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  "Broker:Apache Kafka",
  "kafka",
  [1350, 1351, 1352, 1353, 1354],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  "schema.avro",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `Inventory Adjusted` event is triggered whenever there is a change in the inventory levels of a product. This could occur due to various reasons such as receiving new stock, sales, returns, or manual adjustments by the inventory management team. The event ensures that all parts of the system that rely on inventory data are kept up-to-date with the latest inventory levels.\n\n\u003CTiles >\n    \u003CTile icon=\"UserGroupIcon\" href=\"/docs/teams/full-stack\" title=\"Contact the team\" description=\"Any questions? Feel free to contact the owners\" />\n    \u003CTile icon=\"DocumentIcon\" href={`/generated/events/Inventory/${frontmatter.id}/schema.avro`} title=\"View the schema\" description=\"View the schema directly in your browser\" />\n\u003C/Tiles>\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\n\u003CSchemaViewer file=\"schema.yml\" title=\"JSON Schema\" maxHeight=\"500\" />\n\n## Payload example\n\nEvent example you my see being published.\n\n```json title=\"Payload example\"\n{\n  \"Name\": \"John Doe\",\n  \"Age\": 30,\n  \"Department\": \"Engineering\",\n  \"Position\": \"Software Engineer\",\n  \"Salary\": 85000.50,\n  \"JoinDate\": \"2024-01-15\"\n}\n```\n\n## Schema (avro)\n\n\u003CSchema file=\"schema.avro\" title=\"Inventory Adjusted Schema (avro)\" />\n\n## Producing the Event\n\nSelect the language you want to produce the event in to see an example.\n\n\u003CTabs>\n  \u003CTabItem title=\"Python\">\n\n    ```python title=\"Produce event in Python\" frame=\"terminal\"\n    from kafka import KafkaProducer\n    import json\n    from datetime import datetime\n\n    # Kafka configuration\n    producer = KafkaProducer(\n        bootstrap_servers=['localhost:9092'],\n        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n    )\n\n    # Event data\n    event_data = {\n      \"event_id\": \"abc123\",\n      \"timestamp\": datetime.utcnow().isoformat() + 'Z',\n      \"product_id\": \"prod987\",\n      \"adjusted_quantity\": 10,\n      \"new_quantity\": 150,\n      \"adjustment_reason\": \"restock\",\n      \"adjusted_by\": \"user123\"\n    }\n\n    # Send event to Kafka topic\n    producer.send('inventory.adjusted', event_data)\n    producer.flush()\n    ```\n  \u003C/TabItem>\n  \u003CTabItem title=\"TypeScript\">\n\n    ```typescript title=\"Produce event in TypeScript\" frame=\"terminal\"\n    import { Kafka } from 'kafkajs';\n\n    // Kafka configuration\n    const kafka = new Kafka({\n      clientId: 'inventory-producer',\n      brokers: ['localhost:9092']\n    });\n\n    const producer = kafka.producer();\n\n    // Event data\n    const eventData = {\n      event_id: \"abc123\",\n      timestamp: new Date().toISOString(),\n      product_id: \"prod987\", \n      adjusted_quantity: 10,\n      new_quantity: 150,\n      adjustment_reason: \"restock\",\n      adjusted_by: \"user123\"\n    };\n\n    // Send event to Kafka topic\n    async function produceEvent() {\n      await producer.connect();\n      await producer.send({\n        topic: 'inventory.adjusted',\n        messages: [\n          { value: JSON.stringify(eventData) }\n        ],\n      });\n      await producer.disconnect();\n    }\n\n    produceEvent().catch(console.error);\n    ```\n  \u003C/TabItem>\n  \u003CTabItem title=\"Java\">\n\n    ```java title=\"Produce event in Java\" frame=\"terminal\"\n    import org.apache.kafka.clients.producer.*;\n    import org.apache.kafka.common.serialization.StringSerializer;\n    import com.fasterxml.jackson.databind.ObjectMapper;\n    import java.util.Properties;\n    import java.util.HashMap;\n    import java.util.Map;\n    import java.time.Instant;\n\n    public class InventoryProducer {\n        public static void main(String[] args) {\n            // Kafka configuration\n            Properties props = new Properties();\n            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n            props.put(ProducerConfig.CLIENT_ID_CONFIG, \"inventory-producer\");\n            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n            Producer\u003CString, String> producer = new KafkaProducer\u003C>(props);\n            ObjectMapper mapper = new ObjectMapper();\n\n            try {\n                // Event data\n                Map\u003CString, Object> eventData = new HashMap\u003C>();\n                eventData.put(\"event_id\", \"abc123\");\n                eventData.put(\"timestamp\", Instant.now().toString());\n                eventData.put(\"product_id\", \"prod987\");\n                eventData.put(\"adjusted_quantity\", 10);\n                eventData.put(\"new_quantity\", 150);\n                eventData.put(\"adjustment_reason\", \"restock\");\n                eventData.put(\"adjusted_by\", \"user123\");\n\n                // Create producer record\n                ProducerRecord\u003CString, String> record = new ProducerRecord\u003C>(\n                    \"inventory.adjusted\",\n                    mapper.writeValueAsString(eventData)\n                );\n\n                // Send event to Kafka topic\n                producer.send(record, (metadata, exception) -> {\n                    if (exception != null) {\n                        System.err.println(\"Error producing message: \" + exception);\n                    }\n                });\n\n            } catch (Exception e) {\n                e.printStackTrace();\n            } finally {\n                producer.flush();\n                producer.close();\n            }\n        }\n    }\n    ```\n  \u003C/TabItem>\n\u003C/Tabs>\n\n\n\n### Consuming the Event\n\nTo consume an Inventory Adjusted event, use the following example Kafka consumer configuration in Python:\n\n```python title=\"Consuming the event with python\" frame=\"terminal\"\nfrom kafka import KafkaConsumer\nimport json\n\n# Kafka configuration\nconsumer = KafkaConsumer(\n    'inventory.adjusted',\n    bootstrap_servers=['localhost:9092'],\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    group_id='inventory_group',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Consume events\nfor message in consumer:\n    event_data = json.loads(message.value)\n    print(f\"Received Inventory Adjusted event: {event_data}\")\n```\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/InventoryAdjusted/index.mdx",
  "ade660eb81993fdd",
  "OrderAmended-0.0.1",
  {
    "id": 1359,
    "data": 1361,
    "body": 1373,
    "filePath": 1374,
    "digest": 1375,
    "deferredRender": 20
  },
  {
    "channels": 1362,
    "id": 970,
    "name": 1366,
    "summary": 1367,
    "version": 830,
    "badges": 1368,
    "owners": 1371,
    "schemaPath": 1355
  },
  [1363],
  { "parameters": 1364, "id": 1365, "version": 819 },
  { "env": 1185 },
  "orders.{env}.events",
  "Order amended",
  "Indicates an order has been changed\n",
  [1369, 1370],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  [1372],
  { "id": 297 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe OrderAmended event is triggered whenever an existing order is modified. This event ensures that all relevant services are notified of changes to an order, such as updates to order items, quantities, shipping information, or status. The event allows the system to maintain consistency and ensure that all dependent services can react appropriately to the amendments.\n\n\u003CNodeGraph />\n\n## Example payload\n\n```json title=\"Example Payload\"\n{\n  \"orderId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"userId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"amendedItems\": [\n    {\n      \"productId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n      \"productName\": \"Example Product\",\n      \"oldQuantity\": 2,\n      \"newQuantity\": 3,\n      \"unitPrice\": 29.99,\n      \"totalPrice\": 89.97\n    }\n  ],\n  \"orderStatus\": \"confirmed\",\n  \"totalAmount\": 150.75,\n  \"timestamp\": \"2024-07-04T14:48:00Z\"\n}\n```\n\n## Schema (Avro)s\n\n\u003CSchema file=\"schema.avro\" />\n\n## Schema (JSON)\n\n\u003CSchema file=\"schema.json\" />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/events/OrderAmended/index.mdx",
  "1c2e739addd67a6a",
  "OutOfStock-0.0.4",
  {
    "id": 1376,
    "data": 1378,
    "body": 1392,
    "filePath": 1393,
    "digest": 1394,
    "deferredRender": 20
  },
  {
    "channels": 1379,
    "id": 1012,
    "name": 1381,
    "summary": 1382,
    "version": 1162,
    "badges": 1383,
    "owners": 1386
  },
  [1380],
  { "id": 1186, "version": 819 },
  "Inventory out of stock",
  "Indicates inventory is out of stock\n",
  [1384, 1385],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  [1387, 1388, 1389, 1390, 1391],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `Inventory Adjusted` event is triggered whenever there is a change in the inventory levels of a product. This could occur due to various reasons such as receiving new stock, sales, returns, or manual adjustments by the inventory management team. The event ensures that all parts of the system that rely on inventory data are kept up-to-date with the latest inventory levels.\n\n\u003CNodeGraph />\n\n### Payload\nThe payload of the `Inventory Adjusted` event includes the following fields:\n\n```json title=\"Example of payload\" frame=\"terminal\"\n{\n  \"event_id\": \"string\",\n  \"timestamp\": \"ISO 8601 date-time\",\n  \"product_id\": \"string\",\n  \"adjusted_quantity\": \"integer\",\n  \"new_quantity\": \"integer\",\n  \"adjustment_reason\": \"string\"\n}\n```\n\n### Producing the Event\n\nTo produce an Inventory Adjusted event, use the following example Kafka producer configuration in Python:\n\n```python title=\"Produce event in Python\" frame=\"terminal\"\nfrom kafka import KafkaProducer\nimport json\nfrom datetime import datetime\n\n# Kafka configuration\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Event data\nevent_data = {\n  \"event_id\": \"abc123\",\n  \"timestamp\": datetime.utcnow().isoformat() + 'Z',\n  \"product_id\": \"prod987\",\n  \"adjusted_quantity\": 10,\n  \"new_quantity\": 150,\n  \"adjustment_reason\": \"restock\",\n  \"adjusted_by\": \"user123\"\n}\n\n# Send event to Kafka topic\nproducer.send('inventory.adjusted', event_data)\nproducer.flush()\n```\n\n### Consuming the Event\n\nTo consume an Inventory Adjusted event, use the following example Kafka consumer configuration in Python:\n\n```python title=\"Consuming the event with python\" frame=\"terminal\"\nfrom kafka import KafkaConsumer\nimport json\n\n# Kafka configuration\nconsumer = KafkaConsumer(\n    'inventory.adjusted',\n    bootstrap_servers=['localhost:9092'],\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    group_id='inventory_group',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Consume events\nfor message in consumer:\n    event_data = json.loads(message.value)\n    print(f\"Received Inventory Adjusted event: {event_data}\")\n```\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/OutOfStock/index.mdx",
  "5a97dd73fa169ad6",
  "OrderCancelled-0.0.1",
  {
    "id": 1395,
    "data": 1397,
    "body": 1407,
    "filePath": 1408,
    "digest": 1409,
    "deferredRender": 20
  },
  {
    "channels": 1398,
    "id": 972,
    "name": 1400,
    "summary": 1401,
    "version": 830,
    "badges": 1402,
    "owners": 1405,
    "schemaPath": 1199
  },
  [1399],
  { "id": 1365, "version": 819 },
  "Order cancelled",
  "Indicates an order has been canceled\n",
  [1403, 1404],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  [1406],
  { "id": 297 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe OrderCancelled event is triggered whenever an existing order is cancelled. This event ensures that all relevant services are notified of the cancellation, allowing them to take appropriate actions such as updating inventory levels, refunding payments, and notifying the user. The event helps maintain consistency across the system by ensuring all dependent services are aware of the order cancellation.\n\n## Example payload\n\n```json title=\"Example payload\"\n{\n  \"orderId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"userId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"orderItems\": [\n    {\n      \"productId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n      \"productName\": \"Example Product\",\n      \"quantity\": 2,\n      \"unitPrice\": 29.99,\n      \"totalPrice\": 59.98\n    }\n  ],\n  \"orderStatus\": \"cancelled\",\n  \"totalAmount\": 59.98,\n  \"cancellationReason\": \"Customer requested cancellation\",\n  \"timestamp\": \"2024-07-04T14:48:00Z\"\n}\n\n```\n\n## Schema\n\nJSON schema for the event.\n\n\u003CSchema title=\"JSON Schema\" file=\"schema.json\"/>\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/events/OrderCancelled/index.mdx",
  "978875f2e0eb596a",
  "PaymentProcessed-1.0.0",
  {
    "id": 1410,
    "data": 1412,
    "body": 1421,
    "filePath": 1422,
    "digest": 1423,
    "deferredRender": 20
  },
  {
    "channels": 1413,
    "id": 1020,
    "name": 1417,
    "summary": 1418,
    "version": 852,
    "owners": 1419
  },
  [1414],
  { "parameters": 1415, "id": 1416, "version": 819 },
  { "env": 1185 },
  "payments.{env}.events",
  "Payment Processed",
  "Event is triggered after the payment has been successfully processed",
  [1420],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe PaymentProcessed event is triggered after the payment has been successfully processed by the Payment Service. This event signifies that a payment has been confirmed, and it communicates the outcome to other services and components within the system.\n\n\u003CNodeGraph />\n\n### Payload Example\n\n```json title=\"Payload example\"\n{\n  \"transactionId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"userId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"orderId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n  \"amount\": 100.50,\n  \"paymentMethod\": \"CreditCard\",\n  \"status\": \"confirmed\",\n  \"confirmationDetails\": {\n    \"gatewayResponse\": \"Approved\",\n    \"transactionId\": \"abc123\"\n  },\n  \"timestamp\": \"2024-07-04T14:48:00Z\"\n}\n```\n\n### Security Considerations\n\n- **Data Validation**: Ensure that all data in the event payload is validated before publishing to prevent injection attacks or other malicious activities.\n- **Sensitive Data Handling**: Avoid including sensitive information (e.g., full credit card numbers) in the event payload. Use secure channels and encryption for such data.\n- **Authentication and Authorization**: Ensure that only authorized services can publish or consume PaymentProcessed events.\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Payment/services/PaymentService/events/PaymentProcessed/index.mdx",
  "45b319e31a874a90",
  "OrderConfirmed-0.0.1",
  {
    "id": 1424,
    "data": 1426,
    "body": 1437,
    "filePath": 1438,
    "digest": 1439,
    "deferredRender": 20
  },
  {
    "channels": 1427,
    "id": 974,
    "name": 1429,
    "summary": 1430,
    "version": 830,
    "badges": 1431,
    "owners": 1435,
    "schemaPath": 1199
  },
  [1428],
  { "id": 1365, "version": 819 },
  "Order confirmed",
  "Indicates an order has been confirmed\n",
  [1432, 1434],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192, "icon": 1433 },
  "StarIcon",
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  [1436],
  { "id": 297 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe OrderConfirmed event is triggered when an order has been successfully confirmed. This event notifies relevant services that the order is ready for further processing, such as inventory adjustment, payment finalization, and preparation for shipping.\n\n## Architecture Diagram\n\n\u003CNodeGraph />\n\n## Payload\n\n```json title=\"Example payload\"\n{\n  \"orderId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"userId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"orderItems\": [\n    {\n      \"productId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n      \"productName\": \"Example Product\",\n      \"quantity\": 2,\n      \"unitPrice\": 29.99,\n      \"totalPrice\": 59.98\n    }\n  ],\n  \"orderStatus\": \"confirmed\",\n  \"totalAmount\": 150.75,\n  \"confirmationTimestamp\": \"2024-07-04T14:48:00Z\"\n}\n```\n\n## Schema\n\n\u003CSchema file=\"schema.json\"/>\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/events/OrderConfirmed/index.mdx",
  "25c0eccde9d78e40",
  "PaymentInitiated-0.0.1",
  {
    "id": 1440,
    "data": 1442,
    "body": 1450,
    "filePath": 1451,
    "digest": 1452,
    "deferredRender": 20
  },
  {
    "channels": 1443,
    "id": 1105,
    "name": 1446,
    "summary": 1447,
    "version": 830,
    "owners": 1448
  },
  [1444],
  { "parameters": 1445, "id": 1416, "version": 819 },
  { "env": 1185 },
  "Payment Initiated",
  "Event is triggered when a user initiates a payment through the Payment Service",
  [1449],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe Payment Initiated event is triggered when a user initiates a payment through the Payment Service. This event signifies the beginning of the payment process and contains all necessary information to process the payment.\n\n\u003CNodeGraph />\n\n### Payload Example\n\n```json title=\"Payload example\"\n{\n  \"userId\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"orderId\": \"789e1234-b56c-78d9-e012-3456789fghij\",\n  \"amount\": 100.50,\n  \"paymentMethod\": \"CreditCard\",\n  \"timestamp\": \"2024-07-04T14:48:00Z\"\n}\n```\n\n### Security Considerations\n\n- **Authentication**: Ensure that only authenticated users can initiate a payment, and the userId in the payload matches the authenticated user.\n- **Data Validation**: Validate all input data to prevent injection attacks or other malicious input.\n- **Sensitive Data Handling**: Avoid including sensitive information (e.g., credit card numbers) in the event payload. Use secure channels and encryption for such data.\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Payment/services/PaymentService/events/PaymentInitiated/index.mdx",
  "9dba7743582f388c",
  "DeliveryFailed-0.0.1",
  {
    "id": 1453,
    "data": 1455,
    "body": 1460,
    "filePath": 1461,
    "digest": 1462,
    "deferredRender": 20
  },
  {
    "id": 1078,
    "name": 1456,
    "summary": 1457,
    "version": 830,
    "owners": 1458,
    "schemaPath": 1199
  },
  "Delivery failed",
  "Event that is emitted when a shipment delivery fails.\n",
  [1459],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `DeliveryFailed` event is emitted when a shipment delivery fails. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis event can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/events/DeliveryFailed/index.mdx",
  "388b45c6a5232dd7",
  "ReturnInitiated-0.0.1",
  {
    "id": 1463,
    "data": 1465,
    "body": 1470,
    "filePath": 1471,
    "digest": 1472,
    "deferredRender": 20
  },
  {
    "id": 1070,
    "name": 1466,
    "summary": 1467,
    "version": 830,
    "owners": 1468,
    "schemaPath": 1199
  },
  "Return initiated",
  "Event that is emitted when a return is initiated.\n",
  [1469],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `ReturnInitiated` event is emitted when a return is initiated. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis event can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time return data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/events/ReturnInitiated/index.mdx",
  "8c07547b7d221ec0",
  "ShipmentCreated-0.0.1",
  {
    "id": 1473,
    "data": 1475,
    "body": 1480,
    "filePath": 1481,
    "digest": 1482,
    "deferredRender": 20
  },
  {
    "id": 1068,
    "name": 1476,
    "summary": 1477,
    "version": 830,
    "owners": 1478,
    "schemaPath": 1199
  },
  "Shipment created",
  "Event that is emitted when a shipment is created.\n",
  [1479],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `ShipmentCreated` event is emitted when a shipment is created. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis event can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/events/ShipmentCreated/index.mdx",
  "81baf1c8ce150762",
  "ShipmentDelivered-0.0.1",
  {
    "id": 1483,
    "data": 1485,
    "body": 1490,
    "filePath": 1491,
    "digest": 1492,
    "deferredRender": 20
  },
  {
    "id": 1076,
    "name": 1486,
    "summary": 1487,
    "version": 830,
    "owners": 1488,
    "schemaPath": 1199
  },
  "Shipment delivered",
  "Event that is emitted when a shipment is delivered.\n",
  [1489],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `ShipmentDelivered` event is emitted when a shipment is delivered. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis event can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/events/ShipmentDelivered/index.mdx",
  "4aa31b957c9f7f14",
  "ShipmentDispatched-0.0.1",
  {
    "id": 1493,
    "data": 1495,
    "body": 1500,
    "filePath": 1501,
    "digest": 1502,
    "deferredRender": 20
  },
  {
    "id": 1072,
    "name": 1496,
    "summary": 1497,
    "version": 830,
    "owners": 1498,
    "schemaPath": 1199
  },
  "Shipment dispatched",
  "Event that is emitted when a shipment is dispatched.\n",
  [1499],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `ShipmentDispatched` event is emitted when a shipment is dispatched. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis event can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/events/ShipmentDispatched/index.mdx",
  "cb4e2735758f102b",
  "ShipmentInTransit-0.0.1",
  {
    "id": 1503,
    "data": 1505,
    "body": 1510,
    "filePath": 1511,
    "digest": 1512,
    "deferredRender": 20
  },
  {
    "id": 1074,
    "name": 1506,
    "summary": 1507,
    "version": 830,
    "owners": 1508,
    "schemaPath": 1199
  },
  "Shipment in transit",
  "Event that is emitted when a shipment is in transit.\n",
  [1509],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `ShipmentInTransit` event is emitted when a shipment is in transit. It provides information such as the shipment status (e.g., pending, completed, shipped), the items within the shipment, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis event can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time shipment data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/ShippingService/events/ShipmentInTransit/index.mdx",
  "57803e3280f01326",
  "UserSubscriptionCancelled-0.0.1",
  {
    "id": 1513,
    "data": 1515,
    "body": 1522,
    "filePath": 1523,
    "digest": 1524,
    "deferredRender": 20
  },
  {
    "id": 985,
    "name": 1516,
    "summary": 1517,
    "version": 830,
    "badges": 1518,
    "owners": 1520
  },
  "User subscription cancelled",
  "An event that is triggered when a users subscription has been cancelled\n",
  [1519],
  { "content": 1303, "backgroundColor": 1192, "textColor": 1192 },
  [1521],
  { "id": 283 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `UserSubscriptionCancelled` event is triggered when a users subscription has been cancelled.\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/events/UserSubscriptionCancelled/index.mdx",
  "f4393a7ec584dfcb",
  "InventoryAdjusted-1.0.0",
  {
    "id": 1525,
    "data": 1527,
    "body": 1537,
    "filePath": 1538,
    "digest": 1539,
    "deferredRender": 20
  },
  {
    "id": 979,
    "name": 1341,
    "summary": 1342,
    "version": 852,
    "badges": 1528,
    "owners": 1531
  },
  [1529, 1530],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  [1532, 1533, 1534, 1535, 1536],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  "## Overview\n\nThe `Inventory Adjusted` event is triggered whenever there is a change in the inventory levels of a product. This could occur due to various reasons such as receiving new stock, sales, returns, or manual adjustments by the inventory management team. The event ensures that all parts of the system that rely on inventory data are kept up-to-date with the latest inventory levels.\n\n\u003CNodeGraph />\n\n## Event Details\n\n### Event Name\n`inventory.adjusted`\n\n### Description\nThis event indicates that the inventory count for one or more products has been adjusted. The event carries the updated inventory details including the product ID, the new quantity, and the reason for the adjustment.\n\n### Payload\nThe payload of the `Inventory Adjusted` event includes the following fields:\n\n```json title=\"Example of payload\" frame=\"terminal\"\n{\n  \"event_id\": \"string\",\n  \"timestamp\": \"ISO 8601 date-time\",\n  \"product_id\": \"string\",\n  \"adjusted_quantity\": \"integer\",\n  \"new_quantity\": \"integer\",\n  \"adjustment_reason\": \"string\",\n  \"adjusted_by\": \"string\"\n}\n```\n\n### Producing the Event\n\nTo produce an Inventory Adjusted event, use the following example Kafka producer configuration in Python:\n\n```python title=\"Produce event in Python\" frame=\"terminal\"\nfrom kafka import KafkaProducer\nimport json\nfrom datetime import datetime\n\n# Kafka configuration\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Event data\nevent_data = {\n  \"event_id\": \"abc123\",\n  \"timestamp\": datetime.utcnow().isoformat() + 'Z',\n  \"product_id\": \"prod987\",\n  \"adjusted_quantity\": 10,\n  \"new_quantity\": 150,\n  \"adjustment_reason\": \"restock\",\n  \"adjusted_by\": \"user123\"\n}\n\n# Send event to Kafka topic\nproducer.send('inventory.adjusted', event_data)\nproducer.flush()\n```\n\n### Consuming the Event\n\nTo consume an Inventory Adjusted event, use the following example Kafka consumer configuration in Python:\n\n```python title=\"Consuming the event with python\" frame=\"terminal\"\nfrom kafka import KafkaConsumer\nimport json\n\n# Kafka configuration\nconsumer = KafkaConsumer(\n    'inventory.adjusted',\n    bootstrap_servers=['localhost:9092'],\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    group_id='inventory_group',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Consume events\nfor message in consumer:\n    event_data = json.loads(message.value)\n    print(f\"Received Inventory Adjusted event: {event_data}\")\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/InventoryAdjusted/versioned/1.0.0/index.mdx",
  "154e74d94c13ff7f",
  "UserSubscriptionStarted-0.0.1",
  {
    "id": 1540,
    "data": 1542,
    "body": 1549,
    "filePath": 1550,
    "digest": 1551,
    "deferredRender": 20
  },
  {
    "id": 1109,
    "name": 1543,
    "summary": 1544,
    "version": 830,
    "badges": 1545,
    "owners": 1547
  },
  "User subscription started",
  "An event that is triggered when a new user subscription has started\n",
  [1546],
  { "content": 1303, "backgroundColor": 1192, "textColor": 1192 },
  [1548],
  { "id": 283 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `UserSubscriptionStarted` event is triggered when a user starts a new subscription with our service.\n\n## Architecture diagram\n\n\u003CNodeGraph />\n\n\u003CFooter />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/events/UserSubscriptionStarted/index.mdx",
  "975a26dbf5b9579a",
  "InventoryAdjusted-0.0.1",
  {
    "id": 1552,
    "data": 1554,
    "body": 1559,
    "filePath": 1560,
    "digest": 1561,
    "deferredRender": 20
  },
  {
    "id": 979,
    "name": 1341,
    "summary": 1342,
    "version": 830,
    "badges": 1555,
    "owners": 1557
  },
  [1556],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1558],
  { "id": 42 },
  ":::warning\nWhen firing this event make sure you set the `correlation-id` in the headers. Our schemas have standard metadata make sure you read and follow it.\n:::\n\n### Details\n\n\u003CNodeGraph />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/InventoryAdjusted/versioned/0.0.1/index.mdx",
  "fa4f677953cd3546",
  "PaymentProcessed-0.0.1",
  {
    "id": 1562,
    "data": 1564,
    "body": 1421,
    "filePath": 1567,
    "digest": 1568,
    "deferredRender": 20
  },
  { "id": 1020, "name": 1417, "summary": 1418, "version": 830, "owners": 1565 },
  [1566],
  { "id": 42 },
  "../examples/default/domains/E-Commerce/subdomains/Payment/services/PaymentService/events/PaymentProcessed/versioned/0.0.1/index.mdx",
  "1b67685ce94288a6",
  "OutOfStock-0.0.1",
  {
    "id": 1569,
    "data": 1571,
    "body": 1581,
    "filePath": 1582,
    "digest": 1583,
    "deferredRender": 20
  },
  {
    "id": 1012,
    "name": 1381,
    "summary": 1382,
    "version": 830,
    "badges": 1572,
    "owners": 1575
  },
  [1573, 1574],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  { "content": 1347, "backgroundColor": 573, "textColor": 573, "icon": 1348 },
  [1576, 1577, 1578, 1579, 1580],
  { "id": 42 },
  { "id": 129 },
  { "id": 11 },
  { "id": 254 },
  { "id": 270 },
  "## Overview\n\nThe `Inventory Adjusted` event is triggered whenever there is a change in the inventory levels of a product. This could occur due to various reasons such as receiving new stock, sales, returns, or manual adjustments by the inventory management team. The event ensures that all parts of the system that rely on inventory data are kept up-to-date with the latest inventory levels.\n\n\u003CNodeGraph />\n\n### Payload\nThe payload of the `Inventory Adjusted` event includes the following fields:\n\n```json title=\"Example of payload\" frame=\"terminal\"\n{\n  \"event_id\": \"string\",\n  \"timestamp\": \"ISO 8601 date-time\",\n  \"product_id\": \"string\",\n  \"adjusted_quantity\": \"integer\",\n  \"new_quantity\": \"integer\",\n  \"adjustment_reason\": \"string\",\n  \"adjusted_by\": \"string\"\n}\n```\n\n### Producing the Event\n\nTo produce an Inventory Adjusted event, use the following example Kafka producer configuration in Python:\n\n```python title=\"Produce event in Python\" frame=\"terminal\"\nfrom kafka import KafkaProducer\nimport json\nfrom datetime import datetime\n\n# Kafka configuration\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Event data\nevent_data = {\n  \"event_id\": \"abc123\",\n  \"timestamp\": datetime.utcnow().isoformat() + 'Z',\n  \"product_id\": \"prod987\",\n  \"adjusted_quantity\": 10,\n  \"new_quantity\": 150,\n  \"adjustment_reason\": \"restock\",\n  \"adjusted_by\": \"user123\"\n}\n\n# Send event to Kafka topic\nproducer.send('inventory.adjusted', event_data)\nproducer.flush()\n```\n\n### Consuming the Event\n\nTo consume an Inventory Adjusted event, use the following example Kafka consumer configuration in Python:\n\n```python title=\"Consuming the event with python\" frame=\"terminal\"\nfrom kafka import KafkaConsumer\nimport json\n\n# Kafka configuration\nconsumer = KafkaConsumer(\n    'inventory.adjusted',\n    bootstrap_servers=['localhost:9092'],\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    group_id='inventory_group',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Consume events\nfor message in consumer:\n    event_data = json.loads(message.value)\n    print(f\"Received Inventory Adjusted event: {event_data}\")\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/OutOfStock/versioned/0.0.1/index.mdx",
  "01f920ef8d292ab3",
  "queries",
  [
    "Map",
    1586,
    1587,
    1599,
    1600,
    1611,
    1612,
    1625,
    1626,
    1638,
    1639,
    1651,
    1652,
    1664,
    1665,
    1677,
    1678
  ],
  "GetInventoryStatus-0.0.1",
  {
    "id": 1586,
    "data": 1588,
    "body": 1596,
    "filePath": 1597,
    "digest": 1598,
    "deferredRender": 20
  },
  {
    "id": 1049,
    "name": 1589,
    "summary": 1590,
    "version": 830,
    "badges": 1591,
    "owners": 1594,
    "schemaPath": 1199
  },
  "Get inventory status",
  "GET request that will return the current stock status for a specific product.\n",
  [1592],
  { "content": 1593, "backgroundColor": 1192, "textColor": 1192 },
  "GET Request",
  [1595],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe GetInventoryStatus message is a query designed to retrieve the current stock status for a specific product. \n\nThis query provides detailed information about the available quantity, reserved quantity, and the warehouse location where the product is stored. It is typically used by systems or services that need to determine the real-time availability of a product, enabling efficient stock management, order fulfillment, and inventory tracking processes. \n\nThis query is essential for ensuring accurate stock levels are reported to downstream systems, including e-commerce platforms, warehouse management systems, and sales channels.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />\n\n\n### Query using CURL\n\nUse this snippet to query the inventory status\n\n```sh title=\"Example CURL command\"\ncurl -X GET \"https://api.yourdomain.com/inventory/status\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"productId\": \"12345\"\n}'\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/queries/GetInventoryStatus/index.mdx",
  "00eb1684c4519d03",
  "GetInventoryList-0.0.1",
  {
    "id": 1599,
    "data": 1601,
    "body": 1608,
    "filePath": 1609,
    "digest": 1610,
    "deferredRender": 20
  },
  {
    "id": 1014,
    "name": 1602,
    "summary": 1603,
    "version": 830,
    "badges": 1604,
    "owners": 1606,
    "schemaPath": 1199
  },
  "List inventory list",
  "GET request that will return inventory list\n",
  [1605],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1607],
  { "id": 42 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe GetInventoryList message is a query used to retrieve a comprehensive list of all available inventory items within a system. It is designed to return detailed information about each item, such as product names, quantities, availability status, and potentially additional metadata like categories or locations. This query is typically utilized by systems or services that require a real-time view of current stock, ensuring that downstream applications or users have accurate and up-to-date information for decision-making or operational purposes. The GetInventoryList is ideal for use cases such as order processing, stock management, or reporting, providing visibility into the full range of inventory data.\n\n\u003CNodeGraph />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/queries/GetInventoryList/index.mdx",
  "4517b5ebf16ef267",
  "GetNotificationDetails-0.0.1",
  {
    "id": 1611,
    "data": 1613,
    "body": 1622,
    "filePath": 1623,
    "digest": 1624,
    "deferredRender": 20
  },
  {
    "id": 1026,
    "name": 1614,
    "summary": 1615,
    "version": 830,
    "badges": 1616,
    "owners": 1618,
    "schemaPath": 1199,
    "sidebar": 1620
  },
  "Get notification details",
  "GET request that will return detailed information about a specific notification, identified by its notificationId.\n",
  [1617],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1619],
  { "id": 42 },
  { "badge": 1621 },
  "GET",
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `GetNotificationDetails` message is a query used to retrieve detailed information about a specific notification identified by its `notificationId`. It provides a comprehensive overview of the notification, including the title, message content, status (read/unread), the date it was created, and any additional metadata related to the notification, such as associated orders or system events. This query is helpful in scenarios where users or systems need detailed insights into a particular notification, such as retrieving full messages or auditing notifications sent to users.\n\nUse cases include viewing detailed information about order updates, system notifications, or promotional messages, allowing users to view their full notification history and details.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/NotificationService/queries/GetNotificationDetails/index.mdx",
  "5825f3b294eb2545",
  "GetUserNotifications-0.0.1",
  {
    "id": 1625,
    "data": 1627,
    "body": 1635,
    "filePath": 1636,
    "digest": 1637,
    "deferredRender": 20
  },
  {
    "id": 1023,
    "name": 1628,
    "summary": 1629,
    "version": 830,
    "badges": 1630,
    "owners": 1632,
    "schemaPath": 1199,
    "sidebar": 1634
  },
  "Get user notifications",
  "GET request that will return a list of notifications for a specific user, with options to filter by status (unread or all).\n",
  [1631],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1633],
  { "id": 42 },
  { "badge": 1621 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `GetUserNotifications` message is a query used to retrieve a list of notifications for a specific user. It allows filtering by notification status, such as unread or all notifications. This query is typically utilized by notification services to display user-specific messages, such as order updates, promotional offers, or system notifications. It supports pagination through `limit` and `offset` parameters, ensuring that only a manageable number of notifications are retrieved at once. This query helps users stay informed about important events or updates related to their account, orders, or the platform.\n\nUse cases include delivering notifications for order updates, promotional campaigns, or general system messages to keep the user informed.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/NotificationService/queries/GetUserNotifications/index.mdx",
  "4b02e9790b934892",
  "GetOrder-0.0.1",
  {
    "id": 1638,
    "data": 1640,
    "body": 1648,
    "filePath": 1649,
    "digest": 1650,
    "deferredRender": 20
  },
  {
    "id": 981,
    "name": 1641,
    "summary": 1642,
    "version": 830,
    "badges": 1643,
    "owners": 1645,
    "schemaPath": 1199,
    "sidebar": 1647
  },
  "Get order details",
  "GET request that will return detailed information about a specific order, identified by its orderId.\n",
  [1644],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1646],
  { "id": 297 },
  { "badge": 1621 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `GetOrder` message is a query used to retrieve detailed information about a specific order, identified by its `orderId`. It provides information such as the order status (e.g., pending, completed, shipped), the items within the order, billing and shipping details, payment information, and the order's total amount. This query is commonly used by systems managing order processing, customer service, or order tracking functionalities.\n\nThis query can be applied in e-commerce systems, marketplaces, or any platform where users and systems need real-time order data for tracking, auditing, or managing customer purchases.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/queries/GetOrder/index.mdx",
  "71f186bfcba04dad",
  "GetPaymentStatus-0.0.1",
  {
    "id": 1651,
    "data": 1653,
    "body": 1661,
    "filePath": 1662,
    "digest": 1663,
    "deferredRender": 20
  },
  {
    "id": 1107,
    "name": 1654,
    "summary": 1655,
    "version": 830,
    "badges": 1656,
    "owners": 1658,
    "schemaPath": 1199,
    "sidebar": 1660
  },
  "Get payment status",
  "GET request that will return the payment status for a specific order, identified by its orderId.\n",
  [1657],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1659],
  { "id": 42 },
  { "badge": 1621 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `GetPaymentStatus` message is a query used to retrieve the payment status for a specific order, identified by its `orderId`. This query returns the current status of the payment, such as whether it is pending, completed, failed, or refunded. It is used by systems that need to track the lifecycle of payments associated with orders, ensuring that the payment has been successfully processed or identifying if any issues occurred during the transaction.\n\nThis query is useful in scenarios such as order management, refund processing, or payment auditing, ensuring that users or systems have real-time visibility into the payment status for a given order.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Payment/services/PaymentService/queries/GetPaymentStatus/index.mdx",
  "85185bb48c156df5",
  "GetSubscriptionStatus-0.0.2",
  {
    "id": 1664,
    "data": 1666,
    "body": 1674,
    "filePath": 1675,
    "digest": 1676,
    "deferredRender": 20
  },
  {
    "id": 1131,
    "name": 1667,
    "summary": 1668,
    "version": 938,
    "badges": 1669,
    "owners": 1671,
    "schemaPath": 1199,
    "sidebar": 1673
  },
  "Get subscription status",
  "GET request that will return the current subscription status for a specific user, identified by their userId.\n",
  [1670],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1672],
  { "id": 283 },
  { "badge": 1621 },
  "import Footer from '@catalog/components/footer.astro';\n\n## Overview\n\nThe `GetSubscriptionStatus` message is a query used to retrieve the current subscription status for a specific user, identified by their `userId`. This query returns detailed information about the user's subscription, such as its current status (active, canceled, expired), the subscription tier or plan, and the next billing date. It is typically used by systems that manage user subscriptions, billing, and renewal processes to ensure that users are aware of their subscription details and any upcoming renewals.\n\nThis query is particularly useful in managing subscriptions for SaaS products, media services, or any recurring payment-based services where users need to manage and view their subscription information.\n\n\u003CNodeGraph />\n\n\u003CSchemaViewer file=\"schema.json\" title=\"JSON Schema\" maxHeight=\"500\" />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/queries/GetSubscriptionStatus/index.mdx",
  "c8a50e0186a41b74",
  "GetSubscriptionStatus-0.0.1",
  {
    "id": 1677,
    "data": 1679,
    "body": 1674,
    "filePath": 1684,
    "digest": 1685,
    "deferredRender": 20
  },
  {
    "id": 1131,
    "name": 1667,
    "summary": 1668,
    "version": 830,
    "badges": 1680,
    "owners": 1682,
    "schemaPath": 1199
  },
  [1681],
  { "content": 1191, "backgroundColor": 1192, "textColor": 1192 },
  [1683],
  { "id": 42 },
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/services/SubscriptionService/queries/GetSubscriptionStatus/versioned/0.0.1/index.mdx",
  "c1544eddb39795d7",
  "channels",
  ["Map", 1688, 1689, 1706, 1707, 1723, 1724],
  "inventory.{env}.events-1.0.0",
  {
    "id": 1688,
    "data": 1690,
    "body": 1703,
    "filePath": 1704,
    "digest": 1705,
    "deferredRender": 20
  },
  {
    "address": 1186,
    "protocols": 1691,
    "parameters": 1692,
    "id": 1186,
    "name": 1699,
    "summary": 1700,
    "version": 852,
    "owners": 1701
  },
  [1348],
  { "env": 1693 },
  { "enum": 1694, "description": 1698 },
  [1695, 1696, 1697],
  "dev",
  "sit",
  "prod",
  "Environment to use",
  "Inventory Events Channel",
  "Central event stream for all inventory-related events including stock updates, allocations, and adjustments\n",
  [1702],
  { "id": 42 },
  "### Overview\nThe Inventory Events channel is the central stream for all inventory-related events across the system. This includes stock level changes, inventory allocations, adjustments, and stocktake events. Events for a specific SKU are guaranteed to be processed in sequence when using productId as the partition key.\n\n\u003CChannelInformation />\n\n### Publishing and Subscribing to Events\n\n#### Publishing Example\n```python\nfrom kafka import KafkaProducer\nimport json\nfrom datetime import datetime\n\n# Kafka configuration\nbootstrap_servers = ['localhost:9092']\ntopic = f'inventory.{env}.events'\n\n# Create a Kafka producer\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Example inventory update event\ninventory_event = {\n    \"eventType\": \"STOCK_LEVEL_CHANGED\",\n    \"timestamp\": datetime.utcnow().isoformat(),\n    \"version\": \"1.0\",\n    \"payload\": {\n        \"productId\": \"PROD-456\",\n        \"locationId\": \"WH-123\",\n        \"previousQuantity\": 100,\n        \"newQuantity\": 95,\n        \"changeReason\": \"ORDER_FULFILLED\",\n        \"unitOfMeasure\": \"EACH\",\n        \"batchInfo\": {\n            \"batchId\": \"BATCH-789\",\n            \"expiryDate\": \"2025-12-31\"\n        }\n    },\n    \"metadata\": {\n        \"source\": \"warehouse_system\",\n        \"correlationId\": \"inv-xyz-123\",\n        \"userId\": \"john.doe\"\n    }\n}\n\n# Send the message - using productId as key for partitioning\nproducer.send(\n    topic, \n    key=inventory_event['payload']['productId'].encode('utf-8'),\n    value=inventory_event\n)\nproducer.flush()\n\nprint(f\"Inventory event sent to topic {topic}\")\n\n```\n\n### Subscription example\n\n```python\nfrom kafka import KafkaConsumer\nimport json\nfrom datetime import datetime\n\nclass InventoryEventConsumer:\n    def __init__(self):\n        # Kafka configuration\n        self.topic = f'inventory.{env}.events'\n        self.consumer = KafkaConsumer(\n            self.topic,\n            bootstrap_servers=['localhost:9092'],\n            group_id='inventory-processor-group',\n            auto_offset_reset='earliest',\n            enable_auto_commit=False,\n            value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n            key_deserializer=lambda x: x.decode('utf-8') if x else None\n        )\n\n    def process_event(self, event):\n        \"\"\"Process individual inventory events based on type\"\"\"\n        event_type = event.get('eventType')\n        \n        if event_type == 'STOCK_LEVEL_CHANGED':\n            self.handle_stock_level_change(event)\n        elif event_type == 'LOW_STOCK_ALERT':\n            self.handle_low_stock_alert(event)\n        # Add more event type handlers as needed\n\n    def handle_stock_level_change(self, event):\n        \"\"\"Handle stock level change events\"\"\"\n        payload = event['payload']\n        print(f\"Stock level change detected for product {payload['productId']}\")\n        print(f\"New quantity: {payload['newQuantity']}\")\n        # Add your business logic here\n\n    def handle_low_stock_alert(self, event):\n        \"\"\"Handle low stock alert events\"\"\"\n        payload = event['payload']\n        print(f\"Low stock alert for product {payload['productId']}\")\n        print(f\"Current quantity: {payload['currentQuantity']}\")\n        # Add your business logic here\n\n    def start_consuming(self):\n        \"\"\"Start consuming messages from the topic\"\"\"\n        try:\n            print(f\"Starting consumption from topic: {self.topic}\")\n            for message in self.consumer:\n                try:\n                    # Process the message\n                    event = message.value\n                    print(f\"Received event: {event['eventType']} for product: {event['payload']['productId']}\")\n                    \n                    # Process the event\n                    self.process_event(event)\n                    \n                    # Commit the offset after successful processing\n                    self.consumer.commit()\n                    \n                except Exception as e:\n                    print(f\"Error processing message: {str(e)}\")\n                    # Implement your error handling logic here\n                    # You might want to send to a DLQ (Dead Letter Queue)\n        \n        except Exception as e:\n            print(f\"Consumer error: {str(e)}\")\n        finally:\n            # Clean up\n            self.consumer.close()\n\nif __name__ == \"__main__\":\n    # Create and start the consumer\n    consumer = InventoryEventConsumer()\n    consumer.start_consuming()\n  ```",
  "../examples/default/channels/inventory.{env}.events/index.mdx",
  "4b30c49f113c69e3",
  "orders.{env}.events-1.0.1",
  {
    "id": 1706,
    "data": 1708,
    "body": 1720,
    "filePath": 1721,
    "digest": 1722,
    "deferredRender": 20
  },
  {
    "address": 1365,
    "protocols": 1709,
    "parameters": 1711,
    "id": 1365,
    "name": 1714,
    "summary": 1715,
    "version": 1343,
    "owners": 1716,
    "sidebar": 1718
  },
  [1710],
  "azure-servicebus",
  { "env": 1712 },
  { "enum": 1713, "description": 1698 },
  [1695, 1696, 1697],
  "Order Events Channel",
  "Central event stream for all order-related events in the order processing lifecycle\n",
  [1717],
  { "id": 42 },
  { "badge": 1719 },
  "ServiceBus",
  "### Overview\nThe Orders Events channel is the central stream for all order-related events across the order processing lifecycle. This includes order creation, updates, payment status, fulfillment status, and customer communications. All events related to a specific order are guaranteed to be processed in sequence when using orderId as the partition key.\n\n\u003CChannelInformation />\n\n### Publishing a message using Azure Service Bus\n\nHere is an example of how to publish an order event using Azure Service Bus:\n\n```python\nimport json\nfrom datetime import datetime\nfrom azure.servicebus import ServiceBusClient, ServiceBusMessage\n\n# --- Azure Service Bus Configuration ---\n# Replace with your actual connection string and queue/topic name\nCONNECTION_STR = \"YOUR_AZURE_SERVICE_BUS_CONNECTION_STRING\"\nQUEUE_NAME = \"orders\"  # Or your specific queue/topic name e.g., f\"orders.{env}.events\"\n\n# --- Example Order Event Data ---\n# This is the same event structure as before\norder_event_data = {\n    \"eventType\": \"ORDER_CREATED\",\n    \"timestamp\": datetime.utcnow().isoformat() + \"Z\", # ISO 8601 format with Z for UTC\n    \"version\": \"1.0\",\n    \"payload\": {\n        \"orderId\": \"12345\",\n        \"customerId\": \"CUST-789\",\n        \"items\": [\n            {\n                \"productId\": \"PROD-456\",\n                \"quantity\": 2,\n                \"price\": 29.99\n            }\n        ],\n        \"totalAmount\": 59.98,\n        \"shippingAddress\": {\n            \"street\": \"123 Main St\",\n            \"city\": \"Springfield\",\n            \"country\": \"US\"\n        }\n    },\n    \"metadata\": {\n        \"source\": \"web_checkout\",\n        \"correlationId\": \"abc-xyz-123\"\n        # Potentially add a message ID if not automatically handled or for specific tracking\n        # \"messageId\": str(uuid.uuid4()) # Requires import uuid\n    }\n}\n\ndef send_order_event_to_service_bus(connection_string, queue_name, event_data):\n    # Create a ServiceBusClient object\n    with ServiceBusClient.from_connection_string(conn_str=connection_string) as servicebus_client:\n        # Create a sender for the queue\n        # For a topic, use: servicebus_client.get_topic_sender(topic_name=queue_name)\n        sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n        with sender:\n            # Serialize the event data to a JSON string\n            event_json = json.dumps(event_data)\n            # Create a ServiceBusMessage object\n            message = ServiceBusMessage(event_json)\n            \n            # Set properties if needed, e.g., message_id or correlation_id\n            # message.message_id = event_data[\"metadata\"].get(\"messageId\")\n            message.correlation_id = event_data[\"metadata\"][\"correlationId\"]\n            \n            # Send the message\n            sender.send_messages(message)\n            print(f\"Sent order event (ID: {event_data['payload']['orderId']}) to Azure Service Bus queue: {queue_name}\")\n\nif __name__ == \"__main__\":\n    # Example of how to call the function\n    # Ensure azure-servicebus package is installed: pip install azure-servicebus\n    \n    # Basic error handling for placeholders\n    if CONNECTION_STR == \"YOUR_AZURE_SERVICE_BUS_CONNECTION_STRING\" or QUEUE_NAME == \"YOUR_QUEUE_NAME\":\n        print(\"Please update CONNECTION_STR and QUEUE_NAME with your actual Azure Service Bus details.\")\n    else:\n        try:\n            send_order_event_to_service_bus(CONNECTION_STR, QUEUE_NAME, order_event_data)\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            print(\"Ensure your Azure Service Bus connection string and queue name are correct,\")\n            print(\"and the 'azure-servicebus' package is installed ('pip install azure-servicebus').\")",
  "../examples/default/channels/orders.{env}.events/index.mdx",
  "c2c6119dcd57051c",
  "payments.{env}.events-1.0.0",
  {
    "id": 1723,
    "data": 1725,
    "body": 1735,
    "filePath": 1736,
    "digest": 1737,
    "deferredRender": 20
  },
  {
    "address": 1416,
    "protocols": 1726,
    "parameters": 1727,
    "id": 1416,
    "name": 1731,
    "summary": 1732,
    "version": 852,
    "owners": 1733
  },
  [1348],
  { "env": 1728 },
  { "enum": 1729, "description": 1730 },
  [1695, 1696, 1697],
  "Environment to use for payment events",
  "Payment Events Channel",
  "All events contain payment ID for traceability and ordered processing.\n",
  [1734],
  { "id": 42 },
  "### Overview\nThe Payments Events channel is the central stream for all payment lifecycle events. This includes payment initiation, authorization, capture, completion and failure scenarios. Events for a specific payment are guaranteed to be processed in sequence when using paymentId as the partition key.\n\n\u003CChannelInformation />\n\n### Publishing Events Using Kafka\n\nHere's an example of publishing a payment event:\n\n```python\nfrom kafka import KafkaProducer\nimport json\nfrom datetime import datetime\n\n# Kafka configuration\nbootstrap_servers = ['localhost:9092']\ntopic = f'payments.{env}.events'\n\n# Create Kafka producer\nproducer = KafkaProducer(\n   bootstrap_servers=bootstrap_servers,\n   value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Example payment processed event\npayment_event = {\n   \"eventType\": \"PAYMENT_PROCESSED\",\n   \"timestamp\": datetime.utcnow().isoformat(),\n   \"version\": \"1.0\",\n   \"payload\": {\n       \"paymentId\": \"PAY-123-456\", \n       \"orderId\": \"ORD-789\",\n       \"amount\": {\n           \"value\": 99.99,\n           \"currency\": \"USD\"\n       },\n       \"status\": \"SUCCESS\",\n       \"paymentMethod\": {\n           \"type\": \"CREDIT_CARD\",\n           \"last4\": \"4242\",\n           \"expiryMonth\": \"12\",\n           \"expiryYear\": \"2025\",\n           \"network\": \"VISA\"\n       },\n       \"transactionDetails\": {\n           \"processorId\": \"stripe_123xyz\",\n           \"authorizationCode\": \"AUTH123\",\n           \"captureId\": \"CAP456\"\n       }\n   },\n   \"metadata\": {\n       \"correlationId\": \"corr-123-abc\",\n       \"merchantId\": \"MERCH-456\", \n       \"source\": \"payment_service\",\n       \"environment\": \"prod\",\n       \"idempotencyKey\": \"PAY-123-456-2024-11-11-99.99\"\n   }\n}\n\n# Send message - using paymentId as key for partitioning\nproducer.send(\n   topic,\n   key=payment_event['payload']['paymentId'].encode('utf-8'),\n   value=payment_event\n)\nproducer.flush()\n```",
  "../examples/default/channels/payment.{env}.events/index.mdx",
  "261c4005551cac3f",
  "flows",
  ["Map", 1740, 1741, 1796, 1797, 1296, 1870, 1892, 1893],
  "CancelSubscription-1.0.0",
  {
    "id": 1740,
    "data": 1742,
    "body": 1793,
    "filePath": 1794,
    "digest": 1795,
    "deferredRender": 20
  },
  {
    "steps": 1743,
    "id": 1129,
    "name": 1789,
    "summary": 1790,
    "version": 852,
    "owners": 1791
  },
  [1744, 1753, 1759, 1767, 1778, 1784, 1786],
  {
    "id": 1745,
    "title": 1746,
    "summary": 1747,
    "actor": 1748,
    "next_step": 1750
  },
  "cancel_subscription_initiated",
  "Cancels Subscription",
  "User cancels their subscription",
  { "name": 1749 },
  "User",
  { "id": 1751, "label": 1752 },
  "cancel_subscription_request",
  "Initiate subscription cancellation",
  { "id": 1751, "title": 1754, "message": 1755, "next_step": 1756 },
  "Cancel Subscription",
  { "id": 1129, "version": 830 },
  { "id": 1757, "label": 1758 },
  "subscription_service",
  "Proceed to subscription service",
  { "id": 1760, "title": 1761, "externalSystem": 1762, "next_step": 1765 },
  "stripe_integration",
  "Stripe",
  { "name": 1761, "summary": 1763, "url": 1764 },
  "3rd party payment system",
  "https://stripe.com/",
  { "id": 1757, "label": 1766 },
  "Return to subscription service",
  { "id": 1757, "title": 1133, "service": 1768, "next_steps": 1769 },
  { "id": 888, "version": 830 },
  [1770, 1772, 1775],
  { "id": 1760, "label": 1771 },
  "Cancel subscription via Stripe",
  { "id": 1773, "label": 1774 },
  "subscription_cancelled",
  "Successful cancellation",
  { "id": 1776, "label": 1777 },
  "subscription_rejected",
  "Failed cancellation",
  { "id": 1773, "title": 1779, "message": 1780, "next_step": 1781 },
  "Subscription has been Cancelled",
  { "id": 985, "version": 830 },
  { "id": 1782, "label": 1783 },
  "notification_service",
  "Email customer",
  { "id": 1776, "title": 1785 },
  "Subscription cancellation has been rejected",
  { "id": 1782, "title": 1787, "service": 1788 },
  "Notifications Service",
  { "id": 872, "version": 938 },
  "User Cancels Subscription",
  "Flow for when a user has cancelled a subscription",
  [1792],
  { "id": 283 },
  "\u003CNodeGraph />",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/flows/CancelSubscription/index.mdx",
  "a5451a875f5326d4",
  "PaymentFlow-1.0.0",
  {
    "id": 1796,
    "data": 1798,
    "body": 1867,
    "filePath": 1868,
    "digest": 1869,
    "deferredRender": 20
  },
  {
    "steps": 1799,
    "id": 1862,
    "name": 1863,
    "summary": 1864,
    "version": 852,
    "owners": 1865
  },
  [1800, 1808, 1814, 1819, 1828, 1838, 1844, 1847, 1849, 1853, 1860],
  { "id": 1801, "title": 1802, "flow": 1803, "next_step": 1805 },
  "flow_step",
  "Flow Step",
  { "id": 1804, "version": 819 },
  "SubscriptionRenewed",
  { "id": 1806, "label": 1807 },
  "place_order_request",
  "Subscription Renewed, New Order Placed",
  { "id": 1806, "title": 1809, "message": 1810, "next_step": 1811 },
  "Place order",
  { "id": 983, "version": 830 },
  { "id": 1812, "label": 1813 },
  "payment_initiated",
  "Initiate payment",
  { "id": 1812, "title": 1446, "message": 1815, "next_steps": 1816 },
  { "id": 1105, "version": 830 },
  [1817, 1818],
  "payment_processed",
  "payment_failed",
  { "id": 1817, "title": 1417, "message": 1820, "next_steps": 1821 },
  { "id": 1020, "version": 830 },
  [1822, 1825],
  { "id": 1823, "label": 1824 },
  "adjust_inventory",
  "Adjust inventory",
  { "id": 1826, "label": 1827 },
  "send_custom_notification",
  "Notify customer",
  { "id": 1818, "type": 1829, "title": 1830, "next_steps": 1831 },
  "node",
  "Payment Failed",
  [1832, 1835],
  { "id": 1833, "label": 1834 },
  "failure_notification",
  "Notify customer of failure",
  { "id": 1836, "label": 1837 },
  "retry_payment",
  "Retry payment",
  { "id": 1823, "title": 1839, "message": 1840, "next_step": 1841 },
  "Inventory Adjusted",
  { "id": 979, "version": 1343 },
  { "id": 1842, "label": 1843 },
  "payment_complete",
  "Complete order",
  { "id": 1826, "type": 1829, "title": 1845, "next_step": 1846 },
  "Customer Notified of Payment",
  { "id": 1842, "label": 1843 },
  { "id": 1833, "type": 1829, "title": 1848 },
  "Customer Notified of Failure",
  { "id": 1836, "type": 1829, "title": 1850, "next_step": 1851 },
  "Retry Payment",
  { "id": 1812, "label": 1852 },
  "Retry payment process",
  { "id": 1842, "title": 1854, "message": 1855, "next_step": 1857 },
  "Payment Complete",
  { "id": 1856, "version": 938 },
  "PaymentComplete",
  { "id": 1858, "label": 1859 },
  "order-complete",
  "Order completed",
  { "id": 1858, "type": 1829, "title": 1861 },
  "Order Completed",
  "PaymentFlow",
  "Payment Flow for customers",
  "Business flow for processing payments in an e-commerce platform",
  [1866],
  { "id": 42 },
  "### Flow of feature\n\u003CNodeGraph />",
  "../examples/default/domains/E-Commerce/subdomains/Payment/flows/PaymentProcessed/index.mdx",
  "aea61d5977e014a1",
  {
    "id": 1296,
    "data": 1871,
    "body": 1793,
    "filePath": 1890,
    "digest": 1891,
    "deferredRender": 20
  },
  {
    "steps": 1872,
    "id": 1129,
    "name": 1789,
    "summary": 1790,
    "version": 830,
    "owners": 1888
  },
  [1873, 1876, 1879, 1882],
  {
    "id": 1745,
    "title": 1746,
    "summary": 1747,
    "actor": 1874,
    "next_step": 1875
  },
  { "name": 1749 },
  { "id": 1751, "label": 1752 },
  { "id": 1751, "title": 1754, "message": 1877, "next_step": 1878 },
  { "id": 1129, "version": 830 },
  { "id": 1757, "label": 1758 },
  { "id": 1760, "title": 1761, "externalSystem": 1880, "next_step": 1881 },
  { "name": 1761, "summary": 1763, "url": 1764 },
  { "id": 1757, "label": 1766 },
  { "id": 1757, "title": 1133, "service": 1883, "next_steps": 1884 },
  { "id": 888, "version": 830 },
  [1885, 1886, 1887],
  { "id": 1760, "label": 1771 },
  { "id": 1773, "label": 1774 },
  { "id": 1776, "label": 1777 },
  [1889],
  { "id": 283 },
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/flows/CancelSubscription/versioned/0.0.1/index.mdx",
  "9f7066e31241beee",
  "SubscriptionRenewed-1.0.0",
  {
    "id": 1892,
    "data": 1894,
    "body": 2111,
    "filePath": 2112,
    "digest": 2113,
    "deferredRender": 20
  },
  {
    "steps": 1895,
    "id": 1804,
    "name": 2107,
    "summary": 2108,
    "version": 852,
    "owners": 2109
  },
  [
    1896, 1921, 1934, 1939, 1944, 1950, 1954, 1972, 1976, 1985, 1990, 1996,
    2005, 2020, 2026, 2031, 2037, 2046, 2048, 2060, 2065, 2071, 2077, 2100, 2105
  ],
  { "id": 1897, "title": 1898, "custom": 1899, "next_step": 1918 },
  "renewal_timer_triggered",
  "Renewal Period Reached",
  {
    "title": 1900,
    "icon": 1901,
    "type": 1902,
    "summary": 1903,
    "color": 1904,
    "properties": 1905,
    "height": 1910,
    "menu": 1911
  },
  "Renewal Timer",
  "ClockIcon",
  "Scheduler",
  "Automated timer triggers the subscription renewal process",
  "orange",
  {
    "subscription_id": 1906,
    "renewal_type": 1907,
    "billing_cycle": 1908,
    "next_billing_date": 1909
  },
  "sub_12345678",
  "Automatic",
  "Monthly",
  "2024-08-01",
  8,
  [1912, 1915],
  { "label": 1913, "url": 1914 },
  "View scheduler configuration",
  "https://docs.example.com/scheduler",
  { "label": 1916, "url": 1917 },
  "Subscription timing documentation",
  "https://docs.example.com/subscription-timing",
  { "id": 1919, "label": 1920 },
  "check_subscription_status",
  "Verify subscription status",
  { "id": 1919, "title": 1922, "service": 1923, "next_steps": 1924 },
  "Check Subscription Status",
  { "id": 888, "version": 830 },
  [1925, 1928, 1931],
  { "id": 1926, "label": 1927 },
  "payment_approval_check",
  "Subscription active, proceed to payment",
  { "id": 1929, "label": 1930 },
  "subscription_expired",
  "Subscription has expired",
  { "id": 1932, "label": 1933 },
  "subscription_canceled",
  "Subscription was canceled",
  { "id": 1929, "type": 1829, "title": 1935, "next_step": 1936 },
  "Subscription Expired",
  { "id": 1937, "label": 1938 },
  "send_renewal_notification",
  "Notify customer to renew",
  { "id": 1932, "type": 1829, "title": 1940, "next_step": 1941 },
  "Subscription Canceled",
  { "id": 1942, "label": 1943 },
  "send_reactivation_offer",
  "Send special reactivation offer",
  { "id": 1937, "title": 1945, "service": 1946, "next_step": 1947 },
  "Send Renewal Notification",
  { "id": 872, "version": 938 },
  { "id": 1948, "label": 1949 },
  "await_customer_action",
  "Wait for customer response",
  { "id": 1942, "title": 1951, "service": 1952, "next_step": 1953 },
  "Send Reactivation Offer",
  { "id": 872, "version": 938 },
  { "id": 1948, "label": 1949 },
  { "id": 1948, "title": 1955, "custom": 1956, "next_steps": 1965 },
  "Await Customer Action",
  {
    "title": 1957,
    "icon": 1958,
    "type": 1959,
    "summary": 1960,
    "color": 1961,
    "properties": 1962,
    "height": 1910
  },
  "Customer Decision Point",
  "UserIcon",
  "Decision",
  "Waiting period for customer to take action on notification",
  "purple",
  { "timeout_period": 1963, "options": 1964 },
  "7 days",
  "Renew, Upgrade, Cancel, Ignore",
  [1966, 1969],
  { "id": 1967, "label": 1968 },
  "manual_renewal_flow",
  "Customer manually renews",
  { "id": 1970, "label": 1971 },
  "flow_ends",
  "No action taken",
  { "id": 1967, "type": 1829, "title": 1973, "next_step": 1974 },
  "Manual Renewal Flow",
  { "id": 1812, "label": 1975 },
  "Process payment",
  { "id": 1926, "title": 1977, "message": 1978, "next_steps": 1979 },
  "Check Payment Approval",
  { "id": 1107, "version": 830 },
  [1980, 1982],
  { "id": 1812, "label": 1981 },
  "Payment approved, proceed with billing",
  { "id": 1983, "label": 1984 },
  "payment_method_invalid",
  "Invalid payment method",
  { "id": 1983, "type": 1829, "title": 1986, "next_step": 1987 },
  "Invalid Payment Method",
  { "id": 1988, "label": 1989 },
  "request_payment_update",
  "Request updated payment method",
  { "id": 1988, "title": 1991, "service": 1992, "next_step": 1993 },
  "Request Payment Update",
  { "id": 872, "version": 938 },
  { "id": 1994, "label": 1995 },
  "await_updated_payment",
  "Wait for payment update",
  { "id": 1994, "title": 1997, "actor": 1998, "next_steps": 1999 },
  "Await Updated Payment Method",
  { "name": 915 },
  [2000, 2002],
  { "id": 1812, "label": 2001 },
  "Payment method updated",
  { "id": 2003, "label": 2004 },
  "subscription_grace_period",
  "No update received",
  { "id": 2003, "title": 2006, "custom": 2007, "next_steps": 2014 },
  "Grace Period",
  {
    "title": 2008,
    "icon": 2009,
    "type": 2010,
    "summary": 2011,
    "color": 573,
    "properties": 2012
  },
  "Subscription Grace Period",
  "ShieldExclamationIcon",
  "Timer",
  "Limited period where subscription remains active despite payment failure",
  { "duration": 1963, "status": 2013 },
  "At risk",
  [2015, 2017],
  { "id": 1812, "label": 2016 },
  "Payment updated during grace period",
  { "id": 2018, "label": 2019 },
  "subscription_suspended",
  "Grace period expired",
  { "id": 2018, "title": 2021, "message": 2022, "next_step": 2023 },
  "Subscription Suspended",
  { "id": 985, "version": 830 },
  { "id": 2024, "label": 2025 },
  "send_suspension_notification",
  "Notify customer of suspension",
  { "id": 2024, "title": 2027, "service": 2028, "next_step": 2029 },
  "Send Suspension Notification",
  { "id": 872, "version": 938 },
  { "id": 1970, "label": 2030 },
  "Flow ends",
  { "id": 1812, "title": 2032, "message": 2033, "next_step": 2034 },
  "Process Payment",
  { "id": 1105, "version": 830 },
  { "id": 2035, "label": 2036 },
  "payment_gateway",
  "Send to payment gateway",
  { "id": 2035, "title": 2038, "externalSystem": 2039, "next_steps": 2041 },
  "Payment Gateway",
  { "name": 1761, "summary": 2040, "url": 1764 },
  "3rd party payment processor",
  [2042, 2044],
  { "id": 1817, "label": 2043 },
  "Payment successful",
  { "id": 1818, "label": 2045 },
  "Payment failed",
  { "id": 1818, "type": 1829, "title": 1830, "next_step": 2047 },
  { "id": 1836, "label": 1837 },
  { "id": 1836, "title": 1850, "custom": 2049, "next_steps": 2056 },
  {
    "title": 2050,
    "icon": 2051,
    "type": 2052,
    "summary": 2053,
    "color": 570,
    "properties": 2054
  },
  "Payment Retry Logic",
  "ArrowPathIcon",
  "Processor",
  "Automated retry logic for failed payments",
  { "max_attempts": 515, "backoff_interval": 2055, "current_attempt": 470 },
  "24 hours",
  [2057, 2058],
  { "id": 1812, "label": 1837 },
  { "id": 2003, "label": 2059 },
  "Max retries exceeded",
  { "id": 1817, "title": 1417, "message": 2061, "next_step": 2062 },
  { "id": 1020, "version": 852 },
  { "id": 2063, "label": 2064 },
  "update_subscription_status",
  "Update subscription",
  { "id": 2063, "title": 2066, "service": 2067, "next_step": 2068 },
  "Update Subscription",
  { "id": 888, "version": 830 },
  { "id": 2069, "label": 2070 },
  "send_renewal_confirmation",
  "Confirm renewal to customer",
  { "id": 2069, "title": 2072, "service": 2073, "next_step": 2074 },
  "Send Renewal Confirmation",
  { "id": 872, "version": 938 },
  { "id": 2075, "label": 2076 },
  "analyze_customer_usage",
  "Analyze customer usage patterns",
  { "id": 2075, "title": 2078, "custom": 2079, "next_steps": 2094 },
  "Analyze Usage Patterns",
  {
    "title": 2080,
    "icon": 2081,
    "type": 2082,
    "summary": 2083,
    "color": 566,
    "properties": 2084,
    "menu": 2087
  },
  "Usage Analytics",
  "ChartBarIcon",
  "Analytics",
  "Analyze customer usage patterns to identify upsell opportunities",
  { "metrics_analyzed": 2085, "lookback_period": 2086 },
  "Feature usage, Resource consumption, Access patterns",
  "90 days",
  [2088, 2091],
  { "label": 2089, "url": 2090 },
  "View analytics dashboard",
  "https://analytics.example.com/subscriptions",
  { "label": 2092, "url": 2093 },
  "Documentation",
  "https://docs.example.com/analytics",
  [2095, 2098],
  { "id": 2096, "label": 2097 },
  "send_upgrade_recommendation",
  "Usage suggests upgrade opportunity",
  { "id": 1970, "label": 2099 },
  "No upgrade opportunity identified",
  { "id": 2096, "title": 2101, "service": 2102, "next_step": 2103 },
  "Send Upgrade Recommendation",
  { "id": 872, "version": 938 },
  { "id": 1970, "label": 2104 },
  "Flow completed",
  { "id": 1970, "type": 1829, "title": 2106 },
  "Flow Completed",
  "Subscription Renewal Flow",
  "Business flow for automatic subscription renewals and related processes",
  [2110],
  { "id": 283 },
  "## Subscription Renewal Flow\n\nThis flow documents the process of automatic subscription renewals, including handling various edge cases such as payment failures, expired subscriptions, and customer interactions.\n\n\u003CNodeGraph />\n\n### Key Components\n\n* **Automatic Renewal Process**: Triggered by a scheduled timer when the subscription renewal period is reached\n* **Payment Processing**: Integration with payment gateway and handling of payment failures\n* **Customer Notifications**: Various notifications sent throughout the process\n* **Grace Period Handling**: Special handling when payments fail with a grace period before subscription suspension\n* **Usage Analytics**: Analysis of customer usage patterns to identify upgrade opportunities\n\n### Business Rules\n\n1. Subscriptions are renewed automatically unless explicitly canceled\n2. Failed payments trigger a retry process (up to 3 attempts)\n3. Customers receive a 7-day grace period before subscription suspension\n4. Usage patterns are analyzed to provide personalized upgrade recommendations",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/flows/SubscriptionRenewed/index.mdx",
  "c8f29ec346445de8",
  "pages",
  ["Map", 2114, 2116],
  {
    "id": 2114,
    "data": 2117,
    "body": 2119,
    "filePath": 2120,
    "digest": 2121,
    "deferredRender": 20
  },
  { "id": 2118 },
  "index",
  "# **EventCatalog**\n\nWelcome to [EventCatalog](https://www.eventcatalog.dev/).\n\nThis open-source project is designed to help you and your teams bring discoverability and clarity to your event-driven architectures (EDA).\n\nTo get started you can read the following guides:\n\n* [Getting started with EventCatalog](https://eventcatalog.dev/docs/development/getting-started/introduction)  \n* [Creating domains](https://eventcatalog.dev/docs/development/guides/domains/adding-domains)  \n* [Creating services](https://eventcatalog.dev/docs/development/guides/services/adding-services)  \n* [Creating commands](https://eventcatalog.dev/docs/development/guides/messages/commands/introduction)  \n* [Creating events](https://eventcatalog.dev/docs/development/guides/messages/events/introduction)  \n* [Assigning owners to resources](https://eventcatalog.dev/docs/owners)  \n* [Using components in your pages (Schemas, OpenAPI, etc)](https://eventcatalog.dev/docs/development/components/using-components)  \n* [Deploying and hosting your EventCatalog](https://eventcatalog.dev/docs/development/deployment)\n\n### **Join the community**\n\nGot questions about EventCatalog? Feature requests or need support? [Join our community on Discord.](https://discord.gg/3rjaZMmrAm)\n\n### **Enterprise support**\n\nUsing EventCatalog and needs enterprise support? Work with us, find out what we offer on our [enterprise page](https://eventcatalog.dev/enterprise).",
  "../examples/default/pages/index.mdx",
  "859c628ff365e5b2",
  "changelogs",
  [
    "Map",
    2124,
    2125,
    2131,
    2132,
    2138,
    2139,
    2145,
    2146,
    2152,
    2153,
    2162,
    2163,
    2169,
    2170,
    2179,
    2180
  ],
  "domains/e-commerce/subdomains/orders/services/ordersservice/changelog",
  {
    "id": 2124,
    "data": 2126,
    "body": 2128,
    "filePath": 2129,
    "digest": 2130,
    "deferredRender": 20
  },
  { "createdAt": 2127 },
  ["Date", "2024-08-01T00:00:00.000Z"],
  "This is my entry",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/changelog.mdx",
  "789df6d4ae875adf",
  "domains/e-commerce/subdomains/subscriptions/flows/cancelsubscription/changelog",
  {
    "id": 2131,
    "data": 2133,
    "body": 2135,
    "filePath": 2136,
    "digest": 2137,
    "deferredRender": 20
  },
  { "createdAt": 2134 },
  ["Date", "2024-08-20T00:00:00.000Z"],
  "Send an email to the customer confirming the successful cancellation.",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/flows/CancelSubscription/changelog.mdx",
  "ae4b9fedecb51385",
  "domains/e-commerce/subdomains/orders/services/inventoryservice/changelog",
  {
    "id": 2138,
    "data": 2140,
    "body": 2142,
    "filePath": 2143,
    "digest": 2144,
    "deferredRender": 20
  },
  { "createdAt": 2141 },
  ["Date", "2024-08-01T00:00:00.000Z"],
  "### Service receives additional events\n\nService now receives [OrderAmended](/docs/events/OrderAmended/0.0.1) and [UpdateInventory](/docs/commands/UpdateInventory/0.0.3) events.",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/changelog.mdx",
  "fcad7e1186da61fc",
  "domains/e-commerce/subdomains/orders/changelog",
  {
    "id": 2145,
    "data": 2147,
    "body": 2149,
    "filePath": 2150,
    "digest": 2151,
    "deferredRender": 20
  },
  { "createdAt": 2148 },
  ["Date", "2024-08-01T00:00:00.000Z"],
  "### Service added to domain\n\nAdded the InventoryService to the domain.",
  "../examples/default/domains/E-Commerce/subdomains/Orders/changelog.mdx",
  "1701b81cd5272d6f",
  "domains/e-commerce/subdomains/orders/services/inventoryservice/events/inventoryadjusted/changelog",
  {
    "id": 2152,
    "data": 2154,
    "body": 2159,
    "filePath": 2160,
    "digest": 2161,
    "deferredRender": 20
  },
  { "createdAt": 2155, "badges": 2156 },
  ["Date", "2024-08-01T00:00:00.000Z"],
  [2157],
  { "content": 2158, "backgroundColor": 1961, "textColor": 1961 },
  "⭐️ JSON Schema",
  "### Added support for JSON Schema\n\nInventoryAdjusted uses Avro but now also supports JSON Draft 7.\n\n```json title=\"Employee JSON Draft\"\n// labeled-line-markers.jsx\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"title\": \"Employee\",\n  \"properties\": {\n    \"Name\": {\n      \"type\": \"string\"\n    },\n    \"Age\": {\n      \"type\": \"integer\"\n    },\n    \"Town\": {\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\"Name\", \"Age\", \"Town\"]\n}\n\n```\n\nUsing it with our Kafka Cluster\n\n## 1. Create a new topic\n\n```sh\n# Create a topic named 'employee_topic'\nkafka-topics.sh --create --topic employee_topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n```\n\n## Step 2: Prepare the JSON Message\n\nCreate a JSON file named `employee.json` with the following content:\n\n```json\n{\n  \"Name\": \"John Doe\",\n  \"Age\": 30,\n  \"Town\": \"Springfield\"\n}\n```\n\n## Step 3: Produce the Message to Kafka Topic\n\nUse the Kafka producer CLI to send the JSON message:\n\n```sh\ncat employee.json | kafka-console-producer.sh --topic employee_topic --bootstrap-server localhost:9092\n```\n\n## Step 4: Verify the Message (Optional)\n\n```sh\nkafka-console-consumer.sh --topic employee_topic --from-beginning --bootstrap-server localhost:9092\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/InventoryAdjusted/changelog.mdx",
  "bdb75d95ba7e41de",
  "domains/e-commerce/subdomains/orders/services/ordersservice/versioned/002/changelog",
  {
    "id": 2162,
    "data": 2164,
    "body": 2166,
    "filePath": 2167,
    "digest": 2168,
    "deferredRender": 20
  },
  { "createdAt": 2165 },
  ["Date", "2024-08-01T00:00:00.000Z"],
  "Something changed",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/OrdersService/versioned/0.0.2/changelog.mdx",
  "968ee35d7f29eed3",
  "domains/e-commerce/subdomains/orders/services/inventoryservice/events/inventoryadjusted/versioned/100/changelog",
  {
    "id": 2169,
    "data": 2171,
    "body": 2176,
    "filePath": 2177,
    "digest": 2178,
    "deferredRender": 20
  },
  { "createdAt": 2172, "badges": 2173 },
  ["Date", "2024-07-11T00:00:00.000Z"],
  [2174],
  { "content": 2175, "backgroundColor": 1192, "textColor": 1192 },
  "New field",
  "### Added new field to schema\n\nWe added the new town property to the schema for downstream consumers. \n\n```json ins={\"New: Added new Town property to schema:\":9-10}\n// labeled-line-markers.jsx\n{\n  \"type\" : \"record\",\n  \"namespace\" : \"Tutorialspoint\",\n  \"name\" : \"Employee\",\n  \"fields\" : [\n     { \"name\" : \"Name\" , \"type\" : \"string\" },\n     { \"name\" : \"Age\" , \"type\" : \"int\" },\n\n     { \"name\" : \"Town\" , \"type\" : \"string\" },\n  ]\n}\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/InventoryAdjusted/versioned/1.0.0/changelog.mdx",
  "1f3ea024bd6295c0",
  "domains/e-commerce/subdomains/orders/services/inventoryservice/events/inventoryadjusted/versioned/001/changelog",
  {
    "id": 2179,
    "data": 2181,
    "body": 2186,
    "filePath": 2187,
    "digest": 2188,
    "deferredRender": 20
  },
  { "createdAt": 2182, "badges": 2183 },
  ["Date", "2024-07-01T00:00:00.000Z"],
  [2184],
  { "content": 2185, "backgroundColor": 570, "textColor": 570 },
  "Breaking change",
  "### Removed fields from schema, added new owners\n\n`Gender` property has been removed from the Schema of the event\n\nAlso added the [full stackers](/docs/teams/full-stack) team as owners of this event\n\n```diff lang=\"json\"\n  {\n    \"type\" : \"record\",\n    \"namespace\" : \"Tutorialspoint\",\n    \"name\" : \"Employee\",\n    \"fields\" : [\n      { \"name\" : \"Name\" , \"type\" : \"string\" },\n      { \"name\" : \"Age\" , \"type\" : \"int\" },\n-     { \"name\" : \"Gender\" , \"type\" : \"string\" },\n    ]\n  }\n```",
  "../examples/default/domains/E-Commerce/subdomains/Orders/services/InventoryService/events/InventoryAdjusted/versioned/0.0.1/changelog.mdx",
  "162fc00e1c57d61f",
  "entities",
  [
    "Map",
    2191,
    2192,
    2248,
    2249,
    2274,
    2275,
    2302,
    2303,
    2344,
    2345,
    2376,
    2377,
    2405,
    2406,
    2444,
    2445,
    2479,
    2480
  ],
  "Invoice-1.0.0",
  {
    "id": 2191,
    "data": 2193,
    "body": 2245,
    "filePath": 2246,
    "digest": 2247,
    "deferredRender": 20
  },
  {
    "identifier": 2194,
    "properties": 2195,
    "id": 822,
    "name": 822,
    "summary": 2244,
    "version": 852
  },
  "invoiceId",
  [
    2196, 2199, 2202, 2206, 2209, 2213, 2217, 2220, 2224, 2227, 2230, 2233,
    2237, 2241
  ],
  { "name": 2194, "type": 2197, "required": 20, "description": 2198 },
  "UUID",
  "Unique identifier for the invoice.",
  { "name": 2200, "type": 2197, "required": 20, "description": 2201 },
  "customerId",
  "Identifier of the customer being invoiced.",
  { "name": 2203, "type": 2197, "required": 2204, "description": 2205 },
  "orderId",
  false,
  "Identifier of the associated order, if applicable.",
  { "name": 2207, "type": 2197, "required": 2204, "description": 2208 },
  "subscriptionId",
  "Identifier of the associated subscription, if applicable.",
  { "name": 2210, "type": 2211, "required": 20, "description": 2212 },
  "invoiceNumber",
  "string",
  "Human-readable, sequential identifier for the invoice (may have specific format).",
  { "name": 2214, "type": 2215, "required": 20, "description": 2216 },
  "issueDate",
  "Date",
  "Date the invoice was generated and issued.",
  { "name": 2218, "type": 2215, "required": 20, "description": 2219 },
  "dueDate",
  "Date by which the payment for the invoice is due.",
  { "name": 2221, "type": 2222, "required": 20, "description": 2223 },
  "totalAmount",
  "decimal",
  "The total amount due on the invoice.",
  { "name": 2225, "type": 2211, "required": 20, "description": 2226 },
  "currency",
  "Currency of the invoice amount.",
  { "name": 2228, "type": 2211, "required": 20, "description": 2229 },
  "status",
  "Current status of the invoice.",
  { "name": 2231, "type": 2197, "required": 20, "description": 2232 },
  "billingAddressId",
  "Identifier for the billing address used on this invoice.",
  { "name": 2234, "type": 2235, "required": 20, "description": 2236 },
  "lineItems",
  "array",
  "List of individual items or services being charged on the invoice.",
  { "name": 2238, "type": 2239, "required": 20, "description": 2240 },
  "createdAt",
  "DateTime",
  "Timestamp when the invoice record was created.",
  { "name": 2242, "type": 2239, "required": 2204, "description": 2243 },
  "paidAt",
  "Timestamp when the invoice was marked as paid.",
  "Represents a bill issued to a customer, detailing charges for products or services.",
  "## Overview\n\nThe Invoice entity represents a formal request for payment issued by the business to a customer. It details the products, services, quantities, prices, taxes, and total amount due, along with payment terms.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Customer:** An invoice is issued to one `Customer`.\n*   **Order/Subscription:** An invoice may be related to one or more `Order`s or a specific `Subscription` period.\n*   **Payment:** An invoice is settled by one or more `Payment` transactions.\n*   **InvoiceLineItem:** An invoice contains multiple `InvoiceLineItem`s detailing the charges.\n*   **BillingProfile:** Invoice generation often uses details from the customer's `BillingProfile`.\n\n## Examples\n\n*   Invoice #INV-00123 issued to Jane Doe for her monthly subscription renewal, due in 15 days.\n*   Invoice #INV-00124 issued to Acme Corp for consulting services rendered in the previous month, status Paid.",
  "../examples/default/domains/E-Commerce/subdomains/Payment/entities/Invoice/index.mdx",
  "2e24f1a0970b8fcf",
  "Order-1.0.0",
  {
    "id": 2248,
    "data": 2250,
    "body": 2271,
    "filePath": 2272,
    "digest": 2273,
    "deferredRender": 20
  },
  {
    "aggregateRoot": 20,
    "identifier": 2203,
    "properties": 2251,
    "id": 911,
    "name": 911,
    "summary": 2270,
    "version": 852
  },
  [2252, 2254, 2256, 2259, 2261, 2264, 2266],
  { "name": 2203, "type": 2197, "required": 20, "description": 2253 },
  "Unique identifier for the order",
  { "name": 2200, "type": 2197, "required": 20, "description": 2255 },
  "Identifier for the customer placing the order",
  { "name": 2257, "type": 2239, "required": 20, "description": 2258 },
  "orderDate",
  "Date and time when the order was placed",
  { "name": 2228, "type": 2211, "required": 20, "description": 2260 },
  "Current status of the order (e.g., Pending, Processing, Shipped, Delivered, Cancelled)",
  { "name": 2262, "type": 2235, "required": 20, "description": 2263 },
  "orderItems",
  "List of items included in the order",
  { "name": 2221, "type": 2222, "required": 20, "description": 2265 },
  "Total monetary value of the order",
  { "name": 2267, "type": 2268, "required": 20, "description": 2269 },
  "shippingAddress",
  "Address",
  "Address where the order should be shipped",
  "Represents a customer's request to purchase products or services.",
  "## Overview\n\nThe Order entity captures all details related to a customer's purchase request. It serves as the central aggregate root within the Orders domain, coordinating information about the customer, products ordered, payment, and shipping.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Customer:** Each order belongs to one `Customer` (identified by `customerId`).\n*   **OrderItem:** An order contains one or more `OrderItem` entities detailing the specific products and quantities.\n*   **Payment:** An order is typically associated with a `Payment` entity (not detailed here).\n*   **Shipment:** An order may lead to one or more `Shipment` entities (not detailed here).\n\n## Examples\n\n*   **Order #12345:** A customer orders 2 units of Product A and 1 unit of Product B, to be shipped to their home address. Status is 'Processing'.\n*   **Order #67890:** A customer places a large order for multiple items, requiring special shipping arrangements. Status is 'Pending' until payment confirmation.",
  "../examples/default/domains/E-Commerce/subdomains/Orders/entities/Order/index.mdx",
  "b76e69f63f8bf2d5",
  "Customer-1.0.0",
  {
    "id": 2274,
    "data": 2276,
    "body": 2299,
    "filePath": 2300,
    "digest": 2301,
    "deferredRender": 20
  },
  {
    "identifier": 2200,
    "properties": 2277,
    "id": 915,
    "name": 915,
    "summary": 2298,
    "version": 852
  },
  [2278, 2280, 2283, 2286, 2289, 2292, 2295],
  { "name": 2200, "type": 2197, "required": 20, "description": 2279 },
  "Unique identifier for the customer",
  { "name": 2281, "type": 2211, "required": 20, "description": 2282 },
  "firstName",
  "Customer's first name",
  { "name": 2284, "type": 2211, "required": 20, "description": 2285 },
  "lastName",
  "Customer's last name",
  { "name": 2287, "type": 2211, "required": 20, "description": 2288 },
  "email",
  "Customer's primary email address (unique)",
  { "name": 2290, "type": 2211, "required": 2204, "description": 2291 },
  "phone",
  "Customer's phone number",
  { "name": 2293, "type": 2235, "required": 2204, "description": 2294 },
  "addresses",
  "List of addresses associated with the customer (e.g., shipping, billing)",
  { "name": 2296, "type": 2239, "required": 20, "description": 2297 },
  "dateRegistered",
  "Date and time when the customer registered",
  "Represents an individual or organization that places orders.",
  "## Overview\n\nThe Customer entity holds information about the individuals or organizations who interact with the system, primarily by placing orders. It stores contact details, addresses, and other relevant personal or business information.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Order:** A customer can have multiple `Order` entities. The `Order` entity holds a reference (`customerId`) back to the `Customer`.\n*   **Address:** A customer can have multiple associated `Address` value objects or entities.\n\n## Examples\n\n*   **Customer A:** Jane Doe, registered on 2023-01-15, with a primary shipping address and a billing address.\n*   **Customer B:** John Smith, a long-time customer with multiple past orders.",
  "../examples/default/domains/E-Commerce/subdomains/Orders/entities/Customer/index.mdx",
  "5e0ecb5a1abe55bd",
  "Payment-1.0.0",
  {
    "id": 2302,
    "data": 2304,
    "body": 2341,
    "filePath": 2342,
    "digest": 2343,
    "deferredRender": 20
  },
  {
    "identifier": 2305,
    "properties": 2306,
    "id": 824,
    "name": 824,
    "summary": 2340,
    "version": 852
  },
  "paymentId",
  [
    2307, 2309, 2311, 2313, 2315, 2318, 2321, 2323, 2325, 2328, 2331, 2334, 2337
  ],
  { "name": 2305, "type": 2197, "required": 20, "description": 2308 },
  "Unique identifier for the payment transaction.",
  { "name": 2200, "type": 2197, "required": 20, "description": 2310 },
  "Identifier of the customer making the payment.",
  { "name": 2194, "type": 2197, "required": 2204, "description": 2312 },
  "Identifier of the invoice this payment settles.",
  { "name": 2203, "type": 2197, "required": 2204, "description": 2314 },
  "Identifier of the order this payment is for.",
  { "name": 2316, "type": 2197, "required": 20, "description": 2317 },
  "paymentMethodId",
  "Identifier of the payment method used (e.g., credit card, bank transfer).",
  { "name": 2319, "type": 2222, "required": 20, "description": 2320 },
  "amount",
  "The amount of money transferred in the payment.",
  { "name": 2225, "type": 2211, "required": 20, "description": 2322 },
  "Currency of the payment amount.",
  { "name": 2228, "type": 2211, "required": 20, "description": 2324 },
  "Current status of the payment transaction.",
  { "name": 2326, "type": 2211, "required": 2204, "description": 2327 },
  "transactionReference",
  "External reference ID from the payment processor or bank.",
  { "name": 2329, "type": 2239, "required": 20, "description": 2330 },
  "paymentDate",
  "Timestamp when the payment was processed or attempted.",
  { "name": 2332, "type": 2211, "required": 2204, "description": 2333 },
  "failureReason",
  "Reason provided by the payment gateway if the payment failed.",
  { "name": 2335, "type": 2222, "required": 2204, "description": 2336 },
  "refundAmount",
  "The amount refunded, if any part of the payment was returned.",
  { "name": 2338, "type": 2239, "required": 2204, "description": 2339 },
  "refundDate",
  "Timestamp when the refund was processed.",
  "Represents a payment transaction made by a customer, usually to settle an invoice or pay for an order.",
  "## Overview\n\nThe Payment entity records the details of a monetary transaction initiated by a customer towards the business. It captures the amount, currency, payment method used, status (success, failure, refund), and links to the relevant invoice or order being paid.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Customer:** A payment is made by one `Customer`.\n*   **Invoice/Order:** A payment typically corresponds to one `Invoice` or `Order`.\n*   **PaymentMethod:** A payment is executed using a specific `PaymentMethod`.\n*   **Transaction:** A payment attempt often results in one or more lower-level `Transaction` records (e.g., authorization, capture).\n\n## Examples\n\n*   Payment #PAY-98765 from Jane Doe for $19.99 via Credit Card ending in 1234, status Succeeded, settling Invoice #INV-00123.\n*   Payment #PAY-98766 attempt from John Smith for $50.00 via Bank Transfer, status Failed (Reason: Insufficient Funds).\n*   Payment #PAY-98760 for $100.00, partially refunded for $25.00 on 2024-05-18.",
  "../examples/default/domains/E-Commerce/subdomains/Payment/entities/Payment/index.mdx",
  "4800672f12b30efc",
  "PaymentMethod-1.0.0",
  {
    "id": 2344,
    "data": 2346,
    "body": 2373,
    "filePath": 2374,
    "digest": 2375,
    "deferredRender": 20
  },
  {
    "identifier": 2316,
    "properties": 2347,
    "id": 826,
    "name": 826,
    "summary": 2372,
    "version": 852
  },
  [2348, 2350, 2352, 2355, 2359, 2363, 2365, 2367, 2369],
  { "name": 2316, "type": 2197, "required": 20, "description": 2349 },
  "Unique identifier for the payment method.",
  { "name": 2200, "type": 2197, "required": 20, "description": 2351 },
  "Identifier of the customer who owns this payment method.",
  { "name": 2353, "type": 2211, "required": 20, "description": 2354 },
  "type",
  "The type of payment method.",
  { "name": 2356, "type": 2357, "required": 20, "description": 2358 },
  "details",
  "object",
  "Contains type-specific, often sensitive details (e.g., last 4 digits of card, card brand, bank name, account type, token). **Never store raw PANs or sensitive data.**",
  { "name": 2360, "type": 2361, "required": 20, "description": 2362 },
  "isDefault",
  "boolean",
  "Indicates if this is the customer's default payment method.",
  { "name": 2231, "type": 2197, "required": 20, "description": 2364 },
  "Identifier for the billing address verified for this payment method.",
  { "name": 2228, "type": 2211, "required": 20, "description": 2366 },
  "Current status of the payment method.",
  { "name": 2238, "type": 2239, "required": 20, "description": 2368 },
  "Timestamp when the payment method was added.",
  { "name": 2370, "type": 2239, "required": 20, "description": 2371 },
  "updatedAt",
  "Timestamp when the payment method was last updated.",
  "Represents a payment instrument a customer can use, like a credit card or bank account.",
  "## Overview\n\nThe PaymentMethod entity represents a specific payment instrument registered by a customer, such as a credit card or a linked bank account. It stores necessary (non-sensitive) details required to initiate payments and links to the associated customer and billing address.\n\n**Security Note:** Sensitive details like full card numbers or bank account numbers should **never** be stored directly. Rely on tokenization provided by payment gateways.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Customer:** A payment method belongs to one `Customer`.\n*   **Address:** Linked to a specific billing `Address`.\n*   **Payment:** Used to make `Payment` transactions.\n*   **Subscription:** May be designated as the payment method for a `Subscription`.\n\n## Examples\n\n*   Jane Doe's default Visa card ending in 1234, expiring 12/2025, status Active.\n*   John Smith's linked bank account (Chase, Checking), status Active.\n*   An old MasterCard ending in 5678 belonging to Jane Doe, status Expired.",
  "../examples/default/domains/E-Commerce/subdomains/Payment/entities/PaymentMethod/index.mdx",
  "ad998bb9a47b6182",
  "OrderItem-1.0.0",
  {
    "id": 2376,
    "data": 2378,
    "body": 2402,
    "filePath": 2403,
    "digest": 2404,
    "deferredRender": 20
  },
  {
    "identifier": 2379,
    "properties": 2380,
    "id": 913,
    "name": 913,
    "summary": 2401,
    "version": 852
  },
  "orderItemId",
  [2381, 2383, 2385, 2388, 2391, 2395, 2398],
  { "name": 2379, "type": 2197, "required": 20, "description": 2382 },
  "Unique identifier for the order item",
  { "name": 2203, "type": 2197, "required": 20, "description": 2384 },
  "Identifier for the parent Order",
  { "name": 2386, "type": 2197, "required": 20, "description": 2387 },
  "productId",
  "Identifier for the product being ordered",
  { "name": 2389, "type": 2211, "required": 2204, "description": 2390 },
  "productName",
  "Name of the product at the time of order",
  { "name": 2392, "type": 2393, "required": 20, "description": 2394 },
  "quantity",
  "integer",
  "Number of units of the product ordered",
  { "name": 2396, "type": 2222, "required": 20, "description": 2397 },
  "unitPrice",
  "Price per unit of the product at the time of order",
  { "name": 2399, "type": 2222, "required": 20, "description": 2400 },
  "totalPrice",
  "Total price for this item line (quantity * unitPrice)",
  "Represents a single item within a customer's order.",
  "## Overview\n\nThe OrderItem entity details a specific product and its quantity requested within an `Order`. It holds information about the product, the quantity ordered, and the price calculation for that line item. OrderItems are part of the `Order` aggregate.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Order:** Each `OrderItem` belongs to exactly one `Order` (identified by `orderId`). It is a constituent part of the Order aggregate.\n*   **Product:** Each `OrderItem` refers to one `Product` (identified by `productId`).\n\n## Examples\n\n*   **OrderItem A (for Order #12345):** Product ID: P001, Quantity: 2, Unit Price: $50.00, Total Price: $100.00\n*   **OrderItem B (for Order #12345):** Product ID: P002, Quantity: 1, Unit Price: $75.00, Total Price: $75.00",
  "../examples/default/domains/E-Commerce/subdomains/Orders/entities/OrderItem/index.mdx",
  "52e5e80b7d383f9b",
  "Transaction-1.0.0",
  {
    "id": 2405,
    "data": 2407,
    "body": 2441,
    "filePath": 2442,
    "digest": 2443,
    "deferredRender": 20
  },
  {
    "identifier": 2408,
    "properties": 2409,
    "id": 828,
    "name": 828,
    "summary": 2440,
    "version": 852
  },
  "transactionId",
  [2410, 2412, 2414, 2416, 2419, 2421, 2423, 2425, 2428, 2431, 2434, 2437],
  { "name": 2408, "type": 2197, "required": 20, "description": 2411 },
  "Unique identifier for this specific gateway transaction.",
  { "name": 2305, "type": 2197, "required": 20, "description": 2413 },
  "Identifier of the parent Payment this transaction belongs to.",
  { "name": 2353, "type": 2211, "required": 20, "description": 2415 },
  "The type of operation performed with the payment gateway.",
  { "name": 2417, "type": 2211, "required": 20, "description": 2418 },
  "gatewayReferenceId",
  "Unique transaction ID provided by the external payment gateway.",
  { "name": 2319, "type": 2222, "required": 20, "description": 2420 },
  "The amount associated with this specific transaction operation.",
  { "name": 2225, "type": 2211, "required": 20, "description": 2422 },
  "Currency of the transaction amount.",
  { "name": 2228, "type": 2211, "required": 20, "description": 2424 },
  "Status reported by the gateway for this specific operation.",
  { "name": 2426, "type": 2211, "required": 2204, "description": 2427 },
  "responseCode",
  "Response code returned by the payment gateway.",
  { "name": 2429, "type": 2211, "required": 2204, "description": 2430 },
  "responseMessage",
  "Detailed message or reason returned by the gateway.",
  { "name": 2432, "type": 2239, "required": 20, "description": 2433 },
  "processedAt",
  "Timestamp when the transaction was processed by the gateway.",
  { "name": 2435, "type": 316, "required": 2204, "description": 2436 },
  "rawRequest",
  "Raw request payload sent to the gateway (use with caution).",
  { "name": 2438, "type": 316, "required": 2204, "description": 2439 },
  "rawResponse",
  "Raw response payload received from the gateway (use with caution).",
  "Represents a low-level interaction with a payment gateway or processor (e.g., authorize, capture, refund, void).",
  "## Overview\n\nThe Transaction entity logs the individual interactions with an external payment processor (like Stripe, PayPal, Adyen) that occur as part of processing a `Payment`. This provides a detailed audit trail of gateway operations, including authorizations, captures, refunds, and any associated success or failure responses.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Payment:** A transaction is part of one `Payment`.\n\n## Examples\n\n*   **Authorization Success:** Type: Authorize, PaymentID: PAY-98765, GatewayRef: auth_abc, Amount: $19.99, Status: Success.\n*   **Capture Success:** Type: Capture, PaymentID: PAY-98765, GatewayRef: ch_def, Amount: $19.99, Status: Success (following the authorization).\n*   **Authorization Failure:** Type: Authorize, PaymentID: PAY-98766, GatewayRef: auth_ghi, Amount: $50.00, Status: Failure, ResponseCode: 'declined', ResponseMessage: 'Insufficient Funds'.\n*   **Refund Success:** Type: Refund, PaymentID: PAY-98760, GatewayRef: re_jkl, Amount: $25.00, Status: Success.",
  "../examples/default/domains/E-Commerce/subdomains/Payment/entities/Transaction/index.mdx",
  "9469e84c8e06f7c2",
  "SubscriptionPeriod-1.0.0",
  {
    "id": 2444,
    "data": 2446,
    "body": 2476,
    "filePath": 2477,
    "digest": 2478,
    "deferredRender": 20
  },
  {
    "identifier": 2447,
    "properties": 2448,
    "id": 893,
    "name": 893,
    "summary": 2475,
    "version": 852
  },
  "subscriptionPeriodId",
  [2449, 2451, 2453, 2456, 2459, 2462, 2464, 2466, 2468, 2471, 2473],
  { "name": 2447, "type": 2197, "required": 20, "description": 2450 },
  "Unique identifier for this specific subscription period.",
  { "name": 2207, "type": 2197, "required": 20, "description": 2452 },
  "Identifier of the parent Subscription this period belongs to.",
  { "name": 2454, "type": 2197, "required": 20, "description": 2455 },
  "planId",
  "Identifier of the Plan active during this period.",
  { "name": 2457, "type": 2215, "required": 20, "description": 2458 },
  "startDate",
  "The start date of this billing period.",
  { "name": 2460, "type": 2215, "required": 20, "description": 2461 },
  "endDate",
  "The end date of this billing period.",
  { "name": 2194, "type": 2197, "required": 2204, "description": 2463 },
  "Identifier of the invoice created for this period's charge.",
  { "name": 2305, "type": 2197, "required": 2204, "description": 2465 },
  "Identifier of the payment that settled the invoice for this period.",
  { "name": 2228, "type": 2211, "required": 20, "description": 2467 },
  "Status specific to this period (reflects invoicing/payment state).",
  { "name": 2469, "type": 2222, "required": 2204, "description": 2470 },
  "amountBilled",
  "The actual amount billed for this period (could differ from plan due to promotions, usage, etc.).",
  { "name": 2225, "type": 2211, "required": 2204, "description": 2472 },
  "Currency of the billed amount.",
  { "name": 2238, "type": 2239, "required": 20, "description": 2474 },
  "Timestamp when this period record was created (often at the start of the period).",
  "Represents a single billing cycle or interval within a subscription's lifetime.",
  "## Overview\n\nThe SubscriptionPeriod entity tracks the state and details of a specific billing cycle within a `Subscription`. It links the subscription to the relevant invoice and payment for that interval and records the exact dates and amount billed.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Subscription:** A subscription period belongs to one `Subscription`.\n*   **Plan:** Reflects the `Plan` active during this period.\n*   **Invoice:** May be associated with one `Invoice` generated for this period.\n*   **Payment:** May be associated with one `Payment` that settled the period's invoice.\n\n## Examples\n\n*   Period for Jane Doe's 'Pro Plan' from 2024-05-01 to 2024-05-31, invoiced via #INV-00123, status Paid.\n*   Period for Acme Corp's 'Enterprise Plan' from 2024-04-15 to 2024-05-14, status Billed, awaiting payment.\n*   The first period (trial) for a new subscription from 2024-05-20 to 2024-06-19, status Active, amountBilled $0.00.",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/entities/SubscriptionPeriod/index.mdx",
  "6fafd64db9d7994d",
  "BillingProfile-1.0.0",
  {
    "id": 2479,
    "data": 2481,
    "body": 2507,
    "filePath": 2508,
    "digest": 2509,
    "deferredRender": 20
  },
  {
    "identifier": 2482,
    "properties": 2483,
    "id": 891,
    "name": 891,
    "summary": 2506,
    "version": 852
  },
  "billingProfileId",
  [2484, 2486, 2488, 2491, 2494, 2497, 2499, 2502, 2504],
  { "name": 2482, "type": 2197, "required": 20, "description": 2485 },
  "Unique identifier for the billing profile.",
  { "name": 2200, "type": 2197, "required": 20, "description": 2487 },
  "Identifier of the customer this billing profile belongs to.",
  { "name": 2489, "type": 2211, "required": 2204, "description": 2490 },
  "billingEmail",
  "Specific email address for sending invoices and billing notifications.",
  { "name": 2492, "type": 2211, "required": 2204, "description": 2493 },
  "companyName",
  "Company name for billing purposes.",
  { "name": 2495, "type": 2211, "required": 2204, "description": 2496 },
  "taxId",
  "Tax identification number (e.g., VAT ID, EIN).",
  { "name": 2231, "type": 2197, "required": 20, "description": 2498 },
  "Identifier for the primary billing address associated with this profile.",
  { "name": 2500, "type": 2197, "required": 2204, "description": 2501 },
  "preferredPaymentMethodId",
  "Customer's preferred payment method for charges related to this profile.",
  { "name": 2238, "type": 2239, "required": 20, "description": 2503 },
  "Timestamp when the billing profile was created.",
  { "name": 2370, "type": 2239, "required": 20, "description": 2505 },
  "Timestamp when the billing profile was last updated.",
  "Stores billing-related contact information and preferences for a customer, often used for invoices and communication.",
  "## Overview\n\nThe BillingProfile entity consolidates billing-specific details for a customer, such as the billing address, contact email for invoices, tax information, and potentially preferred payment methods. This might be distinct from the customer's general contact information or shipping addresses.\n\n### Entity Properties\n\u003CEntityPropertiesTable />\n\n## Relationships\n\n*   **Customer:** A billing profile belongs to one `Customer`. A customer might potentially have multiple profiles in complex scenarios, but often just one.\n*   **Address:** Linked to a primary billing `Address`.\n*   **PaymentMethod:** May specify a preferred `PaymentMethod`.\n*   **Invoice:** Invoices are typically generated using information from the BillingProfile.\n*   **Subscription:** Subscriptions may use the associated customer's BillingProfile for charging.\n\n## Examples\n\n*   Jane Doe's personal billing profile with her home address and primary email.\n*   Acme Corp's billing profile with their HQ address, VAT ID, and accounts payable email address.",
  "../examples/default/domains/E-Commerce/subdomains/Subscriptions/entities/BillingProfile/index.mdx",
  "a6ba8d592799fd54"
]
